[
    {
        "section": "-1",
        "content": "\\documentclass{article}\n\\usepackage[nonatbib,final]{sty/neurips_2024}\n\\usepackage{sty/mymath}\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\\usepackage{hyperref}\n\\usepackage{url}\n\\usepackage[\n  style=alphabetic,\n  backend=biber,\n  maxalphanames=3,\n  maxbibnames=20,\n  maxcitenames=3,\n  giveninits=true,\n  doi=false,\n  url=true,\n  backref=true,\n]{biblatex}\n\\usepackage{hyperref}\n\\hypersetup{\n  colorlinks=true,\n  linkcolor=blue,\n  filecolor=magenta,\n  urlcolor=cyan,\n  citecolor=purple\n}\n\\addbibresource{references.bib}\n\\usepackage{amsfonts}\n\\usepackage{amsmath}\n \\usepackage{amsthm}\n\\usepackage{amssymb}\n\\usepackage{mathrsfs}\n\\usepackage{bm}\n\\usepackage{nicefrac}\n\\usepackage{xfrac}\n\\usepackage{subdepth}\n\\usepackage{array}\n\\usepackage{booktabs}\n\\usepackage{etoolbox}\n\\usepackage{microtype}\n\\usepackage[nameinlink,capitalise]{cleveref}\n\\usepackage[font=small,labelfont=bf]{caption}\n\\usepackage[format=hang]{subcaption}\n\\usepackage{graphicx}\n\\usepackage[export]{adjustbox}\n\\usepackage{color}\n\\usepackage{xcolor}\n\\usepackage{wrapfig}\n\\usepackage{multirow}\n\\usepackage{ifthen}\n\\usepackage{sidecap}\n\\crefname{lemma}{lemma}{lemmas}\n\\Crefname{lemma}{Lemma}{Lemmas}\n<PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n<PLACEHOLDER_NEWCOMMAND_2>\n<PLACEHOLDER_NEWCOMMAND_3>\n<PLACEHOLDER_NEWCOMMAND_4>\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n<PLACEHOLDER_NEWCOMMAND_7>\n\\usepackage{marginnote}\n<PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n\\definecolor{electric-purple}{RGB}{191, 0, 255}\n<PLACEHOLDER_NEWCOMMAND_10>\n\\definecolor{kelly-green}{RGB}{45, 179, 0}\n<PLACEHOLDER_NEWCOMMAND_11>\n<PLACEHOLDER_NEWCOMMAND_12>\n<PLACEHOLDER_NEWCOMMAND_13>\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_CAP_1>\n\\author{\n\tLeon Lufkin \\\\\n\tYale University \\\\\n\t\\texttt{leon.lufkin@yale.edu} \\\\\n    \t\\And\n\tAndrew Saxe \\\\\n\tGatsby Unit \\& SWC, UCL \\\\\n\t\\texttt{a.saxe@ucl.ac.uk} \\\\\n\t\\And\n\tErin Grant \\\\\n\tGatsby Unit \\& SWC, UCL \\\\\n\t\\texttt{erin.grant@ucl.ac.uk}\n}\n\\usepackage{tcolorbox}\n\\usepackage{enumitem}\n\\tcbuselibrary{theorems, breakable, skins}\n\\tcbset{\n  assumption-box/.style={\n    enhanced,\n    colback=white, colframe=black,\n    colbacktitle=white, coltitle=black,\n    boxed title style={size=small,colframe=white},\n    boxrule=0.3mm,\n    rounded corners,\n    before upper={\\parindent0pt},\n    separator sign none,\n    attach boxed title to top center={yshift=-3mm},\n    before=\\vspace{0pt},\n    after=\\vspace{5pt},\n  }\n}\n\\newtcbtheorem[no counter]{model}{}{\n  assumption-box,\n  before upper={\n    \\setlist[enumerate]{\n      leftmargin=*,\n      label=(M\\arabic*),\n      ref=M\\thetcbcounter\\arabic*,\n    }\n  },\n}{prop}\n\\newtcbtheorem[no counter]{stimulus}{}{\n  assumption-box,\n  before upper={\n    \\setlist[enumerate]{\n      leftmargin=*,\n      label=(S\\arabic*),\n      ref=S\\thetcbcounter\\arabic*,\n    }\n  },\n}{prop}\n\\newtcbtheorem[no counter]{analysis}{}{\n  assumption-box,\n  before upper={\n    \\setlist[enumerate]{\n      leftmargin=*,\n      label=(A\\arabic*),\n      ref=A\\thetcbcounter\\arabic*,\n    }\n  },\n}{assumption}",
        "trans_content": "\\documentclass{article}\n\\usepackage[nonatbib,final]{sty/neurips_2024}\n\\usepackage{sty/mymath}\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\\usepackage{hyperref}\n\\usepackage{url}\n\\usepackage[\n  style=alphabetic,\n  backend=biber,\n  maxalphanames=3,\n  maxbibnames=20,\n  maxcitenames=3,\n  giveninits=true,\n  doi=false,\n  url=true,\n  backref=true,\n]{biblatex}\n\\usepackage{hyperref}\n\\hypersetup{\n  colorlinks=true,\n  linkcolor=blue,\n  filecolor=magenta,\n  urlcolor=cyan,\n  citecolor=purple\n}\n\\addbibresource{references.bib}\n\\usepackage{amsfonts}\n\\usepackage{amsmath}\n \\usepackage{amsthm}\n\\usepackage{amssymb}\n\\usepackage{mathrsfs}\n\\usepackage{bm}\n\\usepackage{nicefrac}\n\\usepackage{xfrac}\n\\usepackage{subdepth}\n\\usepackage{array}\n\\usepackage{booktabs}\n\\usepackage{etoolbox}\n\\usepackage{microtype}\n\\usepackage[nameinlink,capitalise]{cleveref}\n\\usepackage[font=small,labelfont=bf]{caption}\n\\usepackage[format=hang]{subcaption}\n\\usepackage{graphicx}\n\\usepackage[export]{adjustbox}\n\\usepackage{color}\n\\usepackage{xcolor}\n\\usepackage{wrapfig}\n\\usepackage{multirow}\n\\usepackage{ifthen}\n\\usepackage{sidecap}\n\\crefname{lemma}{lemma}{lemmas}\n\\Crefname{lemma}{Lemma}{Lemmas}\n<PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n<PLACEHOLDER_NEWCOMMAND_2>\n<PLACEHOLDER_NEWCOMMAND_3>\n<PLACEHOLDER_NEWCOMMAND_4>\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n<PLACEHOLDER_NEWCOMMAND_7>\n\\usepackage{marginnote}\n<PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n\\definecolor{electric-purple}{RGB}{191, 0, 255}\n<PLACEHOLDER_NEWCOMMAND_10>\n\\definecolor{kelly-green}{RGB}{45, 179, 0}\n<PLACEHOLDER_NEWCOMMAND_11>\n<PLACEHOLDER_NEWCOMMAND_12>\n<PLACEHOLDER_NEWCOMMAND_13>\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_CAP_1>\n\\author{\n\tLeon Lufkin \\\\\n\tYale University \\\\\n\t\\texttt{leon.lufkin@yale.edu} \\\\\n    \t\\And\n\tAndrew Saxe \\\\\n\tGatsby Unit \\& SWC, UCL \\\\\n\t\\texttt{a.saxe@ucl.ac.uk} \\\\\n\t\\And\n\tErin Grant \\\\\n\tGatsby Unit \\& SWC, UCL \\\\\n\t\\texttt{erin.grant@ucl.ac.uk}\n}\n\\usepackage{tcolorbox}\n\\usepackage{enumitem}\n\\tcbuselibrary{theorems, breakable, skins}\n\\tcbset{\n  assumption-box/.style={\n    enhanced,\n    colback=white, colframe=black,\n    colbacktitle=white, coltitle=black,\n    boxed title style={size=small,colframe=white},\n    boxrule=0.3mm,\n    rounded corners,\n    before upper={\\parindent0pt},\n    separator sign none,\n    attach boxed title to top center={yshift=-3mm},\n    before=\\vspace{0pt},\n    after=\\vspace{5pt},\n  }\n}\n\\newtcbtheorem[no counter]{model}{}{\n  assumption-box,\n  before upper={\n    \\setlist[enumerate]{\n      leftmargin=*,\n      label=(M\\arabic*),\n      ref=M\\thetcbcounter\\arabic*,\n    }\n  },\n}{prop}\n\\newtcbtheorem[no counter]{stimulus}{}{\n  assumption-box,\n  before upper={\n    \\setlist[enumerate]{\n      leftmargin=*,\n      label=(S\\arabic*),\n      ref=S\\thetcbcounter\\arabic*,\n    }\n  },\n}{prop}\n\\newtcbtheorem[no counter]{analysis}{}{\n  assumption-box,\n  before upper={\n    \\setlist[enumerate]{\n      leftmargin=*,\n      label=(A\\arabic*),\n      ref=A\\thetcbcounter\\arabic*,\n    }\n  },\n}{assumption}"
    },
    {
        "section": "0",
        "content": "\\begin{document}\n\n\\maketitle\n\n<PLACEHOLDER_ENV_1>\n\n<PLACEHOLDER_sections/01-introduction_begin>",
        "trans_content": "\\begin{document}\n\n\\maketitle\n\n<PLACEHOLDER_ENV_1>\n\n<PLACEHOLDER_sections/01-introduction_begin>"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\n\\label{sec:introduction}\n\nA striking feature of peripheral responses in the animal nervous system is \\emph{localization}---that is,\nthe linear receptive fields of simple-cell neurons often respond to contiguous regions much smaller than their full input domain.\nIn vision, retinal ganglion cells approximate localized center-surround filters that tile the input~\\parencite{dacey2000center,doi2012efficient,knudsen1978space},\nand simple cells downstream in primary visual cortex have localized filters that are selective for spatial frequency and orientation~\\parencite{hubel1959receptive,hubel1968receptive,rolls1995sparseness,niell2008highly,willmore2011sparse,ringach2002orientation,ringach2002spatial}.\nIn primary somatosensory cortex, neurons respond to stimulation of restricted regions of skin~\\parencite{crochet2011synaptic} and\nin primary auditory cortex, spatiotemporal receptive fields are typically localized in both time and frequency domains~\\parencite{deweese2003binary,hromadka2008sparse};\nsee \\cref{fig:sim-real-gabors} (left).\n\nBy contrast, artificial learning systems do not always learn localized filters.\nPrincipal component analysis tends to fit weights that span the entire input signal, as do unregularized autoencoder neural network architectures and restricted Boltzmann machines~\\parencite{saxe2011unsupervised}.\nThis difference has prompted the search for artificial learning models that can learn\nlocalized receptive fields from naturalistic stimuli,\nthe most notable of which are sparse coding~\\parencite{olshausen1996emergence,olshausen1997sparse}\nand independent component analysis~\\parencite[ICA;~][]{bell1997independent,vanhateren1998independent}.\nSparse coding, ICA, and related compression methods that produce localized receptive fields from naturalistic data share a top-down approach---they find an efficient representation of the input signal by optimizing an explicit sparsity criterion, or an independence criterion that necessitates sparsity in a critically parameterized regime~\\cite{field1999wavelets,saxe2011unsupervised}.\n\nThough sparsity is appealing as a potentially unifying explanation for localization, localization also emerges naturally in networks trained to perform classification tasks without any explicit sparsity regularization~\\parencite{krizhevsky2012imagenet,zeiler2013visualizing,yosinski2015understanding,sengupta2018manifoldtiling}; see \\cref{fig:sim-real-gabors} (center) for an example.\n\\textcite{ingrosso2022data} distilled such examples of emergent localization by demonstrating that localized receptive fields emerge in simple feedforward neural networks trained on a data model with properties meant to approximate natural visual input, in particular,\nlocality structure (statistical independence of non-collocated dimensions)\nand non-Gaussianity (higher-order cumulants are non-null).\nIn simulations, \\textcite{ingrosso2022data} tie the dynamical emergence of localization to increased tuning to higher-order statistics of the input, and demonstrate that even a single neuron is sufficient to learn a localized receptive field in this setting.\n\nIn this work, we build on the demonstration of \\textcite{ingrosso2022data} with the aim of describing the mechanisms behind the emergence of a localized receptive field in this minimal setting.\nThe higher-order input statistics that drive localization are challenging to analyze with existing tools that exploit implied Gaussianity~\\parencite{goldt2020modelling}.\nBy separating two stages of learning, we are able to derive equations for the effective early-time learning dynamics of the single neuron model that learns a localized receptive field from idealized naturalistic data.\nOur analytical model identifies a concise description of the higher-order statistics that drive emergence,\nand we validate both positive and negative predictions of this analytical model via simulations with many neurons; see \\cref{fig:sim-real-gabors} (right).\nThese findings suggest an alternative path to account for the ubiquity of localization in early neural responses as resulting from the interaction of the nonlinear dynamics of learning in neural circuits and naturalistic data with higher-order statitistical structure,\nrather than an explicit efficiency criterion.<PLACEHOLDER_sections/01-introduction_end>\n<PLACEHOLDER_figures/sim-real-gabors/fig_begin><PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_ENV_2><PLACEHOLDER_figures/sim-real-gabors/fig_end>\n<PLACEHOLDER_sections/02-model_begin>",
        "trans_content": "\\section{引言}\n\\label{sec:introduction}\n\n动物神经系统中外周反应的一个显著特征是 \\emph{局部化}——也就是说，简单细胞神经元的线性感受野通常只对远小于其整个输入域的连续区域作出反应。\n在视觉系统中，视网膜神经节细胞近似于对中心-周边区域的局部滤波器，这些滤波器铺满输入空间~\\parencite{dacey2000center,doi2012efficient,knudsen1978space}；\n而在下游的初级视觉皮层中，简单细胞具有对空间频率和方向选择性的局部滤波器~\\parencite{hubel1959receptive,hubel1968receptive,rolls1995sparseness,niell2008highly,willmore2011sparse,ringach2002orientation,ringach2002spatial}。\n在初级躯体感觉皮层中，神经元对皮肤上受限区域的刺激产生反应~\\parencite{crochet2011synaptic}；\n在初级听觉皮层中，时空感受野通常在时间和频率域中都是局部化的~\\parencite{deweese2003binary,hromadka2008sparse}；\n参见 \\cref{fig:sim-real-gabors}（左）。\n\n相比之下，人工学习系统并不总是学习出局部化的滤波器。\n主成分分析倾向于拟合跨越整个输入信号的权重，未加正则化的自编码神经网络架构和受限玻尔兹曼机也有类似表现~\\parencite{saxe2011unsupervised}。\n这一差异促使人们寻求能够从自然刺激中学习出局部感受野的人工学习模型，其中最具代表性的是稀疏编码~\\parencite{olshausen1996emergence,olshausen1997sparse} 和独立成分分析~\\parencite[ICA;~][]{bell1997independent,vanhateren1998independent}。\n稀疏编码、ICA 及其他从自然数据中产生局部感受野的压缩方法采用的是自上而下的方式——它们通过优化显式的稀疏性准则，或在关键参数条件下需要稀疏性的独立性准则，从而找到输入信号的高效表示~\\cite{field1999wavelets,saxe2011unsupervised}。\n\n尽管稀疏性作为局部化的潜在统一解释具有吸引力，但局部化也会自然地出现在训练用于分类任务的网络中，即使这些网络并未显式使用稀疏性正则项~\\parencite{krizhevsky2012imagenet,zeiler2013visualizing,yosinski2015understanding,sengupta2018manifoldtiling}；\n参见 \\cref{fig:sim-real-gabors}（中）中的一个示例。\n\\textcite{ingrosso2022data} 提炼了此类涌现性局部化的示例，并展示了在简单前馈神经网络中，训练数据模型具有近似自然视觉输入特性的前提下，局部感受野会自然涌现出来，具体包括：\n局部结构（非共址维度的统计独立性）以及非高斯性（高阶累积量非零）。\n在模拟中，\\textcite{ingrosso2022data} 将局部化的动态涌现与输入的高阶统计量调谐增强联系起来，并表明在这种设置下，甚至单个神经元也足以学习出局部感受野。\n\n在本研究中，我们在 \\textcite{ingrosso2022data} 的成果基础上进一步探究，旨在描述在该最小设置中局部感受野涌现背后的机制。\n驱动局部化的高阶输入统计特性难以用现有依赖高斯性假设的分析工具进行研究~\\parencite{goldt2020modelling}。\n通过将学习过程划分为两个阶段，我们能够推导出在理想化自然数据驱动下，单个神经元模型在早期时间段的有效学习动力学方程。\n我们的解析模型对驱动涌现的高阶统计特性给出了简洁的描述，\n并通过多神经元模拟验证了该模型的正面与负面预测；参见 \\cref{fig:sim-real-gabors}（右）。\n这些发现提出了一种解释早期神经响应中普遍存在的局部化现象的替代路径，即源于神经回路中学习的非线性动态与具有高阶统计结构的自然数据之间的相互作用，\n而非源于显式的效率准则。<PLACEHOLDER_sections/01-introduction_end>\n<PLACEHOLDER_figures/sim-real-gabors/fig_begin><PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_ENV_2><PLACEHOLDER_figures/sim-real-gabors/fig_end>\n<PLACEHOLDER_sections/02-model_begin>"
    },
    {
        "section": "2",
        "content": "\\section{Modeling approach}\n\\label{sec:prelims}\n\nWe extend the setting of \\textcite{ingrosso2022data},\na minimal example of a neural network that learns localized receptive fields\nfrom idealized naturalistic data.\nWe analyze the dynamics of learning in this setting in \\cref{sec:theory}\nand validate our analytical model with simulations in \\cref{sec:experiments}.",
        "trans_content": "\\section{建模方法}\n\\label{sec:prelims}\n\n我们扩展了 \\textcite{ingrosso2022data} 的设定，\n这是一个神经网络的最小示例，能够从理想化的自然数据中学习局部感受野。\n我们在 \\cref{sec:theory} 中分析了这个设定中学习的动态，\n并在 \\cref{sec:experiments} 中通过仿真验证了我们的分析模型。"
    },
    {
        "section": "2_1",
        "content": "\\subsection{Neural network architecture and learning algorithm}\n\\label{sec:model}\n\nWe consider a two-layer feedforward neural network with nonlinear activation and scalar output.\nWhile simple, this architecture is highly expressive, capable of approximating arbitrary\nintegrable univariate functions with appropriate scaling~\\parencite{barron1993universal, pinkus1999approximation},\nand exhibits rich feature learning dynamics that underlie the performance of models at scale~\\parencite{woodworth2020kernel},\nmaking this architecture the ongoing subject of theoretical neural network analyses~\\parencite{mei2018mean, goldt2019dynamics, veiga2022phase}.\nWe denote a two-layer network with $N$-dimensional input, $M$ hidden units, and one-dimensional scalar output as\n\\newcounter{modelenumi}\n<PLACEHOLDER_ENV_3>\nwhere $\\sigma : \\R \\to \\R$ is a pointwise nonlinearity such as the rectified linear unit (ReLU) or sigmoid function,\n$\\mathbf{w}_m^{(1)} \\in \\R^N$ and $w_m^{(2)} \\in \\R$ are learnable weights,\n$b_m^{(1)}, b^{(2)} \\in \\R$ are learnable bias terms, and\n$\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner (dot) product on $\\R^N$.\nWhen the second-layer parameters are fixed, this model is known as a \\emph{soft-committee machine}~\\parencite[SCM;][]{saad1995line},\nwhich~\\parencite{ingrosso2022data} notes learns less noisy receptive fields but exhibits similar localization behavior.\nThe many-neuron architecture in \\labelcref{item:many-neuron-model} is the focus of our \\textbf{simulations}~(\\cref{sec:experiments}), but the dynamics of this model\nare too complex to analyze directly, even for the idealized naturalistic data model considered here.\nIn order to derive \\textbf{analytical} results~(\\cref{sec:theory}), we consider the simplest neural network exhibiting the desired localization phenomenon, a single hidden neuron without bias and with rectified linear unit activation, written as\n<PLACEHOLDER_ENV_4>\nwhere $\\operatorname{ReLU}(x) = \\max(x,0)$, applied pointwise to vectorial input.\nAs \\textcite{ingrosso2022data} demonstrate, the localized receptive fields learned by the\nmany- and single-neuron models defined in \\labelcref{item:many-neuron-model,item:single-neuron-model} are qualitatively similar up to spatial translation,\nwhich permits us to generalize insights from analyzing the learning dynamics of the single-neuron~\\labelcref{item:single-neuron-model} to the many-neuron~\\labelcref{item:many-neuron-model}.\nFor simulations, we initialize the weights and biases\nas independent draws from an isotropic Gaussian distribution with scaled variance,\nand train with batch gradient descent with a fixed learning rate on the mean-squared error\n(MSE) evaluated on input-output pairs from the task; see \\cref{sec:task} for task sampling procedures.",
        "trans_content": "\\subsection{神经网络架构与学习算法}\n\\label{sec:model}\n\n我们考虑一种具有非线性激活函数和标量输出的两层前馈神经网络。尽管结构简单，该架构却具有很强的表达能力，能够在适当缩放下逼近任意可积的一元函数~\\parencite{barron1993universal, pinkus1999approximation}，并展现出丰富的特征学习动态，这些动态是大规模模型性能的基础~\\parencite{woodworth2020kernel}，使得该架构持续成为神经网络理论分析的研究对象~\\parencite{mei2018mean, goldt2019dynamics, veiga2022phase}。我们将一个输入维度为 $N$、隐藏单元数为 $M$、输出为一维标量的两层网络表示为\n\\newcounter{modelenumi}\n<PLACEHOLDER_ENV_3>\n其中，$\\sigma : \\R \\to \\R$ 是逐点非线性函数，如修正线性单元（ReLU）或 sigmoid 函数，\n$\\mathbf{w}_m^{(1)} \\in \\R^N$ 和 $w_m^{(2)} \\in \\R$ 是可学习的权重，\n$b_m^{(1)}, b^{(2)} \\in \\R$ 是可学习的偏置项，$\\langle \\cdot, \\cdot \\rangle$ 表示 $\\R^N$ 上的标准欧几里得内积（点积）。\n当第二层参数固定时，该模型被称为 \\emph{软委员会机}~\\parencite[SCM;][]{saad1995line}，\n正如~\\parencite{ingrosso2022data} 所指出的那样，该模型学习到的感受野噪声更小，但呈现出类似的局部化行为。\n在我们\\textbf{仿真实验}~(\\cref{sec:experiments})中，关注的是 \\labelcref{item:many-neuron-model} 中的多神经元架构，\n但即使在本文考虑的理想自然数据模型下，该模型的动态也过于复杂，难以直接分析。\n为了推导\\textbf{解析}结果~(\\cref{sec:theory})，我们考虑能够展现所需局部化现象的最简单神经网络：一个没有偏置项、采用 ReLU 激活函数的单隐藏神经元模型，表示为\n<PLACEHOLDER_ENV_4>\n其中 $\\operatorname{ReLU}(x) = \\max(x,0)$，逐点作用于向量输入。\n如 \\textcite{ingrosso2022data} 所示，由 \\labelcref{item:many-neuron-model,item:single-neuron-model} 所定义的多神经元与单神经元模型学习到的局部化感受野在空间平移下具有相似的定性特征，\n这使我们能够将对单神经元~\\labelcref{item:single-neuron-model} 学习动态的分析推广至多神经元模型~\\labelcref{item:many-neuron-model}。\n在仿真中，我们将权重与偏置初始化为来自具有缩放方差的各向同性高斯分布的独立样本，\n并使用固定学习率的批量梯度下降算法，在任务输入输出对上最小化均方误差（MSE）；任务采样过程见 \\cref{sec:task}。"
    },
    {
        "section": "2_2",
        "content": "\\subsection{Stimulus properties}\n\\label{sec:input}\n\nThe data model of \\textcite{ingrosso2022data}\ncan be shown to satisfy three conditions that enable the analysis we give in \\cref{sec:theory}.\nWe consider several other data models that share the below properties but differ in generative mechanism\nin order to probe the effect of these properties on localization.\nIn particular, we consider data $\\mathbf{X}$ sampled from distributions $p$ on $\\R^N$ satisfying the following:\n\\newcounter{propenumi}\n<PLACEHOLDER_ENV_5>\nProperties~\\labelcref{item:weak-dependence,item:translation-invariance}\nare defining characteristics of natural image data~\\parencite{hyvarinen2009natural}.\nProperty~\\labelcref{item:sign-symmetry}\ncan also be seen to hold for natural images after centering\nand is convenient analytically\nbecause it implies that $\\E[\\mathbf{X}] = 0$.\nProperty~\\labelcref{item:weak-dependence} assumes that $p$ is implicitly parameterized by $N$\nin order to state that the statistical dependence between entries of $\\mathbf{X}$ vanishes\nas their separation increases.\\smash{\\footnotemark}\\footnotetext{\n  The weak dependence condition in \\labelcref{item:weak-dependence} is based on strong $\\alpha$-mixing, a notion first introduced by \\cite{rosenblatt1956central}\n  to obtain a generalization of the central limit theorem, which we employ later on.\n  We choose $\\alpha$-mixing because it is easy to interpret and verify,\n  but alternative definitions of weak dependence \\parencite[\\eg][]{bardet2008dependent} can be substituted.\n}\n\nWe denote the covariance of $\\mathbf{X}$ by $\\Sigma \\triangleq \\operatorname{Cov}[\\mathbf{X}]$,\nthe square of principal-diagonal entries (the variance of each entry of $\\mathbf{X}$) by $\\sigma^2$,\nand the $i$-th row by $\\sigma_i$.\nWeak dependence (\\labelcref{item:weak-dependence}) implies that entries far from the principal diagonal of $\\Sigma$ will be 0, while translation-invariance (\\labelcref{item:translation-invariance}) implies that $\\Sigma$ is circulant (\\ie entries along each diagonal are equal) and thus identifiable by a single row; see \\cref{fig:task} (center).",
        "trans_content": "\\subsection{刺激特性}\n\\label{sec:input}\n\n\\textcite{ingrosso2022data} 的数据模型\n可以证明满足三个条件，这些条件使我们能够在 \\cref{sec:theory} 中进行分析。\n我们考虑了几个其他的数据模型，这些模型具有以下特性，但在生成机制上有所不同，\n目的是探讨这些特性对定位的影响。\n特别地，我们考虑从 $\\R^N$ 上分布 $p$ 中采样的数据 $\\mathbf{X}$，满足以下条件：\n\\newcounter{propenumi}\n<PLACEHOLDER_ENV_5>\n属性~\\labelcref{item:weak-dependence,item:translation-invariance}\n是自然图像数据的定义特征~\\parencite{hyvarinen2009natural}。\n属性~\\labelcref{item:sign-symmetry}\n也可以通过去中心化自然图像后得到，并且在分析上是方便的，\n因为它意味着 $\\E[\\mathbf{X}] = 0$。\n属性~\\labelcref{item:weak-dependence} 假设 $p$ 由 $N$ 隐式参数化，\n以便说明 $\\mathbf{X}$ 的条目之间的统计依赖随着它们的分离增大而消失。\\smash{\\footnotemark}\\footnotetext{\n  \\labelcref{item:weak-dependence} 中的弱依赖条件基于强 $\\alpha$-混合性，这是 \\cite{rosenblatt1956central} 首次提出的一个概念，\n  用于获得中心极限定理的推广，之后我们会用到。\n  我们选择 $\\alpha$-混合性，因为它容易解释和验证，\n  但也可以使用弱依赖的其他定义 \\parencite[\\eg][]{bardet2008dependent}。\n}\n\n我们将 $\\mathbf{X}$ 的协方差记为 $\\Sigma \\triangleq \\operatorname{Cov}[\\mathbf{X}]$，\n主对角线条目的平方（即每个条目的方差）记为 $\\sigma^2$，\n第 $i$ 行记为 $\\sigma_i$。\n弱依赖（\\labelcref{item:weak-dependence}）意味着 $\\Sigma$ 的主对角线远离的条目将为 0，\n而平移不变性（\\labelcref{item:translation-invariance}）意味着 $\\Sigma$ 是循环矩阵（即沿每条对角线的条目相等），\n因此可以通过单行来识别；见 \\cref{fig:task}（中）。"
    },
    {
        "section": "2_3",
        "content": "\\subsection{Lengthscale discrimination task}\n\\label{sec:task}\n\n\\textcite{ingrosso2022data} develop a minimal task for which localization emerges in a feedforward neural network: binary discrimination between inputs from two distributions that differ in the lengthscale of the correlations between their entries.\nThis lengthscale discrimination task can be seen as a pretext task for self-supervised learning~\\parencite{kolesnikov2019revisiting,chen2020simple} of representations~\\parencite[\\cf~unsupervised:][]{olshausen1996emergence,bell1997independent}.\nMore precisely, we generate data $(\\mathbf{X},Y)$ for supervised training according to\n<PLACEHOLDER_ENV_6>\nwhere $p$ is to be defined, $\\Sigma_y$ are distinct covariance matrices for each $y$, and we sample $Y$ uniformly among a set of increasing \\emph{lengthscale correlation classes} $y \\in \\{0,1,\\ldots\\}$, which correspond to the strength of correlation between distant positions.\nFor instance, in the case of two classes ($y = 0, 1$), we take $\\Sigma_0$ to be closer to $\\sigma^2 \\mathbb{I}_N$ than $\\Sigma_1$, where $\\mathbb{I}_N$ is the $N \\times N$ identity matrix and $\\sigma$ is a fixed value.\nThis construction isolates, via distinct covariance matrices per class, the second-order statistics, which we will see below enter into the learning dynamics separately from other properties of $p(\\mathbf{X})$, including, most critically, the implied marginal distributions, $p(X_i)$.\n\n\\paragraph{\\texttt{Ising}.}\\hspace{-2pt}\nThe first distribution we consider is the one-dimensional Ising model.\nIt is of interest as a distribution that satisfies \\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry} with marginals $p(X_i)$ with extreme support on $\\{ \\pm 1 \\}$,\nmaking it the distribution that promotes localization most strongly, as we will see in \\cref{sec:theory}.\nIn the absence of an external field, the Ising distribution is\n<PLACEHOLDER_ENV_7>\nwhere $J$ is a chosen pairwise interaction strength, $\\mathcal{Z}$ is the normalizing constant, and we enforce a periodic boundary constraint via $x_{N+1} \\equiv x_1$.\nAs $J$ increases, the lengthscale of the correlations in $\\mathbf{X}$ also increases.\nFor simulations, we sample from $p_\\texttt{Ising}$ using a Gibbs sampler~\\parencite{geman1984stochastic}.\nDiscrimination tasks in the simulations in\n\\cref{sec:experiments}\nuse $J_1=0.7$ (for $y=1$) and $J_0=0.3$ (for $y=0$).\n\n<PLACEHOLDER_figures/task/fig_begin><PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n\\setlength{\\tabcolsep}{4pt}\n<PLACEHOLDER_ENV_8><PLACEHOLDER_figures/task/fig_end>\n\n\\paragraph{\\texttt{NLGP}$(g)$.}\\hspace{-2pt}\nWe also consider the data model used in \\textcite{ingrosso2022data}, the nonlinear Gaussian process (NLGP), which enables one to interpolate between distributions that do and do not yield localization via a single parameter, $g$.\nA sample $\\mathbf{X} \\mid Y = y$ from the NLGP is constructed by first sampling a Gaussian $\\mathbf{Z} \\mid Y = y \\sim \\NN(0, \\tilde{\\Sigma}_y)$ and then transforming it via\n<PLACEHOLDER_ENV_9>\nwhere $\\operatorname{erf}$ is the Gauss error function, $\\mathcal{Z}$ is a normalization constant to ensure that the variances of $X_i$ and $Z_i$ are the same, and $\\tilde{\\Sigma}_y$ is a covariance matrix for $\\mathbf{Z}$, where we use $(\\tilde{\\Sigma}_y)_{ij} = \\exp(-(i-j)^2/\\xi^2)$ for a lengthscale parameter $\\xi$ \\cite{ingrosso2022data}.\nIf $g \\approx 0$ (where localization is \\emph{not} observed), $g Z_i$ will tend to lie in the linear regime of $\\operatorname{erf}$, so $Z_i$ will be untransformed, \\ie $\\mathbf{X}$ is Gaussian.\nHowever, as $g \\to \\infty$ (where localization \\emph{is} observed), $g Z_i$ will tend to saturate $\\operatorname{erf}$, so $X_i$ will have support on $\\{ \\pm 1 \\}$.\n\n\\paragraph{\\texttt{Kur}$(k)$.}\\hspace{-2pt}\nThe final family we consider is chosen to give us flexibility over the kurtosis $\\kappa$ of the marginals $p(X_i)$.\nIn the Ising model, the \\emph{excess} kurtosis ($\\kappa - 3$) of the marginals is fixed at $-2$, while in $\\texttt{NLGP}(g)$, it varies from $-2$ to $0$.\nThis family allows us to vary the excess kurtosis from negative through positive values.\nWe sample $\\mathbf{X} \\mid Y = y$ from this family via inverse transform sampling to vary the marginals while enforcing dependence via Gaussian copulas.\nMore concretely, we sample $\\mathbf{Z} \\mid Y = y \\sim \\NN(0, \\tilde{\\Sigma}_y)$ and then transform it via\n<PLACEHOLDER_ENV_10>\nwhere $\\tilde{\\sigma}$ is the standard deviation of $Z_i$, $\\Phi$ is the standard Gaussian cumulative distribution function (CDF), $f$ is the CDF of the desired marginal distribution for $X_i$, and $\\mathcal{Z}$ is a normalization constant, which we compute numerically.\nWe define $\\tilde{\\Sigma}_y$ as for $\\texttt{NLGP}$.\nWe choose $f$ to be the generalized \\emph{algebraic sigmoid} function (see \\cref{sec:algebraic-sigmoid})\nfor $k > 0$ to make use of its tractable inverse, simplifying the procedure in \\cref{eq:alg}.\nWe denote the corresponding distribution by $\\texttt{Kur}(k)$.\nThough we are able to continuously vary excess kurtosis, we lack an explicit form; however, numerical computation shows that for $k \\lessapprox 5.8$, excess kurtosis is positive, while for $k \\gtrapprox 5.9$, it is negative.<PLACEHOLDER_sections/02-model_end>\n<PLACEHOLDER_sections/03-theoretical-results_begin>",
        "trans_content": "\\subsection{长度尺度判别任务}\n\\label{sec:task}\n\n\\textcite{ingrosso2022data} 开发了一个最小化任务，在该任务中，定位在前馈神经网络中出现：在两个分布之间进行二元判别，这些分布的输入在其条目之间的相关性的长度尺度上有所不同。\n这个长度尺度判别任务可以看作是自监督学习~\\parencite{kolesnikov2019revisiting,chen2020simple} 表示的前置任务~\\parencite[\\cf~无监督:][]{olshausen1996emergence,bell1997independent}。\n更精确地说，我们根据以下方式生成数据 $(\\mathbf{X},Y)$ 用于监督训练：\n<PLACEHOLDER_ENV_6>\n其中 $p$ 待定义，$\\Sigma_y$ 是每个 $y$ 对应的不同协方差矩阵，我们从一组递增的 \\emph{长度尺度相关类} $y \\in \\{0,1,\\ldots\\}$ 中均匀抽样，表示远距离位置之间相关性的强度。\n例如，在两个类别的情况下（$y = 0, 1$），我们令 $\\Sigma_0$ 比 $\\Sigma_1$ 更接近 $\\sigma^2 \\mathbb{I}_N$，其中 $\\mathbb{I}_N$ 是 $N \\times N$ 的单位矩阵，$\\sigma$ 是一个固定值。\n这种构造通过为每个类别提供不同的协方差矩阵，隔离了二阶统计量，正如我们将在下文中看到的那样，这些统计量与 $p(\\mathbf{X})$ 的其他属性分别进入学习动态，最关键的是其隐含的边际分布 $p(X_i)$。\n\n\\paragraph{\\texttt{Ising}.}\\hspace{-2pt}\n我们考虑的第一个分布是一维伊辛模型。\n它作为一种分布具有兴趣，因为它满足 \\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry}，其边际分布 $p(X_i)$ 在 $\\{ \\pm 1 \\}$ 上有极端支持，\n使其成为最强烈促进定位的分布，正如我们将在 \\cref{sec:theory} 中看到的那样。\n在没有外部场的情况下，伊辛分布为\n<PLACEHOLDER_ENV_7>\n其中 $J$ 是选择的二元相互作用强度，$\\mathcal{Z}$ 是归一化常数，我们通过 $x_{N+1} \\equiv x_1$ 强制周期边界约束。\n随着 $J$ 的增加，$\\mathbf{X}$ 中的相关性长度尺度也增加。\n在仿真中，我们使用 Gibbs 采样器~\\parencite{geman1984stochastic} 从 $p_\\texttt{Ising}$ 中抽样。\n仿真中的判别任务在\n\\cref{sec:experiments}\n中使用 $J_1=0.7$（对于 $y=1$）和 $J_0=0.3$（对于 $y=0$）。\n\n<PLACEHOLDER_figures/task/fig_begin><PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n\\setlength{\\tabcolsep}{4pt}\n<PLACEHOLDER_ENV_8><PLACEHOLDER_figures/task/fig_end>\n\n\\paragraph{\\texttt{NLGP}$(g)$.}\\hspace{-2pt}\n我们还考虑了 \\textcite{ingrosso2022data} 中使用的数据模型——非线性高斯过程（NLGP），它通过一个单一参数 $g$ 使我们能够在会和不会导致定位的分布之间进行插值。\n从 NLGP 中，给定 $Y = y$，采样一个样本 $\\mathbf{X}$ 的构造方式是，首先从高斯分布采样 $\\mathbf{Z} \\mid Y = y \\sim \\NN(0, \\tilde{\\Sigma}_y)$，然后通过以下方式进行变换：\n<PLACEHOLDER_ENV_9>\n其中 $\\operatorname{erf}$ 是高斯误差函数，$\\mathcal{Z}$ 是归一化常数，确保 $X_i$ 和 $Z_i$ 的方差相同，$\\tilde{\\Sigma}_y$ 是 $\\mathbf{Z}$ 的协方差矩阵，我们使用 $(\\tilde{\\Sigma}_y)_{ij} = \\exp(-(i-j)^2/\\xi^2)$ 作为长度尺度参数 $\\xi$ \\cite{ingrosso2022data}。\n如果 $g \\approx 0$（即没有观察到定位），$g Z_i$ 将趋向于 $\\operatorname{erf}$ 的线性区域，因此 $Z_i$ 将不被变换，即 $\\mathbf{X}$ 是高斯分布。\n然而，当 $g \\to \\infty$（即观察到定位时），$g Z_i$ 将趋向于饱和 $\\operatorname{erf}$，因此 $X_i$ 将支持 $\\{ \\pm 1 \\}$。\n\n\\paragraph{\\texttt{Kur}$(k)$.}\\hspace{-2pt}\n我们考虑的最后一个模型家庭使我们能够灵活控制边际分布 $p(X_i)$ 的峰度 $\\kappa$。\n在伊辛模型中，边际分布的 \\emph{超额}峰度（$\\kappa - 3$）固定为 $-2$，而在 $\\texttt{NLGP}(g)$ 中，它从 $-2$ 变化到 $0$。\n这个模型家庭允许我们将超额峰度从负值变化到正值。\n我们通过逆变换采样从这个家庭中抽样 $\\mathbf{X} \\mid Y = y$，以在强制依赖关系的同时改变边际分布，使用高斯 Copula。\n更具体地说，我们首先从高斯分布中抽样 $\\mathbf{Z} \\mid Y = y \\sim \\NN(0, \\tilde{\\Sigma}_y)$，然后通过以下方式进行变换：\n<PLACEHOLDER_ENV_10>\n其中 $\\tilde{\\sigma}$ 是 $Z_i$ 的标准差，$\\Phi$ 是标准高斯累计分布函数（CDF），$f$ 是 $X_i$ 的期望边际分布的 CDF，$\\mathcal{Z}$ 是归一化常数，我们通过数值方法计算该常数。\n我们将 $\\tilde{\\Sigma}_y$ 定义为与 $\\texttt{NLGP}$ 相同。\n我们选择 $f$ 为广义的 \\emph{代数 Sigmoid} 函数（见 \\cref{sec:algebraic-sigmoid}），对于 $k > 0$，利用其可解的反函数，简化了 \\cref{eq:alg} 中的过程。\n我们将相应的分布表示为 $\\texttt{Kur}(k)$。\n尽管我们能够连续变化超额峰度，但我们没有显式的形式；然而，数值计算表明，对于 $k \\lessapprox 5.8$，超额峰度为正，而对于 $k \\gtrapprox 5.9$，它为负。<PLACEHOLDER_sections/02-model_end>\n<PLACEHOLDER_sections/03-theoretical-results_begin>"
    },
    {
        "section": "3",
        "content": "\\section{Theoretical results}\n\\label{sec:theory}\n\nWe derive an analytical model for the localization dynamics of the single-neuron architecture in \\labelcref{item:single-neuron-model}.\nThis result establishes necessary and sufficient conditions for localization under Properties~\\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry} for the minimal case of a binary response, \\ie $y = 0,1$.\nThe conditions for localization in the single-neuron architecture in \\labelcref{item:single-neuron-model} are demonstrated in \\cref{sec:experiments} to also hold empirically for the many-neuron architecture in \\labelcref{item:many-neuron-model}.\nFurther, we use this model to derive a negative prediction about localization, that the architectures in \\labelcref{item:many-neuron-model,item:single-neuron-model} fail to learn a localized receptive field on elliptical distributions\ndespite their non-Gaussian---in particular, significantly positive kurtosis---statistics~\\parencite[\\cf positive kurtosis as an objective or diagnostic for localization,][]{hyvarinen2000independent,ingrosso2022data}.",
        "trans_content": "\\section{理论结果}\n\\label{sec:theory}\n\n我们推导了\\labelcref{item:single-neuron-model}中单神经元架构的定位动态的解析模型。\n该结果建立了在性质~\\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry}下实现定位的充要条件，适用于响应为二值的最简情形，\\ie $y = 0,1$。\n\\cref{sec:experiments} 中展示了，在\\labelcref{item:single-neuron-model}中的单神经元架构的定位条件，在经验上同样适用于\\labelcref{item:many-neuron-model}中的多神经元架构。\n此外，我们利用该模型推导出一个关于定位的负向预测，即尽管这些架构具有非高斯的统计特性——尤其是显著正的峰度——它们在椭圆分布上仍未能学习出局部化的感受野~\\parencite[\\cf 正峰度作为定位的目标函数或诊断工具,][]{hyvarinen2000independent,ingrosso2022data}。"
    },
    {
        "section": "3_1",
        "content": "\\subsection{An analytical model for the dynamics of localization in a single neuron}\n\nPrevious approaches to obtain analytical dynamics in the architectures in\n\\labelcref{item:many-neuron-model,item:single-neuron-model}\nhave studied the gradient flow under the assumption that the preactivation $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ is approximately Gaussian~\\parencite{goldt2020modelling,gerace2020generalisation,goldt2022gaussian}, but this assumption fails to capture the propagation of higher-order statistics through a neural network that promotes localization~\\cite{ingrosso2022data}.\nHappily, the idealized visual input setting set out in \\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry} permits us some simplification.\nIn particular,\nthe translation-invariance of the data $\\mathbf{X}$ under \\labelcref{item:translation-invariance} and\nthe architecture of \\labelcref{item:single-neuron-model}\nallow us to work with the marginal distributions of each input dimension, $X_i$ rather than the full joint distribution of $\\mathbf{X}$.\n\nWe now give the analytical simplifications that allow us to derive an analytical model for the localization dynamics of the single neuron architecture in \\labelcref{item:single-neuron-model}, namely\ntwo assumptions on $\\mathbf{X} \\mid X_i$ for all $i \\in \\{1, \\dots, N\\}$ as a well as a mild condition on the weights that is satisfied at initialization.\nThese are, where $\\sigma_i^y$ to denotes the $i$-th row of $\\Sigma_y$:\n\n\\newcounter{assumenumi}\n<PLACEHOLDER_ENV_11>\n\nOur motivation for Assumptions~\\labelcref{item:mean-assumption,item:covariance-assumption,item:lindeberg-condition} is that they replicate the kurtosis of the marginal distributions $X_i$ (discussed further below) of two important and distinct limiting cases where localization does and does not appear, respectively:\nwhen $\\mathbf{X}$ has support on the vertices of the hypercube $\\{ \\pm 1 \\}^N$ (satisfied by \\texttt{Ising} for any $J$), and\nwhen $\\mathbf{X}$ is Gaussian (satisfied by \\texttt{NLGP} with $g \\approx 0$).\n\nThe gradient flow in Lemma~\\labelcref{lem:gradient_flow} also relies on Assumption~\\labelcref{item:lindeberg-condition} that Lindeberg's condition holds for the sequence $w_i X_i$, which ensures that no single term $w_i X_i$ in the sequence can dominate.\nIf this holds, then we can conclude that $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\mid X_i$ is approximately Gaussian.\nAs we discuss in \\cref{subsec:pf_of_gradient_flow}, this is almost always satisfied for a Gaussian initialization of $\\mathbf{w}$, and for slight deviations therefrom, and is satisfied by the settings of \\textcite{ingrosso2022data}.\nUsing this fact, we obtain an explicit form for the gradient flow early in training, stated in Lemma~\\labelcref{lem:gradient_flow}.\n\n<PLACEHOLDER_ENV_12>\nLemma~\\labelcref{lem:gradient_flow} reduces the study of higher-order statistics to the marginal distributions, $X_1$, where, by translation invariance, all marginals have the same distribution, so we refer to $X_1$ without loss of generality.\nWhile Lemma~\\labelcref{lem:gradient_flow} technically only holds early in training and breaks down if $\\mathbf{w}$ becomes localized due to violation of \\labelcref{item:lindeberg-condition}, the gradient flow in \\cref{eq:gradient_flow_early} holds sufficiently long to detect the emergence of localization in the weights $\\mathbf{w}$.\nIn particular, numerically integrating \\cref{eq:gradient_flow_early} yields localized weights $\\mathbf{w}$ as $t \\to \\infty$.\nMoreover, the location of the peak of final weights from \\cref{eq:gradient_flow_early} corresponds closely to the actual peak of the weight, when we observe localization; see \\cref{sec:peak-prediction}\nfor empirical validation of this fact.\nThe primary difference observed is that the localized bump from \\cref{eq:gradient_flow_early} is less peaked than when computed exactly; see \\cref{fig:theory} for a comparison between experimentally observed localized receptive fields and theoretical predictions.",
        "trans_content": "\\subsection{单神经元中定位动力学的分析模型}\n\n先前的研究方法在\\labelcref{item:many-neuron-model,item:single-neuron-model}的架构中研究了梯度流，假设预激活$\\langle \\mathbf{w}, \\mathbf{X} \\rangle$近似为高斯分布~\\parencite{goldt2020modelling,gerace2020generalisation,goldt2022gaussian}，但这一假设未能捕捉到通过神经网络传播的高阶统计量，这些统计量促进了定位现象~\\cite{ingrosso2022data}。幸运的是，在\\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry}中提出的理想化视觉输入设置允许我们进行一些简化。特别地，数据$\\mathbf{X}$在\\labelcref{item:translation-invariance}下的平移不变性以及\\labelcref{item:single-neuron-model}中的架构允许我们使用每个输入维度$X_i$的边际分布，而不是$\\mathbf{X}$的联合分布。\n\n我们现在给出允许我们推导单神经元架构中定位动力学的分析模型的简化假设，即对所有$i \\in \\{1, \\dots, N\\}$，$\\mathbf{X} \\mid X_i$的两个假设，以及初始化时满足的权重的温和条件。具体来说，$\\sigma_i^y$表示$\\Sigma_y$的第$i$行：\n\n\\newcounter{assumenumi}\n<PLACEHOLDER_ENV_11>\n\n假设~\\labelcref{item:mean-assumption,item:covariance-assumption,item:lindeberg-condition}的动机是，它们复制了边际分布$X_i$（稍后进一步讨论）的峰度，这对于两个重要且不同的极限情形至关重要，其中分别出现和不出现定位现象：当$\\mathbf{X}$的支持集中在超立方体的顶点上$\\{ \\pm 1 \\}^N$（对于任意$J$，\\texttt{Ising}满足此条件），以及当$\\mathbf{X}$是高斯分布时（对于$g \\approx 0$的\\texttt{NLGP}满足此条件）。\n\n引理~\\labelcref{lem:gradient_flow}中的梯度流也依赖于假设~\\labelcref{item:lindeberg-condition}，即Lindeberg条件对于序列$w_i X_i$成立，这确保了序列中的任何单一项$w_i X_i$不能占主导地位。如果这一条件成立，我们可以得出结论，$\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\mid X_i$近似为高斯分布。正如我们在\\cref{subsec:pf_of_gradient_flow}中讨论的，这几乎总是对$\\mathbf{w}$的高斯初始化成立，且对轻微偏离这一初始化的情况也成立，并且满足\\textcite{ingrosso2022data}的设置。利用这一事实，我们得到了训练初期梯度流的显式形式，如引理~\\labelcref{lem:gradient_flow}所述。\n\n<PLACEHOLDER_ENV_12>\n引理~\\labelcref{lem:gradient_flow}将高阶统计量的研究简化为边际分布$X_1$，由于平移不变性，所有边际分布具有相同的分布，因此我们可以不失一般性地讨论$X_1$。尽管引理~\\labelcref{lem:gradient_flow}严格而言只在训练初期成立，且如果$\\mathbf{w}$由于违反\\labelcref{item:lindeberg-condition}而变得定位，则该引理会失效，但\\cref{eq:gradient_flow_early}中的梯度流在足够长的时间内保持有效，足以检测到权重$\\mathbf{w}$中的定位现象。特别地，数值积分\\cref{eq:gradient_flow_early}会在$t \\to \\infty$时得到定位的权重$\\mathbf{w}$。此外，\\cref{eq:gradient_flow_early}中的最终权重峰值的位置与实际观察到的定位峰值位置非常接近；有关这一事实的经验验证，请参见\\cref{sec:peak-prediction}。观察到的主要差异是，\\cref{eq:gradient_flow_early}中的定位峰值比精确计算的峰值略为平缓；有关实验观测到的定位感受野与理论预测之间的比较，请参见\\cref{fig:theory}。"
    },
    {
        "section": "3_2",
        "content": "\\subsection{Necessary and sufficient conditions for emergent localization}\n\\label{subsec:localization_conditions}\n\nTo establish an exact threshold at which localization emerges requires solving \\cref{eq:gradient_flow_early}, which is not possible exactly for general nonlinear differential equations.\\smash{\\footnotemark}\\footnotetext{\nWe discuss a partial differential equation limit that faces similar intractabilities in \\cref{sec:pde-limit}.\n}\nNevertheless, the form of \\cref{eq:gradient_flow_early} reveals that localization is driven solely by the first term.\nIndeed, the second term depends only on the second-order statistics of the data, and so can be held fixed as $\\mathbf{X}$ is varied from a distribution that induces localization to one that does not.\nSecondly, one can see that the first term in \\cref{eq:gradient_flow_early} does not change as $\\mathbf{w}$ is scaled, in contrast to the second term.\nAs such, the second term in \\cref{eq:gradient_flow_early} serves to constrain the \\emph{scale} of $\\mathbf{w}$, distinct from localization, while the first is primarily concerned with the \\emph{shape} of $\\mathbf{w}$, and thus localization.\nThis further motivates the first term, and thus $\\varphi$,\nwhich we will refer to as the \\emph{amplifier} and which itself depends on properties of the data distribution $p(\\mathbf{X})$,\nas a focus of study for understanding localization.\n\n<PLACEHOLDER_figures/theory/fig_begin><PLACEHOLDER_NEWCOMMAND_21>\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n<PLACEHOLDER_ENV_13><PLACEHOLDER_figures/theory/fig_end>\n\nWe present an analysis of $\\varphi$ in \\cref{sec:varphi-analysis} that reveals the role of the marginal distribution of the data in driving localization.\nFor each marginal, $\\varphi(a) \\approx (\\sqrt{2/\\pi}) a$ for $a \\approx 0$.\nFor larger $a$, $\\varphi$ depends more strongly on the data distribution and can be super-linear (sub-linear), \\ie greater (smaller) than $(\\sqrt{2/\\pi}) a$.\nSuper-linear $\\varphi$ encourage entries in $\\mathbf{w}$ that are large in some neighborhood to grow faster than those that are smaller, yielding localization.\nLinear and sub-linear $\\varphi$ are the opposite, encouraging oscillatory or flat weights by suppressing neighborhoods in $\\mathbf{w}$.\nHowever, super- and sub-linearity may not hold uniformly, as $\\varphi$ can be both  over its domain (see \\cref{fig:theory}, bottom row, black line).\nAs an approximation, we consider a third-order Taylor expansion (red lines in \\cref{fig:theory}, second column), which reveals that for the canonical setting of $\\sigma^2 = 1$, \\emph{negative excess kurtosis of the marginals yields super-linearity}, while \\emph{positive excess kurtosis yields sub-linearity};\nsee \\cref{sec:varphi-analysis}.\nThis leads us to the following claim, which is validated by our simulations in \\cref{sec:experiments}:\n<PLACEHOLDER_ENV_14>\n\nAs a minimal positive example, the distribution with the most negative excess kurtosis is the symmetric Bernoulli, with a value of $-2$.\nIn our setting, this corresponds to a data vector $\\mathbf{X}$ with support on the vertices of the hypercube, $\\{ \\pm 1 \\}^N$.\nAs mentioned above, it can be seen from the law of total covariance combined with sign-symmetry that \\labelcref{item:mean-assumption,item:covariance-assumption} hold exactly.\nNote that $\\varphi$ is the same for all such distributions, which leads us to Claim~\\labelcref{thm:localization} that \\emph{any} distribution satisfying conditions \\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry} whose marginals are maximally concentrated will induce a localized receptive field in~\\labelcref{item:single-neuron-model}.\nImportantly, this claim includes the limiting case of \\textcite{ingrosso2022data}, $g\\to\\infty$ in $\\texttt{NLGP}$.\nIt also includes the Ising model as another example, corroborating an observation for restricted Boltzmann machines \\cite{harsh2020placecell} that Ising data induces localization in a learning model.\nThese claims are validated for the single-neuron model in \\cref{fig:theory} and in \\cref{sec:experiments} for the many-neuron model.",
        "trans_content": "\\subsection{涌现局部化的必要和充分条件}\n\\label{subsec:localization_conditions}\n\n要确定局部化出现的精确阈值，需要求解\\cref{eq:gradient_flow_early}，但对于一般的非线性微分方程，无法精确求解。\\smash{\\footnotemark}\\footnotetext{\n我们在\\cref{sec:pde-limit}中讨论了一个面临类似难以处理的偏微分方程极限问题。\n}\n尽管如此，\\cref{eq:gradient_flow_early}的形式表明局部化仅由第一项驱动。\n实际上，第二项仅依赖于数据的二阶统计量，因此可以在从诱导局部化的分布变化到不诱导局部化的分布时保持固定。\n其次，可以看出，\\cref{eq:gradient_flow_early}中的第一项在$\\mathbf{w}$缩放时不发生变化，而第二项则有所不同。\n因此，\\cref{eq:gradient_flow_early}中的第二项用于约束$\\mathbf{w}$的\\emph{尺度}，与局部化不同，而第一项则主要关注$\\mathbf{w}$的\\emph{形状}，因此与局部化相关。\n这进一步推动了第一项的关注，因此我们将其称为\\emph{放大器}，并且它本身依赖于数据分布$p(\\mathbf{X})$的性质，\n作为研究局部化的重点。\n\n<PLACEHOLDER_figures/theory/fig_begin><PLACEHOLDER_NEWCOMMAND_21>\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n<PLACEHOLDER_ENV_13><PLACEHOLDER_figures/theory/fig_end>\n\n我们在\\cref{sec:varphi-analysis}中对$\\varphi$进行了分析，揭示了数据的边际分布在驱动局部化中的作用。\n对于每个边际，$\\varphi(a) \\approx (\\sqrt{2/\\pi}) a$，当$a \\approx 0$时成立。\n对于较大的$a$，$\\varphi$更强烈地依赖于数据分布，可能是超线性的（亚线性），即大于（小于）$(\\sqrt{2/\\pi}) a$。\n超线性的$\\varphi$鼓励在某些邻域中较大的$\\mathbf{w}$条目增长得比较小的条目更快，从而产生局部化。\n线性和亚线性的$\\varphi$则相反，通过抑制$\\mathbf{w}$中的邻域，鼓励振荡或平坦的权重。\n然而，超线性和亚线性可能并不总是成立，因为$\\varphi$可以在其定义域内同时存在（参见\\cref{fig:theory}，底行，黑线）。\n作为一种近似方法，我们考虑了三阶泰勒展开（见\\cref{fig:theory}第二列中的红线），这揭示了对于$\\sigma^2 = 1$的典型设置，\\emph{边际的负过度峰度导致超线性}，而\\emph{正过度峰度则导致亚线性}；\n见\\cref{sec:varphi-analysis}。\n这导致了以下主张，并通过我们在\\cref{sec:experiments}中的模拟得到了验证：\n<PLACEHOLDER_ENV_14>\n\n作为一个最小的正例，具有最负过度峰度的分布是对称的伯努利分布，峰度值为$-2$。\n在我们的设置中，这对应于一个数据向量$\\mathbf{X}$，其支持在超立方体的顶点上，$\\{ \\pm 1 \\}^N$。\n如上所述，可以从总协方差法则结合符号对称性看出，\\labelcref{item:mean-assumption,item:covariance-assumption}完全成立。\n请注意，$\\varphi$对于所有此类分布是相同的，这使我们得出了\\labelcref{thm:localization}的主张，即\\emph{任何}满足条件\\labelcref{item:weak-dependence,item:translation-invariance,item:sign-symmetry}的分布，其边际最大集中，将在~\\labelcref{item:single-neuron-model}中引发局部化的感受野。\n值得注意的是，这一主张包括了\\textcite{ingrosso2022data}的极限情形，即$\\texttt{NLGP}$中的$g\\to\\infty$。\n它还包括了伊辛模型作为另一个例子，验证了对限制玻尔兹曼机的观察 \\cite{harsh2020placecell}，即伊辛数据在学习模型中引发局部化。\n这些主张在\\cref{fig:theory}中对单神经元模型进行了验证，并在\\cref{sec:experiments}中的多神经元模型中得到了验证。"
    },
    {
        "section": "3_3",
        "content": "\\subsection{Case study: Elliptical distributions fail to produce localization}\n\\label{sub:elliptical}\n\nAbove, we assume weak dependence (\\labelcref{item:weak-dependence}) as it enables a focus on how the marginals control localization.\nAs a first investigation into departures from this regime, we consider data $\\mathbf{X}$ sampled from an elliptical distribution, where weak dependence may not hold.\nWe specialize the definition of an elliptical distribution~\\parencite{frahm2004generalized} to our setting of multiple class labels and sign-symmetry:\n<PLACEHOLDER_ENV_15>\n\nThe class of elliptical distributions is broad, imposing only the constraint that the contours of the density be ellipses;\nthe multivariate Gaussian and Student-$t$ distributions are examples.\nAs such, they can vary greatly in measures of non-Gaussianity, including kurtosis, while maintaining enough structure for analytical convenience.\nProposition \\labelcref{thm:elliptical} states that training on elliptical data \\emph{prevents} localization in the single ReLU neuron model.\n<PLACEHOLDER_ENV_16>\n\nThe condition on the number $i$ such that the ratio of the $i$-th eigenvalues are the same constrains the number of Fourier components that can be non-zero in the steady state of $\\mathbf{w}$.\nWhile opaque, this requirement seems to always hold in practice, as even slight increases in length-scale correlation can dramatically change the spectrum of $\\Sigma_y$.\n\nThe proposition is surprising because it reveals that the kurtosis of the preactivation is not an appropriate metric for explaining localization.\nConsider the example of the $N$-dimensional Student-$t$ distribution with $\\nu$ degrees of freedom, $t_N(\\nu)$.\nIf $\\mathbf{X} \\sim t_N(\\nu)$, then $\\langle w, \\mathbf{X} \\rangle \\sim t_1(\\nu)$.\nNote the kurtosis of $t_1(\\nu)$ is non-zero, and can be very large or even infinite for small $\\nu$.\nThis prediction is validated in \\cref{sec:elliptical-experiments}.\nThe condition also reveals that not all symmetries in the data (here, elliptical symmetry) induce structure in the trained model weights, if localization is to be seen as a sparsity more structured than oscillatory weights~\\parencite[\\cf][]{godfrey2023symmetries}; indeed, translational symmetry (\\labelcref{item:translation-invariance}) is more relevant for localization than elliptical symmetry.<PLACEHOLDER_sections/03-theoretical-results_end>\n<PLACEHOLDER_sections/04-experimental-results_begin>",
        "trans_content": "\\subsection{案例研究：椭圆分布未能产生局部化}\n\\label{sub:elliptical}\n\n如上所述，我们假设弱依赖性（\\labelcref{item:weak-dependence}）使我们能够专注于边缘分布如何控制局部化。\n作为对这种假设的首次调查，我们考虑从椭圆分布中采样的数据 $\\mathbf{X}$，其中弱依赖性可能不成立。\n我们将椭圆分布的定义~\\parencite{frahm2004generalized}专门化到我们的多类标签和符号对称性的设置：\n<PLACEHOLDER_ENV_15>\n\n椭圆分布类是广泛的，仅要求密度的等高线是椭圆形的；多元高斯分布和Student-$t$分布就是例子。\n因此，它们在非高斯性度量（包括峰度）上可以变化很大，同时保持足够的结构以便于分析。\n命题 \\labelcref{thm:elliptical} 表明，在单个ReLU神经元模型中，训练椭圆数据会\\emph{防止}局部化。\n<PLACEHOLDER_ENV_16>\n\n对于数字 $i$ 的条件，使得 $i$-th 特征值的比率相同，限制了在$\\mathbf{w}$的稳态中可以非零的傅里叶分量的数量。\n尽管这一要求较为晦涩，但在实践中似乎总是成立，因为即使是轻微的长度尺度相关性增加，也能显著改变$\\Sigma_y$的谱。\n\n这一命题令人惊讶，因为它揭示了预激活的峰度并不是解释局部化的合适指标。\n考虑$N$维Student-$t$分布的例子，具有$\\nu$自由度，$t_N(\\nu)$。\n如果 $\\mathbf{X} \\sim t_N(\\nu)$，则 $\\langle w, \\mathbf{X} \\rangle \\sim t_1(\\nu)$。\n注意，$t_1(\\nu)$的峰度是非零的，对于小$\\nu$，峰度可以非常大甚至趋于无穷大。\n这一预测在\\cref{sec:elliptical-experiments}中得到了验证。\n这一条件还揭示了并非所有数据中的对称性（这里是椭圆对称性）都会在训练后的模型权重中产生结构，如果局部化被视为比振荡权重更具结构的稀疏性~\\parencite[\\cf][]{godfrey2023symmetries}；实际上，平移对称性（\\labelcref{item:translation-invariance}）比椭圆对称性在局部化中更为相关。<PLACEHOLDER_sections/03-theoretical-results_end>\n<PLACEHOLDER_sections/04-experimental-results_begin>"
    },
    {
        "section": "4",
        "content": "\\section{Experimental results}\n\\label{sec:experiments}\n\nWe describe experiments to validate the generalizability of the analytical results from \\cref{sec:theory}.\nWe run all experiments on a single CPU machine locally or on a compute cluster.\nSince all datasets are procedurally generated, training depends on both the model architecture and the complexity of sampling the data,\nbut is between 10 and 60 minutes for any single simulation run.",
        "trans_content": "\\section{实验结果}\n\\label{sec:experiments}\n\n我们进行了一系列实验，以验证来自 \\cref{sec:theory} 的分析结果的泛化能力。  \n所有实验均在本地的单核 CPU 机器或计算集群上运行。  \n由于所有数据集均为程序生成，训练过程依赖于模型结构以及数据采样的复杂性，  \n但每次模拟运行的训练时间在 10 到 60 分钟之间。"
    },
    {
        "section": "4_1",
        "content": "\\subsection{Validating Claim~\\labelcref{thm:localization} with positive and negative predictions}\n\\label{sec:theory-validation}\n<PLACEHOLDER_figures/elliptical/fig_begin><PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_ENV_17><PLACEHOLDER_figures/elliptical/fig_end>\n\nIn \\cref{fig:replications}, we validate Claim~\\labelcref{thm:localization} first via the single-neuron model (\\labelcref{item:single-neuron-model}) with 30 initial conditions trained across a range of excess kurtoses for the $\\texttt{NLGP}(g)$ and $\\texttt{Kur}(k)$ data models.\nWe use the inverse participation ratio (IPR), defined in \\cref{app:IPR}.\nThis measure, also used by \\textcite{ingrosso2022data}, is large when proportionally few weight dimensions ``participate'' (have large magnitude), and small when weight dimension magnitudes are more uniform.\nWe see that when $g$ and $k$ assume values that yield a negative excess kurtosis, IPR is close to its maximum of $1.0$, suggesting the weights are localized; if the excess kurtosis is positive, IPR is nearly zero, suggesting the weights are \\emph{not} localized.\nThe IPR is extremely consistent across random initializations, suggesting that localization is determined by data statistics and not initialization.\nThe trend in IPR \\vs excess kurtosis is very similar between the $\\texttt{NLGP}(g)$ and $\\texttt{Kur}(k)$ data models, demonstrating that excess kurtosis is a primary driver of localization and localization is largely independent from other properties of the data distribution.\n<PLACEHOLDER_figures/replications/fig_begin><PLACEHOLDER_ENV_18><PLACEHOLDER_figures/replications/fig_end>\n\n\\Cref{fig:theory} further validates Claim~\\labelcref{thm:localization} with specific examples.\nWe maintain constant initial conditions for our model and train on the \\texttt{Ising}, $\\texttt{NLGP}(g=0.01)$, and $\\texttt{Kur}(k=5)$ data models.\nThe marginals of the \\texttt{Ising} model have an excess kurtosis of $-2$, the smallest possible value for any distribution.\nAs a result, we see that the amplifier $\\varphi$ for \\texttt{Ising} (top left) is super-linear (the dark line exceeds the dashed light line for larger $a$), which drives localization via its role in \\cref{eq:gradient_flow_early}.\nIntegrating \\cref{eq:gradient_flow_early} with $\\varphi$ expanded via a third-order Taylor approximation (red line) yields a similar localized receptive field to that from simulation (two right panels), validating this approximation.\n\nFor the remaining distributions (middle and bottom rows) that elicit oscillatory (sinusoidal) weights,\nClaim~\\labelcref{thm:localization} is validated due to their positive excess kurtosis.\nThe dynamical steady state (far right) assumes a more negative value than in the simulation (to the left),\na difference that is the result of deviations of our \\emph{early-time} gradient flow in\n\\cref{eq:gradient_flow_early}, but these deviations remain mild enough nevertheless to recover the qualitative structure of the learned receptive field.\n\n<PLACEHOLDER_figures/extensions/fig_begin><PLACEHOLDER_ENV_19><PLACEHOLDER_figures/extensions/fig_end>",
        "trans_content": "\\subsection{通过正负预测验证 Claim~\\labelcref{thm:localization}}\n\\label{sec:theory-validation}\n<PLACEHOLDER_figures/elliptical/fig_begin><PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_ENV_17><PLACEHOLDER_figures/elliptical/fig_end>\n\n在 \\cref{fig:replications} 中，我们首先通过单神经元模型（\\labelcref{item:single-neuron-model}）验证了 Claim~\\labelcref{thm:localization}，该模型在 $\\texttt{NLGP}(g)$ 和 $\\texttt{Kur}(k)$ 数据模型的一系列超额峰度条件下进行了 30 个初始条件的训练。\n我们使用在 \\cref{app:IPR} 中定义的反参与比（IPR）。\n这一度量也被 \\textcite{ingrosso2022data} 使用，当仅有少量权重维度“参与”（具有较大幅值）时，其值较大；当权重维度的幅值分布较为均匀时，其值较小。\n我们观察到，当 $g$ 和 $k$ 的取值导致负的超额峰度时，IPR 接近其最大值 $1.0$，表明权重是局域化的；而当超额峰度为正时，IPR 几乎为零，表明权重\\emph{不}是局域化的。\nIPR 在不同随机初始化下表现出极高的一致性，表明局域化由数据统计特性决定，而非初始化。\n在 IPR 与超额峰度之间的趋势在 $\\texttt{NLGP}(g)$ 和 $\\texttt{Kur}(k)$ 数据模型中非常相似，这表明超额峰度是驱动局域化的主要因素，且局域化在很大程度上独立于数据分布的其他属性。\n<PLACEHOLDER_figures/replications/fig_begin><PLACEHOLDER_ENV_18><PLACEHOLDER_figures/replications/fig_end>\n\n\\Cref{fig:theory} 进一步通过具体示例验证了 Claim~\\labelcref{thm:localization}。\n我们在模型中保持相同的初始条件，并在 \\texttt{Ising}、$\\texttt{NLGP}(g=0.01)$ 和 $\\texttt{Kur}(k=5)$ 数据模型上进行训练。\n\\texttt{Ising} 模型的边缘分布具有 $-2$ 的超额峰度，这是任何分布可能具有的最小值。\n因此，我们观察到 \\texttt{Ising}（左上）的放大器 $\\varphi$ 是超线性的（深色实线在较大的 $a$ 上超过浅色虚线），这通过其在 \\cref{eq:gradient_flow_early} 中的作用驱动了局域化。\n将 \\cref{eq:gradient_flow_early} 中的 $\\varphi$ 通过三阶泰勒展开（红线）积分，得到的局域感受野与模拟结果（右侧两个面板）相似，验证了该近似的有效性。\n\n对于其余导致振荡（正弦）型权重的分布（中间和底部行），由于其超额峰度为正，因此 Claim~\\labelcref{thm:localization} 得到了验证。\n动态稳态（最右侧）的取值比模拟结果（其左侧）更为负，\n这一差异源于我们在 \\cref{eq:gradient_flow_early} 中\\emph{早期}梯度流的偏离，\n但这些偏离仍足够温和，因此仍能恢复学习出的感受野的定性结构。\n\n<PLACEHOLDER_figures/extensions/fig_begin><PLACEHOLDER_ENV_19><PLACEHOLDER_figures/extensions/fig_end>"
    },
    {
        "section": "4_2",
        "content": "\\subsection{Validating \\cref{eq:gradient_flow_early} with localization position prediction}\n\\label{sec:peak-prediction}\nThe simulated and integrated receptive fields in \\cref{fig:theory} demonstrate that our analytical model is able to meaningfully reproduce localization in receptive fields from neural network training.\nFor the Ising model, we see that the integration even has a peak in the exact same position as the simulation (at index $i=6$), suggesting precision in our approximation.\nIndeed, we simulated the condition in \\cref{fig:theory} for the Ising model under 28 different initial conditions (weight initializations), and found that in 26 of them (93\\%), the peaks of the integrated and simulated receptive fields matched exactly.\nIn the two cases where the peaks differed, they did so substantially (see \\cref{fig:time} for an example).",
        "trans_content": "\\subsection{通过位置预测验证\\cref{eq:gradient_flow_early}}\n\\label{sec:peak-prediction}\n在\\cref{fig:theory}中的模拟和积分感受野表明，我们的分析模型能够有意义地再现神经网络训练中的感受野定位。\n对于伊辛模型，我们看到积分甚至在与模拟相同的位置（在索引 $i=6$ 处）具有峰值，表明我们的近似具有精确性。\n事实上，我们在28种不同的初始条件（权重初始化）下模拟了\\cref{fig:theory}中的伊辛模型，并发现其中26种（93\\%）的积分和模拟感受野的峰值完全匹配。\n在两个峰值不同的情况下，它们的差异显著（参见\\cref{fig:time}中的示例）。"
    },
    {
        "section": "4_3",
        "content": "\\subsection{Validating Proposition \\ref{thm:elliptical}: Elliptical distributions fail to localize}\n\\label{sec:elliptical-experiments}\nProposition \\ref{thm:elliptical} claims that the single-neuron model (\\labelcref{item:single-neuron-model})\ntrained on elliptical data will yield sinusoidal receptive fields,\nsubject to a condition on the spectra of $\\Sigma_0$ and $\\Sigma_1$.\nWe verify this claim in \\cref{fig:elliptical} with three distinct elliptical distributions.\nThe first, $t_{40}(\\nu=3)$, gives preactivations $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ that have \\emph{infinite} kurtosis, yet our theory predicts the final receptive field will be sinusoidal.\nThis is confirmed in \\cref{fig:elliptical}, where the learned receptive field is indeed a sinusoid with period 1 and intercept at zero.\n\nWe also consider data sampled from the surface of an ellipse, which is done by fixing $R_y \\equiv 1$ in \\cref{def:elliptical}.\nHere, we observe that the learned receptive field is a near-constant function at $-0.04$ (note that $\\cos(2\\pi \\cdot 0 \\cdot x) \\equiv 1$ is a sinusoid, allowing nonzero intercepts and constant functions).\nFinally, we consider an unconventional elliptical distribution where the density of $R$ is given by\n$p_R(r) = (4e^{2r+4}) / (e^{2r}+e^{4})^{2} \\cdot \\mathbbm{1}(r \\geq 2)$.\nThis particular density places most of its mass near $r = 2$ before rapidly falling off, imposing a minimum norm on $\\mathbf{X}$ and pushing support near the surface of an ellipse.\nThis distribution, too, yields an oscillatory steady state, as shown in \\cref{fig:elliptical} (right).\nWe confirm our visual observations by fitting sinusoids to the final receptive fields and see the relative errors are quite low.",
        "trans_content": "\\subsection{验证命题 \\ref{thm:elliptical}：椭圆分布无法实现局部化}\n\\label{sec:elliptical-experiments}\n命题 \\ref{thm:elliptical} 指出，单神经元模型（\\labelcref{item:single-neuron-model}）在椭圆分布数据上训练时，将产生正弦形式的感受野，前提是 $\\Sigma_0$ 和 $\\Sigma_1$ 的谱满足一定条件。我们在 \\cref{fig:elliptical} 中通过三种不同的椭圆分布验证了该结论。\n\n第一种分布 $t_{40}(\\nu=3)$ 所产生的预激活 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ 具有 \\emph{无限} 峰度，尽管如此，我们的理论仍预测最终的感受野将为正弦形式。该结论在 \\cref{fig:elliptical} 中得到了验证，学习得到的感受野的确是一个周期为 1、截距为 0 的正弦函数。\n\n我们还考虑了另一种从椭圆曲面上采样的数据，其方式是在 \\cref{def:elliptical} 中固定 $R_y \\equiv 1$。在这种情况下，我们观察到学习得到的感受野近似为常数函数 $-0.04$（注意 $\\cos(2\\pi \\cdot 0 \\cdot x) \\equiv 1$ 是一个正弦函数，允许非零截距和常数函数）。\n\n最后，我们考虑一种非常规的椭圆分布，其半径 $R$ 的密度为 $p_R(r) = (4e^{2r+4}) / (e^{2r}+e^{4})^{2} \\cdot \\mathbbm{1}(r \\geq 2)$。该密度函数在 $r = 2$ 附近具有大部分概率质量，随后迅速衰减，从而对 $\\mathbf{X}$ 施加了一个最小范数限制，并使其支持集中于椭圆曲面附近。如 \\cref{fig:elliptical}（右）所示，该分布同样会产生震荡的稳态。\n\n我们通过对最终的感受野拟合正弦函数验证了视觉观察，发现相对误差非常小。"
    },
    {
        "section": "4_4",
        "content": "\\subsection{Extensions to many-neuron model and ICA}\n\\label{sec:extensions}\n\nAll of our analysis thus far has concerned single-neuron models with ReLU activation and without hidden-to-output or bias terms, assumptions which were made to make our analysis tractable.\nHere, we depart from that regime by considering the SCM and the full two-layer network (Model~\\labelcref{item:many-neuron-model}).\nIn \\cref{fig:extensions} (left) and (center), we train a SCM with 10 hidden units and sigmoid activation on the $\\texttt{Kur}(10)$ and $\\texttt{Kur}(4)$ datasets, which have excess kurtoses of $-0.93$ and $3.28$, respectively.\nSo, based on our single-neuron analysis, we \\emph{do} and \\emph{do not} expect to see localization for these distributions.\nIndeed, this is precisely what we observe in \\cref{fig:extensions}, where the receptive fields are sharply localized for the former distribution, while they look like low-frequency oscillations for the latter.\n\n<PLACEHOLDER_figures/multi-neuron/fig_begin><PLACEHOLDER_ENV_20><PLACEHOLDER_figures/multi-neuron/fig_end>\nIn \\cref{fig:multi-neuron}, we train many-neuron models with $N=40$ input units and $K=10$ hidden units, where all weights are learnable.\nIn general, adding flexibility in the second layer leads to more varied structure in the first layer.\nWe train on $\\texttt{Kur}(4)$ (top), which has an excess kurtosis of $3.28$, and $\\texttt{Kur}(30)$ (bottom), which has an excess kurtosis of $-1.17$.\nThe receptive fields from the former are not localized, as in the single-neuron model; however, they appear more like high-frequency oscillations than low-frequency sinusoids.\nFor $\\texttt{Kur}(30)$, where we expect localization, we see that the first three receptive fields exhibit localization, but less so than for a single neuron.\nImportantly, not all receptive fields are localized,\na result of a variable second-layer weight effectively changing the variance $\\sigma^2$ in the third-derivative term in Lemma~\\labelcref{lem:varphi}.\n\nWe further compare these predictions against ICA, another framework that has been used to model receptive fields in visual cortex.\nWe train on the $\\texttt{Kur}(3)$ dataset, which has marginals with excess kurtosis $7.66$, fitting 10 components using the FastICA implementation from scikit-learn \\parencite{hyvarinen2000independent,scikit-learn}.\nWe observe in \\cref{fig:extensions} (right) that we learn localized receptive fields; this contrasts our neural network models, which require negative excess kurtosis.\nThis stems from ICA's objective to maximize non-Gaussianity, regardless of how specifically it is done.\nThe sign of the excess kurtosis is irrelevant, so long as it is nonzero.\nThis deviation between our analytical model and ICA is an interesting avenue for future study, perhaps by validation with natural images.<PLACEHOLDER_sections/04-experimental-results_end>\n<PLACEHOLDER_sections/05-conclusions_begin>",
        "trans_content": "\\subsection{对多神经元模型和 ICA 的扩展}\n\\label{sec:extensions}\n\n到目前为止，我们的所有分析都集中在使用 ReLU 激活函数的单神经元模型，并且未考虑从隐藏层到输出层的连接或偏置项，这些假设是为了使我们的分析更易处理而做出的。\n在此，我们跳出这一假设范式，考虑 SCM 以及完整的两层网络（模型~\\labelcref{item:many-neuron-model}）。\n在 \\cref{fig:extensions}（左）和（中）中，我们在 $\\texttt{Kur}(10)$ 和 $\\texttt{Kur}(4)$ 数据集上训练了一个具有 10 个隐藏单元和 sigmoid 激活函数的 SCM，这两个数据集的超峭度分别为 $-0.93$ 和 $3.28$。\n因此，根据我们对单神经元的分析，对于这些分布，我们分别\\emph{预期会}和\\emph{不会}观察到局部化现象。\n这正是我们在 \\cref{fig:extensions} 中观察到的结果：对于前者分布，其感受野呈现出明显的局部化特征；而对于后者分布，其感受野则表现为低频振荡。\n\n<PLACEHOLDER_figures/multi-neuron/fig_begin><PLACEHOLDER_ENV_20><PLACEHOLDER_figures/multi-neuron/fig_end>\n在 \\cref{fig:multi-neuron} 中，我们训练了多个神经元模型，具有 $N=40$ 个输入单元和 $K=10$ 个隐藏单元，所有权重都是可学习的。\n一般而言，在第二层中增加灵活性会导致第一层中出现更为多样的结构。\n我们在 $\\texttt{Kur}(4)$（上）和 $\\texttt{Kur}(30)$（下）数据集上进行训练，前者的超峭度为 $3.28$，后者为 $-1.17$。\n如同在单神经元模型中一样，前者产生的感受野不具有局部化特征；然而，它们更像是高频振荡而非低频正弦波。\n对于 $\\texttt{Kur}(30)$，我们预期会出现局部化现象，实际上我们确实看到前三个感受野展现出一定程度的局部化，但其程度低于单神经元模型的表现。\n重要的是，并非所有感受野都具有局部化特征，\n这是由于第二层权重的变化在实际中改变了引理~\\labelcref{lem:varphi} 中三阶导数项中的方差 $\\sigma^2$ 所致。\n\n我们进一步将这些预测与 ICA 进行比较，后者是另一种被用于模拟视觉皮层感受野的框架。\n我们在 $\\texttt{Kur}(3)$ 数据集上进行训练，该数据集的边缘分布具有 $7.66$ 的超峭度，使用 scikit-learn 中的 FastICA 实现拟合了 10 个成分 \\parencite{hyvarinen2000independent,scikit-learn}。\n我们在 \\cref{fig:extensions}（右）中观察到学习到的感受野是局部化的；这与我们的神经网络模型形成对比，后者需要负的超峭度才能实现局部化。\n这种差异源于 ICA 的目标是最大化非高斯性，而不关心实现该目标的具体方式。\n超峭度的符号在此并不重要，只要它不是零即可。\n我们的解析模型与 ICA 之间的这种偏离是一个有趣的未来研究方向，或许可以通过对自然图像的验证来加以探究。<PLACEHOLDER_sections/04-experimental-results_end>\n<PLACEHOLDER_sections/05-conclusions_begin>"
    },
    {
        "section": "5",
        "content": "\\section{Conclusions}\n\\label{sec:conclusions}\n\nWe derive effective learning dynamics for the minimal example of emergent localization in a neural receptive field given by \\textcite{ingrosso2022data}.\nThe analytical approach we take relies on the assumption that the \\emph{conditional} preactivation is Gaussian, a refinement of previous work that assumes Gaussianity of the unconditioned preactivation as asserted by the \\emph{Gaussian equivalence property} targeted by \\textcite{ingrosso2022data}.\nThis approach may prove extensible beyond our specialized setting and may enable further analysis of how statistics of an input task drive emergent structure in neural network learning.\n\nEmergence as an alternative mechanism to top-down constraints like sparsity\nis in line with recent work that reformulates data-distributional properties as a driver for complex behavior~\\parencite{chan2022data}.\nVia these analytical effective dynamics, we observe that specific data properties---the covariance structure\nand the marginals---shape localization in neural receptive fields.\nThough we cannot capture dynamical interactions between neurons that may shape receptive fields in other settings with the single-neuron analytical model, our empirical validations with many neurons suggest that these interactions do not, in fact, play a significant role in shaping localization~\\cite[\\cf][]{harsh2020placecell}.\n\nThe data model we consider is a simplified abstraction of the task faced by early sensory systems, and, as a consequence, we do not yet capture certain features of receptive fields that are observed in early sensory systems.\nIn particular, we do not observe orientation nor phase selectivity, features of simple-cell receptive fields in early sensory cortices and in artificial neural networks that can be seen in a subset of receptive fields in \\cref{fig:sim-real-gabors} (left and center, respectively).\nTo capture orientation selectivity, it may be fruitful to follow the approach of \\textcite{karklin2011efficient}, who tie orientation selectivity in a population-based efficient-coding framework to the presence of noise.\nFurthermore, on-center-off-surround-filtering input data, including the idealized data, gives receptive fields with subfields in our simulations, but is difficult to analyze.\nLastly, we do not yet look at the distribution of receptive field shapes and do not validate against other models of receptive field learning beyond a brief comparison with ICA~\\parencite[\\cf][]{saxe2011unsupervised}, but these are exciting avenues for future work.<PLACEHOLDER_sections/05-conclusions_end>\n\n\\clearpage\n<PLACEHOLDER_paratext/acks_begin>",
        "trans_content": "\\section{结论}\n\\label{sec:conclusions}\n\n我们推导了神经感受野中涌现局部化的最小示例的有效学习动态，如 \\textcite{ingrosso2022data} 所述。\n我们采用的分析方法依赖于以下假设：\\emph{条件}前激活是高斯分布的，这是一种对之前工作假设的改进，之前的工作假设无条件前激活是高斯分布的，如 \\textcite{ingrosso2022data} 所提到的\\emph{高斯等效性属性}。\n这一方法可能能够扩展到我们专门的设置之外，并且可能为进一步分析输入任务的统计特性如何推动神经网络学习中的涌现结构提供了可能。\n\n作为与稀疏性等自上而下约束的替代机制，涌现机制与近期的工作一致，后者重新将数据分布特性作为复杂行为的驱动力~\\parencite{chan2022data}。\n通过这些分析有效动态，我们观察到特定的数据属性——协方差结构和边际——在神经感受野中的局部化起到了塑造作用。\n尽管我们无法通过单神经元分析模型捕捉神经元之间可能影响感受野的动态相互作用，但我们对多个神经元的实证验证表明，这些相互作用实际上在局部化的形成中并不起重要作用~\\cite[\\cf][]{harsh2020placecell}。\n\n我们考虑的数据模型是对早期感觉系统面临的任务的简化抽象，因此，我们尚未捕捉到在早期感觉系统中观察到的感受野的某些特征。\n特别是，我们没有观察到定向或相位选择性，这是早期感觉皮层和人工神经网络中简单细胞感受野的特征，能够在 \\cref{fig:sim-real-gabors} 中某些感受野的子集中看到（分别位于左侧和中间）。\n为了捕捉定向选择性，跟随 \\textcite{karklin2011efficient} 的方法可能会有所帮助，后者将基于群体的高效编码框架中的定向选择性与噪声的存在联系起来。\n此外，基于中心-周围过滤的输入数据，包括理想化数据，在我们的仿真中给出了具有子区域的感受野，但这一点难以分析。\n最后，我们尚未研究感受野形状的分布，也没有与其他感受野学习模型进行验证，除了与 ICA 的简短比较~\\parencite[\\cf][]{saxe2011unsupervised}，但这些是未来工作的令人兴奋的方向。<PLACEHOLDER_sections/05-conclusions_end>\n\n\\clearpage\n<PLACEHOLDER_paratext/acks_begin>"
    },
    {
        "section": "6",
        "content": "\\section*{Acknowledgements}\nThis work was supported by a Schmidt Science Polymath Award to A.S., and the Sainsbury Wellcome Centre Core Grant from Wellcome (219627/Z/19/Z) and the Gatsby Charitable Foundation (GAT3850). A.S. is a CIFAR Azrieli Global Scholar in the Learning in Machines \\& Brains program.<PLACEHOLDER_paratext/acks_end>\n\n\\printbibliography\n\n\\clearpage\n\\appendix\n<PLACEHOLDER_sections/a1-proofs_begin>",
        "trans_content": "\\section*{致谢}\n本研究得到了 A.S. 所获得的施密特科学多才奖的资助，以及来自惠康基金会（219627/Z/19/Z）和盖茨比慈善基金会（GAT3850）对桑斯伯里惠康中心核心项目资助的支持。A.S. 是 CIFAR 阿兹列利全球学者，隶属于“机器与大脑中的学习”项目。<PLACEHOLDER_paratext/acks_end>\n\n\\printbibliography\n\n\\clearpage\n\\appendix\n<PLACEHOLDER_sections/a1-proofs_begin>"
    },
    {
        "section": "7+7_1+7_2",
        "content": "\\section{Definitions and Notation}\n\n\n\\subsection{Notation}\nWe use $[n]$ to refer to the set $\\{ i \\in \\N : 1 \\leq i \\leq n \\}$.\n\n\n\\subsection{Algebraic sigmoid}\n\\label{sec:algebraic-sigmoid}\nFor $k > 0$, the generalized algebraic sigmoid function is defined as\n<PLACEHOLDER_ENV_21>\nFollowing the main text, we drop the subscript when $k = 2$.",
        "trans_content": "\\section{定义与符号}\n\n\\subsection{符号}\n我们用 $[n]$ 来表示集合 $\\{ i \\in \\N : 1 \\leq i \\leq n \\}$。\n\n\\subsection{代数sigmoid函数}\n\\label{sec:algebraic-sigmoid}\n对于 $k > 0$，广义代数sigmoid函数定义为\n<PLACEHOLDER_ENV_21>\n在后续的正文中，当 $k = 2$ 时，我们省略下标。"
    },
    {
        "section": "7_3",
        "content": "\\subsection{Inverse participation ratio (IPR)}\n\\label{app:IPR}\nThe IPR is defined as:\n$$ \\operatorname{IPR}(\\mathbf{w}) \\triangleq \\left(\\sum_{i=1}^D w_i^4\\right)/\\left(\\sum_{i=1}^D w_i^2\\right)^2, $$\nwhere $w_i$ is the magnitude of dimension $i$ of weight $\\mathbf{w}$.",
        "trans_content": "\\subsection{反参与比（IPR）}  \n\\label{app:IPR}  \n反参与比（IPR）定义为：  \n$$ \\operatorname{IPR}(\\mathbf{w}) \\triangleq \\left(\\sum_{i=1}^D w_i^4\\right)/\\left(\\sum_{i=1}^D w_i^2\\right)^2, $$  \n其中，$w_i$ 表示权重 $\\mathbf{w}$ 在第 $i$ 个维度上的幅值。"
    },
    {
        "section": "8+8_1",
        "content": "\\section{Extensions beyond the scope of the main text}\n\n\n\\subsection{Analytical properties of the amplifier $\\varphi$}\n\\label{sec:varphi-analysis}\n\nWe present several properties of the amplifying function $\\varphi$\ndefined in Lemma~\\labelcref{lem:varphi}.\n\n<PLACEHOLDER_ENV_22>\n\\vspace{-6pt}\n\nTo gain some understanding of how the marginal distributions of $\\mathbf{X}$ impact localization, we use the derivatives in Lemma~\\labelcref{lem:varphi}\nto construct a third-order Taylor approximation of $\\varphi$ about 0.\nThe derivatives in Lemma~\\labelcref{lem:varphi} reveal that every distribution for $X_1$ with constant variance will look like the same linear function near 0.\n$\\varphi$ only looks nonlinear once we move sufficiently far away from zero when the third-order term becomes relevant.\nFor the case of $\\sigma^2 = 1$ (where the variance of $X_1$ is equal to the value of the larger target $y=1$) the third order term suggests that $\\varphi$ is super-linear when $\\kappa < 3$, \\ie the excess kurtosis is positive, and sub-linear otherwise.\n\nA super-linear $\\varphi$ will encourage entries where $\\Sigma_1 \\mathbf{w}$ is large to grow at a faster rate than other entries, which are all subject to the same \\emph{linear} norm constraint through the second term in \\cref{eq:gradient_flow_early}.\nThe covariance $\\Sigma_1$, as a circulant matrix, acts as the convolution operator between some vector and $\\sigma_1^1$ (the first row in $\\Sigma_1$).\nSince $y=1$ corresponds to the class with a larger length-scale correlation, $\\sigma_1^1$ will decay relatively slowly and act like a weight local average.\nThus, $\\Sigma_1 \\mathbf{w}$ is the weighted local average for each entry in $\\mathbf{w}$.\nSo, entries where $\\mathbf{w}$ is large within some neighborhood will be encouraged to grow faster than those which are smaller, an effect that compounds as \\cref{eq:gradient_flow_early} is integrated.\nThus, super-linearity encourages localization.\n\nAs we will see in \\cref{thm:elliptical} for the setting of elliptical data, if $\\varphi$ is linear, $\\mathbf{w}$ learns to be sinusoidal, and thus not localized.\nIn the case of sub-linearity, we expect suppression of larger values, rather than promotion, as in the super-linear setting.\nThus, to a first approximation, the sign of the excess kurtosis, $\\kappa - 3$ (for $\\sigma^2 = 1$), indicates whether $\\mathbf{w}$ localizes.\n\nHowever, simply studying $\\varphi'''(0)$ is not sufficient to fully characterize how the marginals impact localization.\nA function can be sub-linear for small $a$ and super-linear for larger $a$, making it unclear whether it will yield localization.\nFor marginal distributions where $\\kappa \\approx 3$ that do not exhibit strict super- or sub-linearity, this condition is no longer precise enough to determine whether we see localization.",
        "trans_content": "\\section{超出主文范围的扩展}\n\n\\subsection{放大器 $\\varphi$ 的解析性质}\n\\label{sec:varphi-analysis}\n\n我们展示了在引理~\\labelcref{lem:varphi} 中定义的放大函数 $\\varphi$ 的几个性质。\n\n<PLACEHOLDER_ENV_22>\n\\vspace{-6pt}\n\n为了更好地理解 $\\mathbf{X}$ 的边际分布如何影响定位，我们使用引理~\\labelcref{lem:varphi} 中的导数构造了关于 0 的 $\\varphi$ 的三阶泰勒近似。\n引理~\\labelcref{lem:varphi} 中的导数表明，对于具有恒定方差的 $X_1$ 的每个分布，在 0 附近将表现为相同的线性函数。\n只有当我们远离零时，第三阶项才变得重要，$\\varphi$ 才表现为非线性。\n在 $\\sigma^2 = 1$ 的情况下（即 $X_1$ 的方差等于较大目标值 $y=1$），第三阶项表明当 $\\kappa < 3$ 时，$\\varphi$ 是超线性的，即过度峰度是正的，反之则为亚线性。\n\n超线性的 $\\varphi$ 会鼓励那些 $\\Sigma_1 \\mathbf{w}$ 较大的项比其他项以更快的速度增长，后者都通过 \\cref{eq:gradient_flow_early} 中的第二项受到相同的 \\emph{线性} 范数约束。\n协方差矩阵 $\\Sigma_1$ 作为循环矩阵，充当某些向量与 $\\sigma_1^1$（$\\Sigma_1$ 的第一行）之间的卷积算子。\n由于 $y=1$ 对应于具有较大长度尺度相关性的类别，$\\sigma_1^1$ 会相对缓慢地衰减，像是一个加权的局部平均。\n因此，$\\Sigma_1 \\mathbf{w}$ 是 $\\mathbf{w}$ 中每个条目的加权局部平均。\n因此，在某些邻域内，$\\mathbf{w}$ 较大的条目将被鼓励比较小的条目增长得更快，这一效应会随着 \\cref{eq:gradient_flow_early} 的积分而加剧。\n因此，超线性促使定位。\n\n正如我们将在引理~\\ref{thm:elliptical} 中看到的，对于椭圆形数据的设置，如果 $\\varphi$ 是线性的，$\\mathbf{w}$ 会学习为正弦函数，因此不会定位。\n在亚线性情况下，我们预期较大值会被抑制，而不是像超线性情况下那样被促进。\n因此，首先近似地，过度峰度 $\\kappa - 3$（对于 $\\sigma^2 = 1$）的符号表明 $\\mathbf{w}$ 是否定位。\n\n然而，仅仅研究 $\\varphi'''(0)$ 并不足以充分表征边际分布如何影响定位。\n一个函数可能在小的 $a$ 下是亚线性的，而在较大的 $a$ 下是超线性的，这使得无法明确判断它是否会导致定位。\n对于那些 $\\kappa \\approx 3$ 且没有表现出严格的超线性或亚线性的边际分布，这一条件已不再足够精确，无法确定是否会发生定位。"
    },
    {
        "section": "8_2",
        "content": "\\subsection{PDE limit of \\cref{eq:gradient_flow_early}}\n\\label{sec:pde-limit}\n\nBy taking $N$ to be large and treating $w$ as a continuous function with respect to position, \\ie $w \\equiv w(x, t)$, one can treat \\cref{eq:gradient_flow_early} as a partial differential equation (PDE).\nFinding its steady state then amounts to solving\n<PLACEHOLDER_ENV_23>\nwhere $w : [0,1] \\to \\R$ is periodic and $\\sigma^y$ is the convolution corresponding to the limiting case of the matrix $\\Sigma_y$.\nThis equation does not appear to have an explicit solution for non-identity $\\Sigma_1$,\nand thus, it may not be possible to solve the steady states of \\cref{eq:gradient_flow_early} exactly in this PDE limit or for finite $N$.",
        "trans_content": "\\subsection{方程 \\cref{eq:gradient_flow_early} 的 PDE 极限}\n\\label{sec:pde-limit}\n\n通过将 $N$ 取为较大并将 $w$ 视为与位置相关的连续函数，即 $w \\equiv w(x, t)$，可以将 \\cref{eq:gradient_flow_early} 视为偏微分方程（PDE）。  \n求解其稳态等价于求解  \n<PLACEHOLDER_ENV_23>  \n其中 $w : [0,1] \\to \\R$ 是周期性的，$\\sigma^y$ 是与矩阵 $\\Sigma_y$ 极限情况对应的卷积。  \n这个方程似乎没有非单位矩阵 $\\Sigma_1$ 的显式解，因此，在这个 PDE 极限或对于有限 $N$ 时，可能无法精确求解 \\cref{eq:gradient_flow_early} 的稳态。"
    },
    {
        "section": "8_3",
        "content": "\\subsection{Assumptions~\\labelcref{item:mean-assumption,item:covariance-assumption}\n\\vs Gaussian equivalence}\n\nAssumptions~\\labelcref{item:mean-assumption,item:covariance-assumption} are equivalent to approximating $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\mid X_i$ as Gaussian early in training.\nSimilar ideas have been used to derive gradient flow dynamics for neural networks, including in developing the Gaussian equivalence property of \\cite{goldt2020modelling,gerace2020generalisation,goldt2022gaussian}.\nHowever, these works model the unconditional preactivation $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ as a Gaussian, rather than first conditioning on $X_i$.\nHow this arises is that these previous works assert the Gaussian approximation \\emph{prior} to differentiating the loss function for the gradient flow dynamics.\nHowever, an approximation at that stage neglects the presence of a multiplicative factor $\\mathbf{X}$ that appears as a result of the chain rule applied to $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$.\nAbstractly, this approach assumes that $\\LL_\\text{exact} \\to \\LL_\\text{Gauss}$ implies $\\nabla_\\mathbf{w} \\LL_\\text{exact} \\to \\nabla_\\mathbf{w} \\LL_\\text{Gauss}$, but, in general, this does not follow, and here in particular this assumption does not capture the interplay of learning and higher-order input statistics.\nThis contributes to the failure of Gaussian equivalence in \\cite{ingrosso2022data}.\nIn contrast, we can account for the additional $\\mathbf{X}$ term in the derivation of\nLemma~\\labelcref{lem:gradient_flow} by assuming that $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\mid X_i$ rather than $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ is Gaussian.\nThis conditioning approach, along with the translation invariance of the data (Property~\\labelcref{item:translation-invariance}), also motivates considering the marginal distributions $X_i$ as the object of study to obtain gradient flows for neural networks trained on non-Gaussian inputs.",
        "trans_content": "\\subsection{假设~\\labelcref{item:mean-assumption,item:covariance-assumption} \\vs 高斯等价性}\n\n假设~\\labelcref{item:mean-assumption,item:covariance-assumption} 等价于在训练初期将 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\mid X_i$ 近似为高斯分布。\n类似的思想已被用于推导神经网络的梯度流动力学，包括在建立高斯等价性性质的研究中，如 \\cite{goldt2020modelling,gerace2020generalisation,goldt2022gaussian}。\n然而，这些工作建模的是无条件的预激活项 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ 为高斯分布，而不是首先对 $X_i$ 进行条件化。\n其产生的原因在于，这些先前的工作是在对损失函数进行梯度流动力学求导 \\emph{之前} 就提出了高斯近似。\n但在该阶段进行近似会忽略一个乘性因子 $\\mathbf{X}$，这是由于链式法则作用于 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ 所导致的。\n从抽象角度来看，这种方法假设 $\\LL_\\text{exact} \\to \\LL_\\text{Gauss}$ 推出 $\\nabla_\\mathbf{w} \\LL_\\text{exact} \\to \\nabla_\\mathbf{w} \\LL_\\text{Gauss}$，但一般来说这一推理并不成立，尤其在此处，这一假设未能体现学习过程与高阶输入统计量之间的相互作用。\n这也是 \\cite{ingrosso2022data} 中高斯等价性失败的一个原因。\n相比之下，在推导引理~\\labelcref{lem:gradient_flow} 时，我们可以通过假设 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\mid X_i$ 而不是 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ 是高斯分布，从而考虑额外的 $\\mathbf{X}$ 项。\n这种条件化的方法，加之数据的平移不变性（性质~\\labelcref{item:translation-invariance}），也进一步激发了将边缘分布 $X_i$ 作为研究对象的动机，以获得适用于非高斯输入训练的神经网络的梯度流。"
    },
    {
        "section": "9+9_1",
        "content": "\\section{Proofs of theoretical results}\n\\label{app:proofs}\n\n\n\\subsection{Gradient flow for mean-squared error (MSE) loss}\nThe loss is given by:\n<PLACEHOLDER_ENV_24>\nThe assumption of sign symmetry (\\labelcref{item:sign-symmetry}) gives that\n$\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ is also sign-symmetric.\nFirst, this implies that $\\PR( \\langle \\mathbf{w}, \\mathbf{X} \\rangle > 0 ) = \\frac{1}{2}$, so:\n<PLACEHOLDER_ENV_25>\nSecond, sign-symmetry of $\\langle \\mathbf{w}, \\mathbf{X} \\rangle$\nimplies that we can drop the conditioning on $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\geq 0$, since $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\overset{d}{=} -\\langle \\mathbf{w}, \\mathbf{X} \\rangle$.\nThus,\n<PLACEHOLDER_ENV_26>\nwhere $K$ is the number of values (classes) of discrete $y$.\nFinally, we differentiate $\\LL$ with respect to $\\mathbf{w}$:\n<PLACEHOLDER_ENV_27>\nThe gradient flow~\\cite{elkabetz2024continuous} is given by $\\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = -\\tau \\nabla_\\mathbf{w} \\LL$, where $\\tau$ is the learning rate.\nThus,\n<PLACEHOLDER_ENV_28>",
        "trans_content": "\\section{理论结果的证明}\n\\label{app:proofs}\n\n\\subsection{均方误差（MSE）损失的梯度流}\n损失函数由以下给出：\n<PLACEHOLDER_ENV_24>\n符号对称性假设（\\labelcref{item:sign-symmetry}）表明\n$\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ 也具有符号对称性。\n首先，这意味着 $\\PR( \\langle \\mathbf{w}, \\mathbf{X} \\rangle > 0 ) = \\frac{1}{2}$，所以：\n<PLACEHOLDER_ENV_25>\n其次，$\\langle \\mathbf{w}, \\mathbf{X} \\rangle$ 的符号对称性\n意味着我们可以去掉条件 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\geq 0$，因为 $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\overset{d}{=} -\\langle \\mathbf{w}, \\mathbf{X} \\rangle$。\n因此，\n<PLACEHOLDER_ENV_26>\n其中 $K$ 是离散的 $y$ 的值（类别）数。\n最后，我们对 $\\mathbf{w}$ 对损失函数 $\\LL$ 求导：\n<PLACEHOLDER_ENV_27>\n梯度流~\\cite{elkabetz2024continuous}由 $\\frac{\\mathrm{d}\\mathbf{w}}{\\mathrm{d}t} = -\\tau \\nabla_\\mathbf{w} \\LL$ 给出，其中 $\\tau$ 是学习率。\n因此，\n<PLACEHOLDER_ENV_28>"
    },
    {
        "section": "9_2",
        "content": "\\subsection{Proof of lemma \\ref{lem:gradient_flow}} \\label{subsec:pf_of_gradient_flow}\nSetting $K = 2$ in equation \\eqref{eq:gradient_flow_two}, we have\n<PLACEHOLDER_ENV_29>\nWe wish to express the first term explicitly.\nNote that the first term is a vector in $\\R^N$.\nWe consider each of its entries separately by\nusing the law of total expectation to write the $i$-th entry as:\n<PLACEHOLDER_ENV_30>\n\nBy Assumption~\\labelcref{item:lindeberg-condition}, $\\{ w_i X_i \\mid 1 \\leq i \\leq N \\}$ satisfies Lindeberg's condition as $N\\to\\infty$.\nThis is also known as a \\emph{uniform integrability} requirement.\nBefore formally stating it, let us introduce two variables: $S_N \\triangleq \\sum_{j=1}^{N} w_j (X_j - \\mu_{j\\mid x_i})$, the partial sums, and their variance, $\\sigma_N^2 \\triangleq \\E[ S_N^2 ]$, where $\\mu_{j \\mid x_i} \\triangleq \\E[X_j \\mid X_i = x_i]$ is the conditional mean of the $j$-entry given that $i$-th entry has value $x_i$.\nThen, Lindeberg's condition is formally stated as\n<PLACEHOLDER_ENV_31>\nThis condition effectively states that no term in the partial sum $w_i X_i$ will dominate.\nUnder this condition, along with weak dependence (Property~\\labelcref{item:weak-dependence}), we conclude from \\cite[Theorems 1.19, 10.2]{bradley2007introduction} that $S_N / \\sigma_N \\overset{d}{\\to} \\NN(0,1)$.\nNote that\n<PLACEHOLDER_ENV_32>\nwhere $\\mathbf{w}, \\mathbf{X}$ are $N$-dimensional vectors, $\\mathbf{\\mu}_{\\mid x_i} = \\E[\\mathbf{X} \\mid X_i = x_i]$ is the vector of conditional means, and $\\Sigma_{\\mid x_i}^{1} \\triangleq \\text{Cov}[\\mathbf{X} \\mid X_i = x_i, Y = 1] = \\Sigma_1 - \\sigma_i^1 \\sigma_i^{1\\top}$.\nSince $\\sigma_N$ and $\\mathbf{\\mu}_{\\mid x_i}$ are constant, we can write\n<PLACEHOLDER_ENV_33>\nwhere the second step, in which we acquire $o_N(1)$, follows from the definition of convergence in distribution.\nUnder Assumptions \\labelcref{item:mean-assumption,item:covariance-assumption}, we may express this as\n<PLACEHOLDER_ENV_34>\nTherefore,\n<PLACEHOLDER_ENV_35>\nDefining $\\varphi_i(a) \\triangleq \\E_{X_i \\mid Y=1}[ X_i \\operatorname{erf}(X_i \\operatorname{alg}^{-1}(a) / \\sqrt{2}) ]$ we can write\n<PLACEHOLDER_ENV_36>\nNote that $\\E_{X_i}[|X_i|] \\leq \\sqrt{ \\E_{X_i}[X_i^2] } = \\sigma$ by Cauchy-Schwarz.\nSo, $\\E_{X_i}[|X_i|] o_N(1) = o_N(1)$.\nMoreover, by translation-invariance, all $X_i$ have the same marginal, so $\\varphi_i \\equiv \\varphi_1 \\triangleq \\varphi$.\nThus,\n<PLACEHOLDER_ENV_37>\nThis form holds for all entries $i$.\nConcatenating them, we obtain\n<PLACEHOLDER_ENV_38>\nwhich gives the desired result.\n\\qed",
        "trans_content": "\\subsection{引理 \\ref{lem:gradient_flow} 的证明} \\label{subsec:pf_of_gradient_flow}\n在方程 \\eqref{eq:gradient_flow_two} 中设定 $K = 2$，我们得到\n<PLACEHOLDER_ENV_29>\n我们希望显式地表示第一项。\n注意，第一项是 $\\R^N$ 中的一个向量。\n我们通过使用全期望法则，分别考虑每一项，写出第 $i$ 项为：\n<PLACEHOLDER_ENV_30>\n\n根据假设~\\labelcref{item:lindeberg-condition}，$\\{ w_i X_i \\mid 1 \\leq i \\leq N \\}$ 满足 Lindeberg 条件，当 $N \\to \\infty$ 时。\n这也被称为 \\emph{一致可积性} 要求。\n在正式阐述之前，首先引入两个变量：$S_N \\triangleq \\sum_{j=1}^{N} w_j (X_j - \\mu_{j\\mid x_i})$，部分和，以及它们的方差，$\\sigma_N^2 \\triangleq \\E[ S_N^2 ]$，其中 $\\mu_{j \\mid x_i} \\triangleq \\E[X_j \\mid X_i = x_i]$ 是在给定第 $i$ 项值为 $x_i$ 时，第 $j$ 项的条件均值。\n然后，Lindeberg 条件被正式表述为\n<PLACEHOLDER_ENV_31>\n该条件实际上表明，部分和中的任何项 $w_i X_i$ 都不会占主导地位。\n在该条件下，结合弱依赖性（性质~\\labelcref{item:weak-dependence}），我们根据 \\cite[定理 1.19, 10.2]{bradley2007introduction} 得出 $S_N / \\sigma_N \\overset{d}{\\to} \\NN(0,1)$。\n注意到\n<PLACEHOLDER_ENV_32>\n其中，$\\mathbf{w}, \\mathbf{X}$ 是 $N$ 维向量，$\\mathbf{\\mu}_{\\mid x_i} = \\E[\\mathbf{X} \\mid X_i = x_i]$ 是条件均值的向量，而 $\\Sigma_{\\mid x_i}^{1} \\triangleq \\text{Cov}[\\mathbf{X} \\mid X_i = x_i, Y = 1] = \\Sigma_1 - \\sigma_i^1 \\sigma_i^{1\\top}$。\n由于 $\\sigma_N$ 和 $\\mathbf{\\mu}_{\\mid x_i}$ 是常数，我们可以写作\n<PLACEHOLDER_ENV_33>\n其中第二步，得到 $o_N(1)$，是由分布收敛的定义得出的。\n在假设 \\labelcref{item:mean-assumption,item:covariance-assumption} 下，我们可以将其表示为\n<PLACEHOLDER_ENV_34>\n因此，\n<PLACEHOLDER_ENV_35>\n定义 $\\varphi_i(a) \\triangleq \\E_{X_i \\mid Y=1}[ X_i \\operatorname{erf}(X_i \\operatorname{alg}^{-1}(a) / \\sqrt{2}) ]$，我们可以写作\n<PLACEHOLDER_ENV_36>\n注意，由于 Cauchy-Schwarz 不等式，$\\E_{X_i}[|X_i|] \\leq \\sqrt{ \\E_{X_i}[X_i^2] } = \\sigma$。\n所以，$\\E_{X_i}[|X_i|] o_N(1) = o_N(1)$。\n此外，由于平移不变性，所有的 $X_i$ 具有相同的边际分布，因此 $\\varphi_i \\equiv \\varphi_1 \\triangleq \\varphi$。\n因此，\n<PLACEHOLDER_ENV_37>\n这个形式对于所有项 $i$ 都成立。\n将它们连接起来，我们得到\n<PLACEHOLDER_ENV_38>\n从而得到所需的结果。\n\\qed"
    },
    {
        "section": "9_3",
        "content": "\\subsection{Proof of lemma \\ref{lem:varphi}} \\label{subsec:pf_of_varphi}\nProperty 1 follows from the fact that $\\operatorname{alg}$ and $\\operatorname{erf}$ are odd functions.\nThis implies Property 4 since an odd function must have zero for even Taylor coefficients.\n\nProperties 2 and 3 follow from differentiating \\cref{eq:varphi}.\nThe first derivative is given by\n<PLACEHOLDER_ENV_39>\nSetting $a = 0$, we get\n<PLACEHOLDER_ENV_40>\nThe third derivative is given by\n<PLACEHOLDER_ENV_41>\nAgain setting $a = 0$ gives\n<PLACEHOLDER_ENV_42>\n\\qed",
        "trans_content": "\\subsection{引理 \\ref{lem:varphi} 的证明} \\label{subsec:pf_of_varphi}\n性质 1 来源于 $\\operatorname{alg}$ 和 $\\operatorname{erf}$ 是奇函数这一事实。这意味着性质 4 得以成立，因为奇函数的偶次 Taylor 系数必须为零。\n\n性质 2 和 3 来源于对 \\cref{eq:varphi} 求导。第一导数为\n<PLACEHOLDER_ENV_39>\n设置 $a = 0$，我们得到\n<PLACEHOLDER_ENV_40>\n第三导数为\n<PLACEHOLDER_ENV_41>\n再次设置 $a = 0$ 得到\n<PLACEHOLDER_ENV_42>\n\\qed"
    },
    {
        "section": "9_4",
        "content": "\\subsection{Proof of Proposition~\\ref{thm:elliptical}}\nThe pdf of $X \\sim \\EE_N(\\mu, \\Sigma, \\phi)$ is\n<PLACEHOLDER_ENV_43>\nfor some function $g : \\R_{\\geq 0} \\to \\R_{\\geq 0}$ \\cite{frahm2004generalized}.\nA key property of elliptical distributions is that if $X \\sim \\EE_N(\\mu, \\Sigma, \\phi)$, then its affine transformation is also elliptical: $\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\sim \\EE_1(\\langle \\mathbf{w}, \\mu \\rangle, \\mathbf{w}^\\top \\Sigma \\mathbf{w}, \\phi)$ for any $\\mathbf{w} \\in \\R^N$.\nThus,\n<PLACEHOLDER_ENV_44>\nfor some other function $\\tilde{g} : \\R_{\\geq 0} \\to \\R_{\\geq 0}$.\n\nFrom our assumption of sign-symmetry, we have $\\mu = 0$.\nFor brevity, we define $\\sigma^2 \\triangleq \\mathbf{w}^\\top \\Sigma \\mathbf{w}$ and $S \\triangleq \\langle \\mathbf{w}, \\mathbf{X} \\rangle$.\nWe begin by computing (I) in \\cref{eq:loss_2relu_neuron}.\nRecall that we have $y = 0,1$ (\\ie $K = 2$).\nSo,\n<PLACEHOLDER_ENV_45>\nAt this point, we apply a $u$-substitution with $u = s^2 / \\sigma^2$, and thus $\\mathrm{d}u = 2 s \\ \\mathrm{d}s / \\sigma^2 \\iff \\sigma \\mathrm{d}u / 2 = s\\ \\mathrm{d}s / \\sigma$.\nThis yields\n<PLACEHOLDER_ENV_46>\nRecall that we assume the MSE loss $\\LL$ is finite.\nThe first term in \\cref{eq:loss_2relu_neuron} is clearly finite for $y = 0,1$.\nThe term $(II)$ is also easily seen to be finite, since it evaluates to $\\mathbf{w}^\\top (\\Sigma_0 + \\Sigma_1) \\mathbf{w} / 4$.\nThus, $\\LL$ being finite implies $(I)$ in \\cref{eq:loss_2relu_neuron} is finite,  which implies $C < \\infty$.\nWe computed (II) from \\cref{eq:loss_2relu_neuron} above as\n<PLACEHOLDER_ENV_47>\nfor $K = 2$.\nPlugging in (I) and (II) and differentiating with respect to $\\mathbf{w}$, we get\n<PLACEHOLDER_ENV_48>\nThe steady states of \\cref{eq:elliptical_gradient_flow} thus satisfy\n<PLACEHOLDER_ENV_49>\nRecall that translation-invariance implies that $\\Sigma_0$ and $\\Sigma_1$ are circulant, and since they are covariance matrices, they are symmetric.\nThen, they diagonalize in the basis given by the real and imaginary parts of the first $n/2$ Fourier components in the discrete Fourier transform, which we denote by the $n \\times n$ real matrix $\\FF$.\nNote that $\\FF$ is orthogonal.\nDefine $\\mathbf{v} = \\FF^\\top \\mathbf{w}$ and $\\Lambda_y = \\FF^\\top \\Sigma_y \\FF$ for $y = 0,1$.\nThus, the steady states satisfy\n<PLACEHOLDER_ENV_50>\nThis holds iff for all $i \\in [n]$,\n<PLACEHOLDER_ENV_51>\nThus, if $v_i \\neq 0$, we must have that\n<PLACEHOLDER_ENV_52>\nThat is, the ratio of the $i$-th eigenvalues of $\\Sigma_0$ and $\\Sigma_1$ must be constant for all $i$ s.t. $v_i \\neq 0$.\nThe eigenvalues of these matrices always come in pairs because of how we defined $\\FF$ using both the real and imaginary parts of the discrete Fourier transform.\nIn general, we observe that each pair assumes a unique value.\nSo, since $C$ is finite, the condition above can hold for at most two distinct values of $i$.\nTherefore, $v_i = 0$ for all but at most two $i \\in [n]$, implying that the steady state $w$ is of the form $a \\cos(2\\pi k x) + b \\sin(2 \\pi k x)$, \\ie it is oscillatory.\nAs such, it is \\emph{not} localized.\n\\qed<PLACEHOLDER_sections/a1-proofs_end>\n<PLACEHOLDER_sections/a2-experiments_begin>",
        "trans_content": "\\subsection{命题~\\ref{thm:elliptical}的证明}\n$X \\sim \\EE_N(\\mu, \\Sigma, \\phi)$的pdf为\n<PLACEHOLDER_ENV_43>\n对于某个函数$g : \\R_{\\geq 0} \\to \\R_{\\geq 0}$ \\cite{frahm2004generalized}。\n椭圆分布的一个关键性质是，如果$X \\sim \\EE_N(\\mu, \\Sigma, \\phi)$，则其仿射变换也是椭圆分布：$\\langle \\mathbf{w}, \\mathbf{X} \\rangle \\sim \\EE_1(\\langle \\mathbf{w}, \\mu \\rangle, \\mathbf{w}^\\top \\Sigma \\mathbf{w}, \\phi)$，对于任何$\\mathbf{w} \\in \\R^N$。\n因此，\n<PLACEHOLDER_ENV_44>\n对于另一个函数$\\tilde{g} : \\R_{\\geq 0} \\to \\R_{\\geq 0}$。\n\n根据我们的符号对称性假设，我们有$\\mu = 0$。\n为了简便起见，我们定义$\\sigma^2 \\triangleq \\mathbf{w}^\\top \\Sigma \\mathbf{w}$和$S \\triangleq \\langle \\mathbf{w}, \\mathbf{X} \\rangle$。\n我们首先计算\\cref{eq:loss_2relu_neuron}中的(I)。\n回忆一下，我们有$y = 0,1$（即$K = 2$）。\n所以，\n<PLACEHOLDER_ENV_45>\n此时，我们对$u = s^2 / \\sigma^2$进行$u$-代换，因此$\\mathrm{d}u = 2 s \\ \\mathrm{d}s / \\sigma^2 \\iff \\sigma \\mathrm{d}u / 2 = s\\ \\mathrm{d}s / \\sigma$。\n这给出\n<PLACEHOLDER_ENV_46>\n回忆一下，我们假设MSE损失$\\LL$是有限的。\n\\cref{eq:loss_2relu_neuron}中的第一项显然对于$y = 0,1$是有限的。\n项$(II)$也容易看出是有限的，因为它等于$\\mathbf{w}^\\top (\\Sigma_0 + \\Sigma_1) \\mathbf{w} / 4$。\n因此，$\\LL$是有限的意味着\\cref{eq:loss_2relu_neuron}中的(I)是有限的，这意味着$C < \\infty$。\n我们从上面的\\cref{eq:loss_2relu_neuron}中计算了(II)为\n<PLACEHOLDER_ENV_47>\n对于$K = 2$。\n将(I)和(II)代入并对$\\mathbf{w}$求导，我们得到\n<PLACEHOLDER_ENV_48>\n因此，\\cref{eq:elliptical_gradient_flow}的稳态满足\n<PLACEHOLDER_ENV_49>\n回忆一下，平移不变性意味着$\\Sigma_0$和$\\Sigma_1$是循环矩阵，并且由于它们是协方差矩阵，因此它们是对称的。\n然后，它们在由离散傅里叶变换中前$n/2$个傅里叶分量的实部和虚部给出的基中对角化，我们用$n \\times n$实矩阵$\\FF$表示。\n注意$\\FF$是正交的。\n定义$\\mathbf{v} = \\FF^\\top \\mathbf{w}$和$\\Lambda_y = \\FF^\\top \\Sigma_y \\FF$，其中$y = 0,1$。\n因此，稳态满足\n<PLACEHOLDER_ENV_50>\n当且仅当对于所有$i \\in [n]$，\n<PLACEHOLDER_ENV_51>\n因此，如果$v_i \\neq 0$，我们必须有\n<PLACEHOLDER_ENV_52>\n也就是说，$\\Sigma_0$和$\\Sigma_1$的第$i$个特征值的比率必须对于所有$v_i \\neq 0$的$i$保持不变。\n由于我们使用离散傅里叶变换定义了$\\FF$，这些矩阵的特征值总是成对出现。\n通常，我们观察到每对特征值都有唯一的值。\n因此，由于$C$是有限的，上述条件至多可以在两个不同的$i$值下成立。\n因此，除最多两个$i \\in [n]$外，$v_i = 0$，这意味着稳态$w$的形式为$a \\cos(2\\pi k x) + b \\sin(2 \\pi k x)$，即它是振荡的。\n因此，它是\\emph{非}局部化的。\n\\qed<PLACEHOLDER_sections/a1-proofs_end>\n<PLACEHOLDER_sections/a2-experiments_begin>"
    },
    {
        "section": "10+10_1",
        "content": "\\section{Additional experiments}\n\n\n\\subsection{Visualizing breakdown of Assumption~\\labelcref{item:lindeberg-condition}}\n\n\\cref{fig:time} demonstrates that our analytical model holds for long enough during training to capture the emergence of localization in the single ReLU neuron (\\labelcref{item:single-neuron-model}).\nIn the first three columns, we visualize the IPR of the weights from our empirical and analytical models, as well as the $\\ell_2$ difference between these two weights.\nIn the first four rows, we visualize these metrics for four random initializations of the model, training each on $\\texttt{NLGP}(g=100)$ with $\\xi_0 = 0.3$ and $\\xi_1 = 0.7$, where we expect from \\cref{thm:localization} to see localization.\nWe see the error rapidly increase shortly after IPR increases, indicating the formation of localized receptive fields.\nThe last three columns confirm this, as they show snapshots from \\emph{before}, \\emph{during}, and \\emph{after} the divergence between the empirical and analytical weights.\nWe observe the weights are nearly identical \\emph{before}, differ only slightly at the most localized point \\emph{during}, and are both localized \\emph{after}, but possibly with different magnitudes and positions.\nThe difference that emerges \\emph{during} is due to a breakdown of Assumption~\\labelcref{item:lindeberg-condition} used to create our analytical model, which is violated when the norm of $\\mathbf{w}$ is dominated by just a few entries, \\ie it is localized.\nWhile \\cite{ingrosso2022data} also observe a breakdown in their analytical model as localization emerges, ours, crucially, holds for long enough to characterize the emergence of localization.\n\nWe discuss the individual subplots in more detail.\nIn all but the third row of \\cref{fig:time}, the analytical predictions are near-exact; in the third row, we predict localization, but at the wrong position.\nFocusing again on the first row, we see that at $t=20$, the weights have not yet become localized (from IPR, left, first, and visually) and analytical and empirical weights match nearlt exactly, as confirmed by the small distance in left, center above.\nAt $t=30$, a localized peak around $i=21$ begins to emerge, violating Assumption \\labelcref{item:lindeberg-condition} and weakening analytical precision.\nThe analytical model then underestimates the degree to which the main peak at $i=21$ dominates, while it overestimates the size of competing peaks at $i=30$, $37$, and $90$.\nDespite this, at $t=50$, we see that predictions from the analytical model retain a match to the empirical model.\n\nIn the last row of \\cref{fig:time}, we use the same initialization and setting as in the first row, except that we train on $\\texttt{NLGP}(g=0.01)$ data instead.\nFrom \\cref{thm:localization}, we \\emph{do not} expect to see localization.\nThe evolution of IPR confirms this, as it stays low in magnitude.\nWe also see that, because localization never emerges, Assumption~\\labelcref{item:lindeberg-condition} is never violated, and so our analytical model accounts for the empirical model nearly perfectly.\n\n<PLACEHOLDER_figures/time/fig_begin><PLACEHOLDER_ENV_53><PLACEHOLDER_figures/time/fig_end><PLACEHOLDER_sections/a2-experiments_end>\n\n\\end{document}",
        "trans_content": "\\section{附加实验}\n\n\\subsection{可视化假设~\\labelcref{item:lindeberg-condition} 的分解}\n\n\\cref{fig:time} 展示了我们的分析模型在训练过程中足够长的时间内有效，能够捕捉到单个 ReLU 神经元中的定位现象（\\labelcref{item:single-neuron-model}）。\n在前面三列中，我们可视化了来自我们经验模型和分析模型的权重的 IPR，以及这两个权重之间的 $\\ell_2$ 差异。\n在前四行中，我们可视化了这些指标，分别对应模型的四个随机初始化，每个初始化在 $\\texttt{NLGP}(g=100)$ 上训练，且 $\\xi_0 = 0.3$ 和 $\\xi_1 = 0.7$，根据 \\cref{thm:localization}，我们期望看到定位现象。\n我们看到，在 IPR 增加后不久，误差迅速增加，表明局部感受野的形成。\n最后三列确认了这一点，它们展示了在经验权重和分析权重之间的发散发生前、中、后的快照。\n我们观察到，权重在 \\emph{之前} 几乎完全相同，在 \\emph{期间} 最局部化的点仅有微小差异，而在 \\emph{之后} 都是局部化的，但可能具有不同的幅度和位置。\n在 \\emph{期间} 出现的差异是由于假设~\\labelcref{item:lindeberg-condition} 的失效，这个假设在构建我们的分析模型时使用，当 $\\mathbf{w}$ 的范数被少数几项主导时（即局部化时）会被违背。\n虽然 \\cite{ingrosso2022data} 也观察到他们的分析模型在局部化出现时发生崩溃，但我们的方法关键地在足够长的时间内有效，从而能够表征局部化的出现。\n\n我们将对各个子图进行更详细的讨论。\n在 \\cref{fig:time} 的所有行中，除了第三行，分析预测几乎完全准确；在第三行中，我们预测了定位现象，但位置错误。\n再次聚焦在第一行，我们看到在 $t=20$ 时，权重尚未局部化（从 IPR 左侧、第一列和视觉上来看），分析权重和经验权重几乎完全一致，这一点可以通过左侧、中心上方的小距离确认。\n在 $t=30$ 时，围绕 $i=21$ 的局部峰值开始出现，这违背了假设 \\labelcref{item:lindeberg-condition}，并削弱了分析精度。\n分析模型低估了主峰 $i=21$ 的主导程度，同时高估了 $i=30$、$37$ 和 $90$ 处竞争峰的大小。\n尽管如此，在 $t=50$ 时，我们看到分析模型的预测仍然与经验模型相匹配。\n\n在 \\cref{fig:time} 的最后一行，我们使用与第一行相同的初始化和设置，只不过我们改为在 $\\texttt{NLGP}(g=0.01)$ 数据上进行训练。\n根据 \\cref{thm:localization}，我们\\emph{不}期望看到定位现象。\nIPR 的演化确认了这一点，因为它保持在较低的幅度。\n我们还观察到，由于定位现象从未出现，假设~\\labelcref{item:lindeberg-condition} 从未被违背，因此我们的分析模型几乎完全符合经验模型。\n\n<PLACEHOLDER_figures/time/fig_begin><PLACEHOLDER_ENV_53><PLACEHOLDER_figures/time/fig_end><PLACEHOLDER_sections/a2-experiments_end>\n\n\\end{document}"
    }
]