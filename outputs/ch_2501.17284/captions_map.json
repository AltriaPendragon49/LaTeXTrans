[
    {
        "placeholder": "<PLACEHOLDER_CAP_1>",
        "cap_type": "title",
        "content": "\\title{\n  Nonlinear dynamics of localization in \\\\ neural receptive fields\n}",
        "trans_content": "\\title{\n  神经感受野中局部化的非线性动力学\n}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_2>",
        "cap_type": "caption",
        "content": "\\caption{\n    \\textbf{(Left)}\n    Localization in\n    spatial receptive fields (RFs) measured from non-human primate (NHP) primary visual cortex~\\parencite[][Fig.~2]{ringach2002spatial}\n    and in spatiotemporal RFs measured from NHP~\\parencite[][Fig.~2]{decharms1998optimizing} and ferret~\\parencite[][Fig.~2]{singer2018sensory} primary auditory cortex.\n    \\textbf{(Center)}\n    Half-slice of the localized first-layer kernels of AlexNet trained for ImageNet classification~\\parencite{krizhevsky2012imagenet}.\n    \\textbf{(Right)}\n    Localized receptive fields learned from the task of \\cref{sec:task} in 2-D\n    using ICA~\\parencite{hyvarinen2000independent}\n    and the soft committee machine (SCM; \\labelcref{item:many-neuron-model} with fixed second-layer weights)\n    of \\cref{sec:model}.\n    \\emph{Localization---spatial and/or temporal selectivity---appears across settings,\n      as measured by response maximization in biological systems (left) and by inspecting linear filters in artificial systems (center, right).}\n  }",
        "trans_content": "\\caption{\n    \\textbf{(左)}\n    来自非人灵长类动物（NHP）初级视觉皮层的空间感受野（RFs）局部化~\\parencite[][图~2]{ringach2002spatial}\n    以及来自NHP~\\parencite[][图~2]{decharms1998optimizing}和雪貂~\\parencite[][图~2]{singer2018sensory}初级听觉皮层的时空感受野（RFs）局部化。\n    \\textbf{(中)}\n    用于ImageNet分类训练的AlexNet的局部化第一层卷积核半切片~\\parencite{krizhevsky2012imagenet}。\n    \\textbf{(右)}\n    从任务\\cref{sec:task}中学习到的局部化感受野，在二维空间中使用独立成分分析（ICA）~\\parencite{hyvarinen2000independent}\n    和软委员会机器（SCM；\\labelcref{item:many-neuron-model}，固定第二层权重）\n    来自\\cref{sec:model}。\n    \\emph{局部化——空间和/或时间选择性——在不同的设置中都有出现，\n      无论是通过生物系统中的响应最大化（左），还是通过检查人工系统中的线性滤波器（中、右）。}\n  }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_3>",
        "cap_type": "caption",
        "content": "\\caption{\n    From left:\n    Long- and short-lengthscale samples $\\mathbf{x}$,\n    covariances $\\Sigma$ for one lengthscale,\n    and marginals $p(X_i)$\n    for the data models described in \\cref{sec:task}:\n    Ising (with $J=1.2, 0.3$ for left, right samples),\n    the nonlinear Gaussian process~\\parencite[NLGP;~][]{ingrosso2022data},\n    and the controllable kurtosis model, \\texttt{Kur}\n    (with $\\xi=5, 1$ for left, right samples).\n    \\emph{\n    Each model generates samples centered about zero and with covariances that can be constrained to be similar,\n    but with differing higher-order statistics, as can be seen from the dimension-wise marginals.\n    }\n  }",
        "trans_content": "\\caption{\n    从左到右：\n    长尺度和短尺度样本 $\\mathbf{x}$，\n    单一尺度的协方差 $\\Sigma$，\n    以及数据模型的边缘分布 $p(X_i)$，如 \\cref{sec:task} 所述：\n    Ising 模型（左、右样本分别为 $J=1.2, 0.3$），\n    非线性高斯过程~\\parencite[NLGP;~][]{ingrosso2022data}，\n    以及可控峰度模型 \\texttt{Kur}\n    （左、右样本分别为 $\\xi=5, 1$）。\n    \\emph{\n    每个模型生成的样本以零为中心，且其协方差可以被约束为相似，\n    但具有不同的高阶统计量，从维度方向的边缘分布中可以看出这一点。\n    }\n}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_4>",
        "cap_type": "caption",
        "content": "\\caption{\n    From left and for the same \\texttt{Ising}, \\texttt{NLGP}, and \\texttt{Kur} data models as in \\cref{fig:task}:\n    the marginals $p(X_i)$,\n    the amplifier $\\varphi$ defined in \\cref{lem:gradient_flow} and kurtosis $\\kappa$,\n    and the evolution of simulated receptive fields for the single-neuron model (\\labelcref{item:single-neuron-model}) trained on its data, and\n    lastly the receptive field given by numerically integrating \\cref{eq:gradient_flow_early} with $\\varphi$ expanded to a third-order Taylor approximation for the same data;\n    training or evolution time is indicated by line color (blue for early-time; red for late-time).\n    \\emph{See \\cref{sec:theory-validation} for exposition.}\n  }",
        "trans_content": "\\caption{\n    从左侧开始，对于与 \\cref{fig:task} 中相同的 \\texttt{Ising}、\\texttt{NLGP} 和 \\texttt{Kur} 数据模型：\n    边缘分布 $p(X_i)$，\n    放大器 $\\varphi$（定义见 \\cref{lem:gradient_flow}）与峰度 $\\kappa$，\n    以及在其对应数据上训练的单神经元模型（\\labelcref{item:single-neuron-model}）的模拟感受野演化，\n    最后是通过数值积分 \\cref{eq:gradient_flow_early} 并将 $\\varphi$ 展开为三阶泰勒近似后得到的感受野；\n    训练或演化时间由线条颜色表示（蓝色表示早期，红色表示晚期）。\n    \\emph{详见 \\cref{sec:theory-validation} 的阐述。}\n  }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_5>",
        "cap_type": "caption",
        "content": "\\caption{\n    Evolution of receptive fields learned by the single-neuron model (\\labelcref{item:single-neuron-model}), along with sinusoids fit to final states (red dashes) when trained on data from three elliptical distributions: $t_{40}(\\nu=3)$ (\\textbf{left}), the surface of an ellipse (\\textbf{middle}), and a custom elliptical distribution that places its mass near the outside of an ellipse (\\textbf{right}).\n    In all cases, the learned receptive field is oscillatory (a sinusoid), as predicted by Proposition \\ref{thm:elliptical}.\n    The $\\ell_2$ distances between the fitted oscillatory weights and empirical RFs, as a ratio of the $\\ell_2$ norm of the empirical RFs, are (left) 9.77\\%, (center) 3.75\\%, and (right) 4.14\\%.\n    \\emph{See \\cref{sec:elliptical-experiments} for exposition.}\n    }",
        "trans_content": "\\caption{\n    单神经元模型 (\\labelcref{item:single-neuron-model}) 学习的感受野的演化，以及当在三种椭圆分布的数据上训练时，拟合到最终状态的正弦波（红色虚线）：$t_{40}(\\nu=3)$ (\\textbf{左图})，椭圆表面 (\\textbf{中图})，以及一个自定义的椭圆分布，其质量位于椭圆的外侧 (\\textbf{右图})。\n    在所有情况下，学习到的感受野都是振荡的（正弦波），正如命题 \\ref{thm:elliptical} 所预测的。\n    拟合的振荡权重与经验感受野之间的 $\\ell_2$ 距离，作为经验感受野的 $\\ell_2$ 范数的比率分别为（左图）9.77\\%，（中图）3.75\\%，（右图）4.14\\%。\n    \\emph{参见 \\cref{sec:elliptical-experiments} 以获取详细说明。}\n}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_6>",
        "cap_type": "caption",
        "content": "\\caption{\n    IPR \\vs excess kurtosis for $\\texttt{NLGP}$ and $\\texttt{Kur}$ data models, with mean and std.~dev.~across 30 re-initializations for the single-neuron model (\\labelcref{item:single-neuron-model});\n    error bars are small and may not be visible.\n    }",
        "trans_content": "\\caption{\n    IPR \\vs 超额峰度对于 $\\texttt{NLGP}$ 和 $\\texttt{Kur}$ 数据模型，基于30次重新初始化的单神经元模型的均值和标准差（\\labelcref{item:single-neuron-model}）；\n    误差条较小，可能不可见。\n    }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_7>",
        "cap_type": "caption",
        "content": "\\caption{\n    (\\textbf{Left}, \\textbf{Center}) Receptive fields learned\n    by many-neuron (\\labelcref{item:many-neuron-model})\n    soft committee machines (second-layer weights fixed at $\\frac{1}{K}$)\n    trained on the $\\texttt{Kur}(10)$ and $\\texttt{Kur}(4)$ datasets, respectively.\n    The models had $N=40$ input units, $K=10$ hidden units, and an initialization variance of $0.1$.\n    (\\textbf{Right}) A random subset of 10 components from the 40 learned by\n    the FastICA algorithm from scikit-learn~\\parencite{hyvarinen1999fast,scikit-learn} on the\n    $\\texttt{Kur}(3)$ dataset with\n    length-scale correlation values of $\\xi_0 = 1$ and $\\xi_1 = 3$.\n    \\emph{See \\cref{sec:extensions} for exposition.}\n  }",
        "trans_content": "\\caption{\n    (\\textbf{左}, \\textbf{中}) 由多神经元（\\labelcref{item:many-neuron-model}）软委员会机器（第二层权重固定为 $\\frac{1}{K}$）学习到的感受野，分别在 $\\texttt{Kur}(10)$ 和 $\\texttt{Kur}(4)$ 数据集上训练得到。\n    模型具有 $N=40$ 个输入单元，$K=10$ 个隐藏单元，初始化方差为 $0.1$。\n    (\\textbf{右}) 从 scikit-learn 的 FastICA 算法中学习到的 40 个分量的随机子集，其中包含 10 个分量，使用的 $\\texttt{Kur}(3)$ 数据集，其长度尺度相关值为 $\\xi_0 = 1$ 和 $\\xi_1 = 3$。\n    \\emph{参见 \\cref{sec:extensions} 以了解详细说明。}\n}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_8>",
        "cap_type": "caption",
        "content": "\\caption{\n    Receptive fields learned by the many-neuron model (\\labelcref{item:many-neuron-model})\n    with learnable second-layer weights, $N=40$, $K=10$.\n    (\\textbf{Top}) A random subset of 4 receptive fields from a model with sigmoid activation,\n    trained on $\\texttt{Kur(4)}$ (positive excess kurtosis of $3.28$).\n    As predicted by Claim~\\labelcref{thm:localization},\n    the receptive fields are \\emph{not} localized, and appear as high-frequency oscillations.\n    (\\textbf{Bottom}) A random subset of 4 receptive fields from a model with ReLU activation,\n    trained on $\\texttt{Kur(30)}$ (negative excess kurtosis of $-1.17$).\n    Receptive fields are localized (\\textbf{left three}) or exhibit low-frequency oscillations (\\textbf{right}).\n    \\emph{See \\cref{sec:extensions} for exposition.}\n  }",
        "trans_content": "\\caption{\n    具有可学习的第二层权重的多神经元模型 (\\labelcref{item:many-neuron-model})\n    的感受野，$N=40$，$K=10$。\n    (\\textbf{上图}) 来自一个使用sigmoid激活函数的模型的4个随机感受野子集，\n    该模型在$\\texttt{Kur(4)}$上训练（正的超额峰度为$3.28$）。\n    正如命题~\\labelcref{thm:localization}所预测的，\n    这些感受野\\emph{不是}局部化的，而表现为高频振荡。\n    (\\textbf{下图}) 来自一个使用ReLU激活函数的模型的4个随机感受野子集，\n    该模型在$\\texttt{Kur(30)}$上训练（负的超额峰度为$-1.17$）。\n    感受野是局部化的（\\textbf{左三}）或表现为低频振荡（\\textbf{右}）。\n    \\emph{参见 \\cref{sec:extensions} 以获取详细说明。}\n  }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_9>",
        "cap_type": "caption",
        "content": "\\caption{\n(\\textbf{Top}) Four initializations trained on $\\texttt{NLGP}(g=100)$ with $\\xi_0 = 0.3$ and $\\xi_1 = 0.7$.\nAs expected, weights always localize.\nIn (Left, First) we plot IPR for empirical and analytical receptive fields (RFs) across time (defined as (\\# of gradient steps) $\\times \\ \\, \\tau$, the learning rate).\nIn (Left, Second) we plot the time-evolution of $\\ell_2$ distance between the empirical and analytical RFs.\nIn (Left, Third) we zoom in on (Left, First), restricting the range to $[0,0.1]$ to more closely see divergence in IPR early in training.\nIn (Right, First) and (Right, Second), we snapshot the empirical and analytical RFs at a time \\emph{before} and \\emph{just after}, respectively, the analytical model breaks down  (according to IPR and $\\ell_2$ distance) due to localization.\nFinally, in (Right, Third), we snapshot \\emph{at the end} of the training period.\n(\\textbf{Bottom}) Same initialization as first row in \\textbf{top}, but trained on $\\texttt{NLGP}(g=0.01)$ data, again with $\\xi_0 = 0.3$ and $\\xi_1 = 0.7$.\nAs expected, weights do not localize.\nWe plot the same quantities as above, but here the predictions of our analytical model hold \\emph{throughout} the entire training process as localization never emerges and so assumption (A3) is not violated as above.\n\\label{fig:time}\n}",
        "trans_content": "\\caption{\n(\\textbf{顶部}) 在 $\\texttt{NLGP}(g=100)$ 上训练的四个初始化，$\\xi_0 = 0.3$ 和 $\\xi_1 = 0.7$。\n如预期的那样，权重始终会本地化。\n在（左，第一）中，我们绘制了经验和解析感受野（RF）随时间变化的 IPR（定义为（\\# 梯度步数）$\\times \\, \\tau$，学习率）。\n在（左，第二）中，我们绘制了经验和解析 RF 之间的 $\\ell_2$ 距离的时间演化。\n在（左，第三）中，我们放大（左，第一），将范围限制在 $[0,0.1]$ 以更清楚地观察训练初期 IPR 的发散。\n在（右，第一）和（右，第二）中，我们分别在解析模型因本地化而崩溃之前和之后的时刻快照了经验和解析 RF（根据 IPR 和 $\\ell_2$ 距离）。\n最后，在（右，第三）中，我们快照了训练期结束时的情况。\n(\\textbf{底部}) 与顶部第一行相同的初始化，但在 $\\texttt{NLGP}(g=0.01)$ 数据上训练，仍然使用 $\\xi_0 = 0.3$ 和 $\\xi_1 = 0.7$。\n如预期的那样，权重不会本地化。\n我们绘制了与上面相同的量，但在这里我们的解析模型的预测在整个训练过程中保持有效，因为本地化从未出现，因此假设（A3）未像上面那样被违反。\n\\label{fig:time}\n}"
    }
]