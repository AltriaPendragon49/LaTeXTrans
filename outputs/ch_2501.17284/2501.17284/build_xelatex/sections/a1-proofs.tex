\section{定义与符号}

\subsection{符号}
我们用 $[n]$ 来表示集合 $\{ i \in \N : 1 \leq i \leq n \}$。

\subsection{代数sigmoid函数}
\label{sec:algebraic-sigmoid}
对于 $k > 0$，广义代数sigmoid函数定义为
\begin{equation}
    \operatorname{alg}_k(x) \triangleq \frac{1}{2} \left( 1  + \frac{x}{(1+|x|^k)^{1/k}} \right)~.
\end{equation}
在后续的正文中，当 $k = 2$ 时，我们省略下标。
\subsection{反参与比（IPR）}  
\label{app:IPR}  
反参与比（IPR）定义为：  
$$ \operatorname{IPR}(\mathbf{w}) \triangleq \left(\sum_{i=1}^D w_i^4\right)/\left(\sum_{i=1}^D w_i^2\right)^2, $$  
其中，$w_i$ 表示权重 $\mathbf{w}$ 在第 $i$ 个维度上的幅值。
\section{超出主文范围的扩展}

\subsection{放大器 $\varphi$ 的解析性质}
\label{sec:varphi-analysis}

我们展示了在引理~\labelcref{lem:varphi} 中定义的放大函数 $\varphi$ 的几个性质。

\begin{lemma} \label{lem:varphi}
    局部放大器 $\varphi$ 在 \cref{eq:varphi} 中满足 $\varphi(-a) = -\varphi(a)$，对于所有 $a \in (-1,1)$ 都成立。  
    此外，其导数满足以下关系，其中 $\sigma^2$ 和 $\kappa$ 分别表示 $X_1$ 的方差和峰度：
    \vspace{-10pt}
    \begin{align*}
      \varphi'(0) &= \sqrt{\frac{2}{\pi}} \sigma^2~, &&\text { and }&&
      \varphi'''(0) = -\sqrt{\frac{2}{\pi}} (\kappa^4 \sigma^4 - 3 \sigma^2)~.
    \end{align*}
\end{lemma}
\vspace{-6pt}

为了更好地理解 $\mathbf{X}$ 的边际分布如何影响定位，我们使用引理~\labelcref{lem:varphi} 中的导数构造了关于 0 的 $\varphi$ 的三阶泰勒近似。
引理~\labelcref{lem:varphi} 中的导数表明，对于具有恒定方差的 $X_1$ 的每个分布，在 0 附近将表现为相同的线性函数。
只有当我们远离零时，第三阶项才变得重要，$\varphi$ 才表现为非线性。
在 $\sigma^2 = 1$ 的情况下（即 $X_1$ 的方差等于较大目标值 $y=1$），第三阶项表明当 $\kappa < 3$ 时，$\varphi$ 是超线性的，即过度峰度是正的，反之则为亚线性。

超线性的 $\varphi$ 会鼓励那些 $\Sigma_1 \mathbf{w}$ 较大的项比其他项以更快的速度增长，后者都通过 \cref{eq:gradient_flow_early} 中的第二项受到相同的 \emph{线性} 范数约束。
协方差矩阵 $\Sigma_1$ 作为循环矩阵，充当某些向量与 $\sigma_1^1$（$\Sigma_1$ 的第一行）之间的卷积算子。
由于 $y=1$ 对应于具有较大长度尺度相关性的类别，$\sigma_1^1$ 会相对缓慢地衰减，像是一个加权的局部平均。
因此，$\Sigma_1 \mathbf{w}$ 是 $\mathbf{w}$ 中每个条目的加权局部平均。
因此，在某些邻域内，$\mathbf{w}$ 较大的条目将被鼓励比较小的条目增长得更快，这一效应会随着 \cref{eq:gradient_flow_early} 的积分而加剧。
因此，超线性促使定位。

正如我们将在引理~\ref{thm:elliptical} 中看到的，对于椭圆形数据的设置，如果 $\varphi$ 是线性的，$\mathbf{w}$ 会学习为正弦函数，因此不会定位。
在亚线性情况下，我们预期较大值会被抑制，而不是像超线性情况下那样被促进。
因此，首先近似地，过度峰度 $\kappa - 3$（对于 $\sigma^2 = 1$）的符号表明 $\mathbf{w}$ 是否定位。

然而，仅仅研究 $\varphi'''(0)$ 并不足以充分表征边际分布如何影响定位。
一个函数可能在小的 $a$ 下是亚线性的，而在较大的 $a$ 下是超线性的，这使得无法明确判断它是否会导致定位。
对于那些 $\kappa \approx 3$ 且没有表现出严格的超线性或亚线性的边际分布，这一条件已不再足够精确，无法确定是否会发生定位。
\subsection{方程 \cref{eq:gradient_flow_early} 的 PDE 极限}
\label{sec:pde-limit}

通过将 $N$ 取为较大并将 $w$ 视为与位置相关的连续函数，即 $w \equiv w(x, t)$，可以将 \cref{eq:gradient_flow_early} 视为偏微分方程（PDE）。  
求解其稳态等价于求解  
\begin{equation}
    \varphi\left( \frac{\sigma^1 \star w}{\sqrt{ \langle \sigma^1 \star w, w \rangle}} \right) - \frac{1}{2} (\sigma^0 + \sigma^1) \star w \equiv 0~,
\end{equation}  
其中 $w : [0,1] \to \R$ 是周期性的，$\sigma^y$ 是与矩阵 $\Sigma_y$ 极限情况对应的卷积。  
这个方程似乎没有非单位矩阵 $\Sigma_1$ 的显式解，因此，在这个 PDE 极限或对于有限 $N$ 时，可能无法精确求解 \cref{eq:gradient_flow_early} 的稳态。
\subsection{假设~\labelcref{item:mean-assumption,item:covariance-assumption} \vs 高斯等价性}

假设~\labelcref{item:mean-assumption,item:covariance-assumption} 等价于在训练初期将 $\langle \mathbf{w}, \mathbf{X} \rangle \mid X_i$ 近似为高斯分布。
类似的思想已被用于推导神经网络的梯度流动力学，包括在建立高斯等价性性质的研究中，如 \cite{goldt2020modelling,gerace2020generalisation,goldt2022gaussian}。
然而，这些工作建模的是无条件的预激活项 $\langle \mathbf{w}, \mathbf{X} \rangle$ 为高斯分布，而不是首先对 $X_i$ 进行条件化。
其产生的原因在于，这些先前的工作是在对损失函数进行梯度流动力学求导 \emph{之前} 就提出了高斯近似。
但在该阶段进行近似会忽略一个乘性因子 $\mathbf{X}$，这是由于链式法则作用于 $\langle \mathbf{w}, \mathbf{X} \rangle$ 所导致的。
从抽象角度来看，这种方法假设 $\LL_\text{exact} \to \LL_\text{Gauss}$ 推出 $\nabla_\mathbf{w} \LL_\text{exact} \to \nabla_\mathbf{w} \LL_\text{Gauss}$，但一般来说这一推理并不成立，尤其在此处，这一假设未能体现学习过程与高阶输入统计量之间的相互作用。
这也是 \cite{ingrosso2022data} 中高斯等价性失败的一个原因。
相比之下，在推导引理~\labelcref{lem:gradient_flow} 时，我们可以通过假设 $\langle \mathbf{w}, \mathbf{X} \rangle \mid X_i$ 而不是 $\langle \mathbf{w}, \mathbf{X} \rangle$ 是高斯分布，从而考虑额外的 $\mathbf{X}$ 项。
这种条件化的方法，加之数据的平移不变性（性质~\labelcref{item:translation-invariance}），也进一步激发了将边缘分布 $X_i$ 作为研究对象的动机，以获得适用于非高斯输入训练的神经网络的梯度流。
\section{理论结果的证明}
\label{app:proofs}

\subsection{均方误差（MSE）损失的梯度流}
损失函数由以下给出：
\begin{align}
    \LL
    &= \E_{\mathbf{X},Y}\left[ (Y - \operatorname{ReLU}(\langle \mathbf{w}, \mathbf{X} \rangle) )^2 \right] \notag \\
    &= \E_{\mathbf{X},Y}\left[ Y^2 \right] - 2 \underbrace{ \E_{\mathbf{X},Y}\left[ Y \operatorname{ReLU}(\langle \mathbf{w}, \mathbf{X} \rangle) \right] }_{\triangleq (I)} + \underbrace{ \E_{\mathbf{X},Y}\left[ \operatorname{ReLU}^2(\langle \mathbf{w}, \mathbf{X} \rangle) \right] }_{\triangleq (II)}. \label{eq:loss_2relu_neuron}
\end{align}
符号对称性假设（\labelcref{item:sign-symmetry}）表明
$\langle \mathbf{w}, \mathbf{X} \rangle$ 也具有符号对称性。
首先，这意味着 $\PR( \langle \mathbf{w}, \mathbf{X} \rangle > 0 ) = \frac{1}{2}$，所以：
\begin{align*}
    (II)
    &= \frac{1}{2} \E_{\mathbf{X},Y \mid \langle \mathbf{w}, \mathbf{X} \rangle \geq 0}\left[ \operatorname{ReLU}^2(\langle \mathbf{w}, \mathbf{X} \rangle) \right]
    = \frac{1}{2} \E_{\mathbf{X},Y \mid \langle \mathbf{w}, \mathbf{X} \rangle \geq 0}\left[ (\langle \mathbf{w}, \mathbf{X} \rangle)^2 \right].
\end{align*}
其次，$\langle \mathbf{w}, \mathbf{X} \rangle$ 的符号对称性
意味着我们可以去掉条件 $\langle \mathbf{w}, \mathbf{X} \rangle \geq 0$，因为 $\langle \mathbf{w}, \mathbf{X} \rangle \overset{d}{=} -\langle \mathbf{w}, \mathbf{X} \rangle$。
因此，
\begin{align*}
    (II)
    &= \frac{1}{2} \E_{\mathbf{X},Y}\left[ (\langle \mathbf{w}, \mathbf{X} \rangle)^2 \right] \\
    &= \frac{1}{2} \mathbf{w}^\top \E_{\mathbf{X},Y} \left[ \mathbf{X} \mathbf{X}^\top \right] \mathbf{w} \\
    &= \frac{1}{2} \mathbf{w}^\top \left( \frac{1}{K} \sum_{y=0}^{K-1} \Sigma_y \right) \mathbf{w}  \\
    &= \frac{1}{2K} \sum_{y=0}^{K-1} \mathbf{w}^\top \Sigma_y \mathbf{w}~,
\end{align*}
其中 $K$ 是离散的 $y$ 的值（类别）数。
最后，我们对 $\mathbf{w}$ 对损失函数 $\LL$ 求导：
\begin{align*}
  \nabla_\mathbf{w} \LL &= 2 \E_{\mathbf{X},Y}\left[ Y \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0) \mathbf{X} \right] + \frac{1}{K} \sum_{y=0}^{K-1} \Sigma_y \mathbf{w}~.
\end{align*}
梯度流~\cite{elkabetz2024continuous}由 $\frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t} = -\tau \nabla_\mathbf{w} \LL$ 给出，其中 $\tau$ 是学习率。
因此，
\begin{equation}
  \frac{1}{\tau} \frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t}
    = 2 \E_{\mathbf{X},Y}\left[ Y \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0) \mathbf{X} \right] - \frac{1}{K} \sum_{y=1}^{K} \Sigma_y \mathbf{w}~. \label{eq:gradient_flow_two}
\end{equation}
\subsection{引理 \ref{lem:gradient_flow} 的证明} \label{subsec:pf_of_gradient_flow}
在方程 \eqref{eq:gradient_flow_two} 中设定 $K = 2$，我们得到
\begin{align*}
  \frac{1}{\tau} \frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t}
  &= \E_{\mathbf{X} \mid Y=1}[ \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 ) \mathbf{X} ] - \frac{1}{2} \left( \Sigma_0 + \Sigma_1 \right) \mathbf{w}~.
\end{align*}
我们希望显式地表示第一项。
注意，第一项是 $\R^N$ 中的一个向量。
我们通过使用全期望法则，分别考虑每一项，写出第 $i$ 项为：
\begin{align*}
    \E_{\mathbf{X} \mid Y=1}[ \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 ) X_i ]
    &= \E_{ X_i \mid Y=1 } \left[ X_i \PR_{\mathbf{X} \mid X_i=x_i, Y = 1} \left[  \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 \right] \right]~.
\end{align*}

根据假设~\labelcref{item:lindeberg-condition}，$\{ w_i X_i \mid 1 \leq i \leq N \}$ 满足 Lindeberg 条件，当 $N \to \infty$ 时。
这也被称为 \emph{一致可积性} 要求。
在正式阐述之前，首先引入两个变量：$S_N \triangleq \sum_{j=1}^{N} w_j (X_j - \mu_{j\mid x_i})$，部分和，以及它们的方差，$\sigma_N^2 \triangleq \E[ S_N^2 ]$，其中 $\mu_{j \mid x_i} \triangleq \E[X_j \mid X_i = x_i]$ 是在给定第 $i$ 项值为 $x_i$ 时，第 $j$ 项的条件均值。
然后，Lindeberg 条件被正式表述为
\begin{equation*}
  \sup_N \E_{S_N \mid X_i = x_i} \left[ \left| \tfrac{S_N^2}{\sigma_N^2} \right| \mathbbm{1} \left( \left| \tfrac{S_N^2}{\sigma_N^2} \right| > x \right) \right] \to 0 \quad \text{as} \quad x\to\infty~.
\end{equation*}
该条件实际上表明，部分和中的任何项 $w_i X_i$ 都不会占主导地位。
在该条件下，结合弱依赖性（性质~\labelcref{item:weak-dependence}），我们根据 \cite[定理 1.19, 10.2]{bradley2007introduction} 得出 $S_N / \sigma_N \overset{d}{\to} \NN(0,1)$。
注意到
\begin{equation*}
    S_N = \langle \mathbf{w}, \mathbf{X} \rangle - \langle \mathbf{w}, \mathbf{\mu}_{\mid x_i} \rangle \qquad  \text{and} \qquad \sigma_N = \sqrt{\mathbf{w}^\top \Sigma_{\mid x_i}^{1} \mathbf{w}}~,
\end{equation*}
其中，$\mathbf{w}, \mathbf{X}$ 是 $N$ 维向量，$\mathbf{\mu}_{\mid x_i} = \E[\mathbf{X} \mid X_i = x_i]$ 是条件均值的向量，而 $\Sigma_{\mid x_i}^{1} \triangleq \text{Cov}[\mathbf{X} \mid X_i = x_i, Y = 1] = \Sigma_1 - \sigma_i^1 \sigma_i^{1\top}$。
由于 $\sigma_N$ 和 $\mathbf{\mu}_{\mid x_i}$ 是常数，我们可以写作
\begin{align*}
    \PR_{\mathbf{X} \mid X_i = x_i, Y = 1} \left[  \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 \right]
    &= \PR_{\mathbf{X} \mid X_i, Y = 1} \left[ \frac{ \langle \mathbf{w}, \mathbf{X} \rangle - \langle \mathbf{w}, \mathbf{\mu}_{j \mid x_i} \rangle }{ \sigma_N } \geq -\frac{\langle \mathbf{w}, \mathbf{\mu}_{j \mid x_i} \rangle }{ \sigma_N } \right] \\
    &= \PR_{Z \sim \NN(0,1)} \left[ Z \geq -\frac{\langle \mathbf{w}, \mathbf{\mu}_{j \mid x_i} \rangle }{ \sigma_N } \right] + o_N(1) \\
    &= 1 - \frac{1}{2} \left[ 1 + \operatorname{erf}\left( -\frac{\langle \mathbf{w}, \mathbf{\mu}_{j \mid x_i} \rangle }{ \sqrt{2} \sigma_N } \right) \right] + o_N(1) \\
    &= \frac{1}{2} + \frac{1}{2} \operatorname{erf}\left(\frac{\langle \mathbf{w}, \mathbf{\mu}_{j \mid x_i} \rangle }{ \sqrt{2} \sigma_N } \right) + o_N(1)~,
\end{align*}
其中第二步，得到 $o_N(1)$，是由分布收敛的定义得出的。
在假设 \labelcref{item:mean-assumption,item:covariance-assumption} 下，我们可以将其表示为
\begin{align*}
    \PR_{\mathbf{X} \mid X_i = x_i, Y = 1} \left[  \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 \right]
    &= \frac{1}{2} + \frac{1}{2} \operatorname{erf}\left( \frac{\langle \mathbf{w}, x_i \sigma_i^y \rangle }{ \sqrt{2} \sqrt{ \mathbf{w}^\top \Sigma_1 \mathbf{w} - (\langle \mathbf{w}, \sigma_i^y \rangle)^2 } } \right) + o_N(1)~.
\end{align*}
因此，
\begin{align*}
    &\E_{\mathbf{X} \mid Y=1}[ \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 ) X_i ] \\
    &= \E_{ X_i \mid Y=1 } \left[ X_i \left( \frac{1}{2} + \frac{1}{2} \operatorname{erf}\left( \frac{X_i}{\sqrt{2}} \cdot \frac{\langle \mathbf{w}, \sigma_i^y \rangle }{ \sqrt{ \mathbf{w}^\top \Sigma_1 \mathbf{w} - (\langle \mathbf{w}, \sigma_i^y \rangle)^2 } } \right) + o_N(1) \right) \right] \\
    &= \frac{1}{2} \E_{ X_i \mid Y=1 } \left[ X_i \operatorname{erf}\left( \frac{X_i}{\sqrt{2}} \cdot \frac{\langle \mathbf{w}, \sigma_i^y \rangle / \sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}} }{ \sqrt{ 1 - (\langle \mathbf{w}, \sigma_i^y \rangle / \sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}})^2 } } \right) \right] + \E_{X_i}[|X_i|] o_N(1) \\
    &= \frac{1}{2} \E_{ X_i \mid Y=1 } \left[ X_i \operatorname{erf}\left( \frac{X_i}{\sqrt{2}} \cdot \operatorname{alg}^{-1} \left( \frac{ \langle \mathbf{w}, \sigma_i^y \rangle }{ \sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}} } \right) \right) \right] + \E_{X_i}[|X_i|] o_N(1)~.
\end{align*}
定义 $\varphi_i(a) \triangleq \E_{X_i \mid Y=1}[ X_i \operatorname{erf}(X_i \operatorname{alg}^{-1}(a) / \sqrt{2}) ]$，我们可以写作
\begin{align*}
    \E_{\mathbf{X} \mid Y=1}[ \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 ) X_i ]
    &= \frac{1}{2} \varphi_i \left( \frac{ \langle \mathbf{w}, \sigma_i^y \rangle }{ \sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}} } \right) + \E_{X_i}[|X_i|] o_N(1)~.
\end{align*}
注意，由于 Cauchy-Schwarz 不等式，$\E_{X_i}[|X_i|] \leq \sqrt{ \E_{X_i}[X_i^2] } = \sigma$。
所以，$\E_{X_i}[|X_i|] o_N(1) = o_N(1)$。
此外，由于平移不变性，所有的 $X_i$ 具有相同的边际分布，因此 $\varphi_i \equiv \varphi_1 \triangleq \varphi$。
因此，
\begin{align*}
    \E_{\mathbf{X} \mid Y=1}[ \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 ) X_i ]
    &= \frac{1}{2} \varphi \left( \frac{ \langle \mathbf{w}, \sigma_i^y \rangle }{ \sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}} } \right) + o_N(1)~.
\end{align*}
这个形式对于所有项 $i$ 都成立。
将它们连接起来，我们得到
\begin{align*}
    \E_{\mathbf{X} \mid Y=1}[ \mathbbm{1}( \langle \mathbf{w}, \mathbf{X} \rangle \geq 0 ) \mathbf{X} ]
    &= \frac{1}{2} \varphi \left( \frac{ \Sigma_1 \mathbf{w} }{ \sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}} } \right) + o_N(1)~,
\end{align*}
从而得到所需的结果。
\qed
\subsection{引理 \ref{lem:varphi} 的证明} \label{subsec:pf_of_varphi}
性质 1 来源于 $\operatorname{alg}$ 和 $\operatorname{erf}$ 是奇函数这一事实。这意味着性质 4 得以成立，因为奇函数的偶次 Taylor 系数必须为零。

性质 2 和 3 来源于对 \cref{eq:varphi} 求导。第一导数为
\begin{align*}
    \varphi'(a) &= \frac{\sqrt{2}}{{\sqrt{{\pi}}}(1-a^2)^\frac{3}{2}} \E_{X_1}\left[ X_1^2\mathrm{e}^{-\frac{X_1^2a^2}{2(1-a^2)}} \right].
\end{align*}
设置 $a = 0$，我们得到
\begin{align*}
    \varphi'(0)
    &= \sqrt{\frac{2}{\pi}} \E\left[ X_1^2 \right]
    = \sqrt{\frac{2}{\pi}} \sigma^2.
\end{align*}
第三导数为
\begin{align*}
    \varphi'''(a) &= \frac{\sqrt{2}}{\sqrt{{\pi}}(1-a^2)^\frac{7}{2}(a^2-1)^2} \E_{X_1} \left[
    X_1^2(12a^6+(9X_1^2-21)a^4+(X_1^4-8X_1^2+6)a^2-X_1^2+3)\mathrm{e}^{-\frac{X_1^2a^2}{2(1-a^2)}}
    \right].
\end{align*}
再次设置 $a = 0$ 得到
\begin{align*}
    \varphi'''(0) &= \sqrt{\frac{2}{\pi}} \E_{X_1} \left[ X_1^2(-X_1^2+3) \right]
    = \sqrt{\frac{2}{\pi}} \left( 3 \E_{X_1}[X_1^2] - \E_{X_1}[X_1^4] \right)
    = -\sqrt{\frac{2}{\pi}} (\kappa^4 \sigma^4 - 3 \sigma^2)~.
\end{align*}
\qed
\subsection{命题~\ref{thm:elliptical}的证明}
$X \sim \EE_N(\mu, \Sigma, \phi)$的pdf为
\begin{align*}
    p_X(x) &= \frac{1}{\sqrt{\det(\Sigma)}} g( (x-\mu)^\top \Sigma^{-1} (x-\mu) )~,
\end{align*}
对于某个函数$g : \R_{\geq 0} \to \R_{\geq 0}$ \cite{frahm2004generalized}。
椭圆分布的一个关键性质是，如果$X \sim \EE_N(\mu, \Sigma, \phi)$，则其仿射变换也是椭圆分布：$\langle \mathbf{w}, \mathbf{X} \rangle \sim \EE_1(\langle \mathbf{w}, \mu \rangle, \mathbf{w}^\top \Sigma \mathbf{w}, \phi)$，对于任何$\mathbf{w} \in \R^N$。
因此，
\begin{align*}
  p_{\langle \mathbf{w}, \mathbf{X} \rangle}(s) &= \frac{1}{\sqrt{\mathbf{w}^\top \Sigma \mathbf{w}}} \tilde{g}\left( \frac{(s-\langle \mathbf{w}, \mu \rangle)^2}{\mathbf{w}^\top \Sigma \mathbf{w}} \right),
\end{align*}
对于另一个函数$\tilde{g} : \R_{\geq 0} \to \R_{\geq 0}$。

根据我们的符号对称性假设，我们有$\mu = 0$。
为了简便起见，我们定义$\sigma^2 \triangleq \mathbf{w}^\top \Sigma \mathbf{w}$和$S \triangleq \langle \mathbf{w}, \mathbf{X} \rangle$。
我们首先计算\cref{eq:loss_2relu_neuron}中的(I)。
回忆一下，我们有$y = 0,1$（即$K = 2$）。
所以，
\begin{align*}
    2 \times (I)
    &= \E_{S \mid Y=1}[ \operatorname{ReLU}( S ) ]
    = \int_0^{\infty} \frac{1}{\sigma} \tilde{g}\left( \frac{s^2}{\sigma^2} \right) s \ \mathrm{d}s.
\end{align*}
此时，我们对$u = s^2 / \sigma^2$进行$u$-代换，因此$\mathrm{d}u = 2 s \ \mathrm{d}s / \sigma^2 \iff \sigma \mathrm{d}u / 2 = s\ \mathrm{d}s / \sigma$。
这给出
\begin{align*}
    2 \times (I)
    &= \frac{\sigma}{2} \underbrace{\int_0^{\infty} \tilde{g}(u)\ \mathrm{d}u}_{\triangleq C}
    = \frac{C}{2} \sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}}.
\end{align*}
回忆一下，我们假设MSE损失$\LL$是有限的。
\cref{eq:loss_2relu_neuron}中的第一项显然对于$y = 0,1$是有限的。
项$(II)$也容易看出是有限的，因为它等于$\mathbf{w}^\top (\Sigma_0 + \Sigma_1) \mathbf{w} / 4$。
因此，$\LL$是有限的意味着\cref{eq:loss_2relu_neuron}中的(I)是有限的，这意味着$C < \infty$。
我们从上面的\cref{eq:loss_2relu_neuron}中计算了(II)为
\begin{align*}
  \frac{1}{4} \mathbf{w}^\top \left( \Sigma_0 + \Sigma_1 \right) \mathbf{w}
\end{align*}
对于$K = 2$。
将(I)和(II)代入并对$\mathbf{w}$求导，我们得到
\begin{equation}
  \frac{1}{\tau} \frac{\mathrm{d}\mathbf{w}}{\mathrm{d}t} = \frac{\Sigma_1 \mathbf{w}}{\sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}}} - \frac{1}{2} \left( \Sigma_0 + \Sigma_1 \right) \mathbf{w}~. \label{eq:elliptical_gradient_flow}
\end{equation}
因此，\cref{eq:elliptical_gradient_flow}的稳态满足
\begin{align*}
  C \frac{\Sigma_1 \mathbf{w}}{\sqrt{\mathbf{w}^\top \Sigma_1 \mathbf{w}}}
  &= \frac{1}{2} \left( \Sigma_0 + \Sigma_1 \right) \mathbf{w}~.
\end{align*}
回忆一下，平移不变性意味着$\Sigma_0$和$\Sigma_1$是循环矩阵，并且由于它们是协方差矩阵，因此它们是对称的。
然后，它们在由离散傅里叶变换中前$n/2$个傅里叶分量的实部和虚部给出的基中对角化，我们用$n \times n$实矩阵$\FF$表示。
注意$\FF$是正交的。
定义$\mathbf{v} = \FF^\top \mathbf{w}$和$\Lambda_y = \FF^\top \Sigma_y \FF$，其中$y = 0,1$。
因此，稳态满足
\begin{align*}
  C \frac{\Lambda_1 \mathbf{v}}{\sqrt{\mathbf{v}^\top \Lambda_1 \mathbf{v}}} &= \frac{1}{2} \left( \Lambda_0 + \Lambda_1 \right) \mathbf{v}~.
\end{align*}
当且仅当对于所有$i \in [n]$，
\begin{align*}
  C (\Lambda_1)_{ii} (\mathbf{v}^\top \Lambda_1 \mathbf{v})^{-\frac{1}{2}} v_i &= \frac{1}{2} ( (\Lambda_0)_{ii} + (\Lambda_1)_{ii} ) v_i~.
\end{align*}
因此，如果$v_i \neq 0$，我们必须有
\begin{align*}
  \frac{(\Lambda_0)_{ii}}{(\Lambda_1)_{ii}} &= 2 C (\mathbf{v}^\top \Lambda_1 \mathbf{v})^{-\frac{1}{2}} - 1~.
\end{align*}
也就是说，$\Sigma_0$和$\Sigma_1$的第$i$个特征值的比率必须对于所有$v_i \neq 0$的$i$保持不变。
由于我们使用离散傅里叶变换定义了$\FF$，这些矩阵的特征值总是成对出现。
通常，我们观察到每对特征值都有唯一的值。
因此，由于$C$是有限的，上述条件至多可以在两个不同的$i$值下成立。
因此，除最多两个$i \in [n]$外，$v_i = 0$，这意味着稳态$w$的形式为$a \cos(2\pi k x) + b \sin(2 \pi k x)$，即它是振荡的。
因此，它是\emph{非}局部化的。
\qed
