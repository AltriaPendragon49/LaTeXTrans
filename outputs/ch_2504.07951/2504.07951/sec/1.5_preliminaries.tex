\section{预备知识}

\subsection{定义}

\cpar{原生多模态模型（Native Multimodal Models, NMMs）：}
指的是从零开始在所有模态上同时训练的模型，不依赖于预训练的大型语言模型（LLMs）或视觉编码器。我们的关注点是图像和文本这两个典型模态，其中模型同时处理文本和图像作为输入，并生成文本作为输出。

\cpar{早期融合（Early fusion）：}
从一开始就实现多模态交互，几乎不使用模态特定的参数（\eg，除了用于图像块化的线性层）。该方法使用一个单一的 transformer 模型，处理原始的多模态 \edit{输入——即标记化的文本和连续的图像块——而无需图像离散化。} \edit{我们}将该主 transformer 称为解码器。

\cpar{晚期融合（Late fusion）：}
将多模态交互延后到更深层，通常是在各自的单模态组件 \edit{独立处理完}每个模态之后（\eg，将视觉编码器连接到大型语言模型）。

\cpar{模态无关路由（Modality-agnostic routing）：}
在稀疏专家混合机制中，\edit{模态无关}的路由是指依赖于与模型联合训练的学习型路由器模块。

\cpar{模态感知路由（Modality-aware routing）：}
基于预定义规则的路由方式，例如根据模态类型进行路由（\eg，视觉 token、文本 token）。

\input{tables/notations}
\subsection{尺度定律}
我们的目标是理解NMM的尺度特性，以及不同的架构选择如何影响权衡。为此，我们在~\citet{kaplan2020scaling, hoffmann2022training} 提出的尺度定律框架内分析我们的模型。
我们基于总参数数量计算FLOPs，使用近似值 \(C = 6ND\)，这一近似在之前的工作中被采用~\citep{hoffmann2022training, abnar2025parameters}。然而，我们对这个估计进行了修改，以适应我们的设置：对于晚期融合模型，FLOPs被计算为 \(6(N_vD_v + ND)\)。
我们考虑一个设置，在给定计算预算 \(C\) 的情况下，我们的目标是预测模型的最终 \edit{损失}，并确定最佳的参数数量 \edit{和} 训练令牌数量。与之前关于LLM尺度研究一致~\citep{hoffmann2022training}，我们假设最终模型损失与模型大小 (\(N\)) 和训练令牌 (\(D\)) 之间存在幂律关系：

\begin{equation}
\label{eq:scaling_laws}
    L = E + \frac{A}{N^{\alpha}} + \frac{B}{D^{\beta}}.
\end{equation}

\noindent 其中，\(E\) 代表数据集上可实现的最低损失，而 \(\frac{A}{N^{\alpha}}\) 捕捉了增加参数数量的效果，其中更大的模型会导致更低的损失，改善的速度由 \(\alpha\) 控制。类似地，\(\frac{B}{D^{\beta}}\) 反映了更多训练令牌的益处，\(\beta\) 决定了改善的速率。此外，我们假设计算预算（FLOPs）与 \(N\) 和 \(D\) 之间存在线性关系 (\(C \propto ND\))。这进一步导致了在 \cref{tab:power_laws} 中详细描述的幂律关系。

\input{figs/scaling_laws_early_vs_late}

\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{16pt}
    \renewcommand{\arraystretch}{1}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{lccrc}
        Data type & dataset & \#samples & sampling prob. \\
         \shline
         \multirow{3}{*}{Image-Caption} &  DFN~\citep{fang2023data} & 2B & 27\%
         \\
          & COYO~{\citep{kakaobrain2022coyo700m}} &  600M & 11.25\% \\
          & HQITP  & 400M & 6.75\% \\
          Interleaved & Obelics \citep{laurenccon2024obelics}  & 141M Docs &
          45\% \\
          Text & DCLM \citep{li2024datacomp} & 6.6T Toks & 10\% \\

    \end{tabular}} \caption{\textbf{预训练数据混合。} 除非另有说明，训练混合数据包含45\%、45\%和10\%的图像说明、交错文档和仅文本数据。}
    \label{tab:pretraining_datasets}
    \vspace{-5pt}
\end{table}
\subsection{实验设置}
\edit{我们的模型}基于自回归Transformer架构~\citep{vaswani2017attention}，采用SwiGLU FFNs~\citep{shazeer2020glu}和QK-Norm~\citep{dehghani2023scaling}，参照~\citet{li2024datacomp}的方法。在早期融合模型中，图像块被线性映射到与文本标记维度匹配，而晚期融合则遵循CLIP架构~\citep{radford2021learning}。我们为文本标记采用因果注意力，为图像标记采用双向注意力，发现这种方式效果更佳。训练是在混合的公共和私有多模态数据集上进行的，包括DCLM \citep{li2024datacomp}、Obelics \citep{laurenccon2024obelics}、DFN \citep{fang2023data}、COYO \citep{kakaobrain2022coyo700m}，以及一个私有的高质量图像-文本对（HQITP）集合（见\cref{tab:pretraining_datasets}）。图像被\edit{调整大小}为224×224分辨率，图像块大小为14×14。我们为多模态序列采用1k的上下文长度。为了提高训练效率，我们使用\texttt{bfloat16}、完全分片数据并行（FSDP）\citep{zhao2023pytorch}、\edit{激活}检查点和梯度累积进行训练。我们还对图像字幕数据集使用序列打包，以减少填充标记的数量。与之前的工作~\citep{hoffmann2022training,aghajanyan2023scalingmm,abnar2025parameters}类似，我们在交替（Obelics）、图像-字幕（HQITP）和仅文本数据（DCLM）上评估性能。更多实现细节请参见~\cref{app:implementation_details}。
