\section{缩放本地多模态模型}

在本节中，我们呈现了本地多模态模型的缩放规律研究，考察了不同的架构选择~\cref{sec:scaling_laws_early}，探索了不同的数据混合方式~\cref{sec:scaling_data_mix}，分析了后期融合和早期融合NMMs之间的实际权衡，并比较了本地预训练和持续预训练NMMs的性能~\cref{sec:native_vs_continual}。

\cpar{设置。} 我们训练的模型参数范围从0.3B到4B，缩放宽度，同时保持深度不变。对于较小的训练标记预算，我们将预热阶段减少到1K步，而对于较大的预算，则保持5K步。
根据~\citet{hagele2024scaling}，模型使用恒定学习率进行训练，然后使用逆平方根调度器进入冷却阶段。冷却阶段占总步骤的20\%，这些步骤在恒定学习率下进行。为了估算\Cref{eq:scaling_laws}中的缩放系数，我们应用了L-BFGS算法~\citep{lbfgs}和Huber损失~\citep{Huber1992}（$\delta = 10^{-3}$），并在初始化范围上进行网格搜索。

\input{tables/scaling_laws_coeffs}

\input{figs/latr_vs_early_equal_flops}

\vspace{-0.5cm}
\subsection{NMM 的缩放法则}
\label{sec:scaling_laws_early}

\cpar{早期融合与后期融合模型的缩放法则。}
\Cref{fig:early_vs_late_scaleflops_3d}~(左) 展示了早期融合 NMM 在交错数据、图文配对数据和纯文本数据上的最终损失均值。最低损失前沿遵循 FLOPs 的幂律函数。拟合该幂律得到表达式 $L \propto C^{-0.049}$，表明随着计算量增加，模型性能的提升速率。当分别按数据类型（如图文配对、交错、文本）分析缩放法则时，可以观察到指数有所差异（见 \cref{tab:early_vs_late_coeffs}）。例如，在图文配对数据上模型获得更高的提升速率 $(L \propto C^{-0.061})$，而在交错文档上为 $(L \propto C^{-0.046})$。

为了将损失建模为训练 token 数 $D$ 和模型参数量 $N$ 的函数，我们对 \cref{eq:scaling_laws} 中的参数化函数进行拟合，得到缩放指数 $\alpha = 0.301$ 和 $\beta = 0.335$。这两个指数分别描述了随着模型参数量和训练 token 数增加时的性能提升速率。假设计算量 $C$ 与 $N$ 和 $D$ 成线性关系（即 $C \propto ND$），我们推导了模型参数与计算预算之间的关系法则（详见 \cref{app:scaling_laws}）。具体而言，对于给定的计算预算 $C$，我们在对数间距的 $D$ 值下计算对应的模型大小 $N$，并确定能够最小化损失的参数数量 $N_{opt}$。在不同 FLOPs 值上重复此过程，生成 $(C, N_{opt})$ 数据集，并拟合一个幂律函数来预测最优模型大小与计算量之间的关系：$N^* \propto C^{0.526}.$

类似地，我们拟合幂律函数来估计最优训练数据集大小与计算量和模型大小之间的关系：
\[
D_{opt} \propto C^{0.473}, \quad D_{opt} \propto N^{0.899}.
\]
这些关系使实践者能够在固定计算预算下确定最优模型规模和数据集大小。按数据类型分析时，我们发现交错数据相比图文配对数据更受益于更大的模型规模（$a=0.532$ 对比 $a=0.520$），而在训练 token 数方面则表现出相反的趋势。

\input{figs/data_mixtures_scaling}

我们对后期融合模型也进行了类似研究，如~\cref{fig:early_vs_late_scaleflops_3d}~(右) 所示，并观察到类似的缩放行为。尤其是损失缩放指数 ($c = -0.0494$) 几乎与早期融合模型 ($c = -0.0492$) 相同。
该趋势在 \cref{fig:early_vs_late_scaleflops} 中尤为明显，其中早期融合在小规模模型下优于后期融合，而在更大模型规模下两者的性能趋于一致。我们还在调整后期融合配置（例如使用更小的视觉编码器和更大的文本解码器）时观察到相似趋势~\cref{app:late_vs_early}。

\begin{figure}[t!]
    \begin{minipage}[t]{0.48\textwidth}
        \input{figs/late_vs_early_efficiency}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \input{figs/training_mixtures}
    \end{minipage}
\end{figure}

\cpar{NMM 的缩放法则 \textit{vs} LLM。}
将我们 NMM 的缩放法则系数与文本专用 LLM（如 GPT-3、Chinchilla）中报告的系数进行比较，我们发现它们处于相近范围。特别是在预测损失与计算量的关系时，GPT-3~\citep{brown2020language} 遵循 $L \propto C^{-0.048}$，而我们的模型遵循 $L \propto C^{-0.049}$，表明 NMM 的性能遵循与 LLM 相似的缩放法则。
类似地，我们对 \cref{eq:scaling_laws} 中参数 $\alpha=0.301$, $\beta=0.335$ 的估计值，与~\citet{hoffmann2022training} 报告的值 ($\alpha=0.339$, $\beta=0.285$) 接近。同样地，我们计算的 $a=0.526$, $b=0.473$ 与~\citet{hoffmann2022training} 中的 $a=0.46$, $b=0.54$ 非常一致，进一步强化了这样一个观点：对于原生多模态模型，训练 token 数和模型参数应当按比例进行扩展。
然而，由于在 NMM 中 $a$ 与 $b$ 的差值小于 LLM，因此这一原则在 NMM 中体现得更为显著。此外，由于在我们模型中 $a=0.526$ 大于 $b=0.473$，所以在固定计算预算下，NMM 的最优模型规模大于 LLM，而最优训练 token 数则更少。

\cpar{早期融合 \textit{vs.} 后期融合 NMM 的计算最优权衡。}
虽然后期融合和早期融合模型在增加 FLOPs 时降低损失的速率相似，但它们在计算最优模型上存在显著差异。具体而言，后期融合模型的 $N_{opt}$ 更大，而早期融合模型的 $D_{opt}$ 更大。这表明在固定计算预算下，后期融合模型需要更多的参数，而早期融合模型则更依赖于更多的训练 token。
这一趋势也体现在 $\frac{N_{opt}}{D_{opt}} \propto C^{0.053}$（早期融合）相比 $\frac{N_{opt}}{D_{opt}} \propto C^{0.076}$（后期融合）更低。如 \cref{fig:teaser}~(右) 所示，在扩大 FLOPs 时，早期融合模型的参数数量显著减少，这对降低推理成本以及部署后的服务成本至关重要。

\input{tables/scaling_laws_coeffs_datamixture}

\cpar{早期融合训练更高效。}
我们比较了后期融合与早期融合架构的训练效率。如 \cref{fig:early_vs_late_efficiency} 所示，在相同计算预算下，早期融合模型消耗更少内存并训练更快。随着计算量的增加，这一优势更加显著，凸显了早期融合在保持与后期融合相当性能的同时具备更高的训练效率。值得注意的是，在相同 FLOPs 下，后期融合模型具有更多的参数数量以及更高的有效深度（即包含附加视觉编码器层和解码器层），相较于早期融合模型。

\begin{figure*}[h!]
    \centering
    \captionsetup{type=figure}

    \begin{minipage}[t]{0.55\linewidth}
        \centering
        \input{graphs/early_late/pred_loss_vs_loss_early_extrapolation}
        \caption{\textbf{观测损失与预测损失对比。} 我们可视化了由我们的缩放定律 \cref{eq:scaling_laws} 所预测的损失，以及每次运行实际达到的损失。我们能够可靠地预测比用于拟合缩放定律更大的模型（8B 参数）的性能。}
        \label{fig:observed_vs_predicted_loss_extrapolation}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\linewidth}
        \begin{minipage}[t]{\linewidth}
            \centering
            \vspace{-5.5cm}
            \setlength{\tabcolsep}{8pt}
            \renewcommand{\arraystretch}{1}
            \resizebox{1.0\linewidth}{!}{
            \begin{tabular}{lccc}
                Parameter & MSE & R2 & MAE (\%) \\
                \shline
                Held-in      & 0.0029  & 0.9807 & 0.8608 \\
                Held-out     & 0.0004 & 0.9682 & 0.5530 \\
            \end{tabular}
            }
            \captionof{table}{\textbf{Scaling laws prediction errors.} We report the mean square error, R2 and mean absolute error for the loss prediction for held-in and held-out (8B model) data.}
            \label{tab:scaling_laws_errors_main}
        \end{minipage}

        \begin{minipage}[t]{\linewidth}
            \centering
            \vspace{-2.8cm}
            \setlength{\tabcolsep}{12pt}
            \renewcommand{\arraystretch}{1}
            \resizebox{\linewidth}{!}{
            \begin{tabular}{lcc}
                Parameter & Avg & Std \\
                \shline
                $E$      & 1.80922  & 0.33811  \\
                $\alpha$ & 0.29842  & 0.10101  \\
                $\beta$  & 0.33209  & 0.02892  \\
                $a$      & 0.54302  & 0.08813  \\
                $b$      & 0.48301  & 0.05787  \\
                $d$      & 0.92375  & 0.23296  \\
            \end{tabular}
            }
            \captionof{table}{\textbf{Scaling laws sensitivity.} We report the mean and standard deviation after bootstrapping with 100 iterations.}
            \label{tab:scaling_laws_sensitivity_main}
        \end{minipage}
    \end{minipage}
\end{figure*}
\subsection{\edit{缩放定律评估}}
\label{sec:scaling_laws_evaluation}
对于每个模型大小和训练令牌数量，我们使用\cref{eq:scaling_laws}中估计的函数形式计算损失，并将其与我们实验中观察到的实际损失进行比较。\Cref{fig:observed_vs_predicted_loss_extrapolation} 和 \Cref{tab:scaling_laws_errors_main} 可视化了这些比较，结果表明我们的估计非常准确，尤其是在较低损失值和较大FLOP值下。我们还在外推设置中评估了我们的缩放定律，预测了超出用于拟合的模型大小的表现。值得注意的是，我们的方法能够合理准确地估计一个8B模型的表现。

此外，我们通过自助法进行灵敏度分析。具体来说，我们有放回地抽取\( P \)个点（\( P \)为训练模型的总数），并重新估计缩放定律的系数。该过程重复进行100次，我们报告每个系数的均值和标准差。\Cref{tab:scaling_laws_sensitivity_main} 显示，我们的估计在\(\beta\)上的精度高于\(\alpha\)，这主要是由于相对于用于推导缩放定律的不同令牌数量，模型大小的数量较少。
\subsection{不同数据混合比例的缩放规律}  
\label{sec:scaling_data_mix}  
我们研究了训练数据混合比例的变化如何影响原生多模态模型的缩放规律。为此，我们考察了四种不同的数据混合，反映了社区的常见做法~\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}，其图像描述-交错文本-纯文本的比例分别为 \colorbox{blue!10}{45-45-10}（我们的默认设置）、\colorbox{red!10}{30-30-40}、\colorbox{green!10}{40-20-40} 和 \colorbox{orange!10}{20-40-40}。对于每种混合比例，我们分别开展了缩放研究，共训练了 76 个不同的模型，遵循我们在 \cref{sec:scaling_laws_early} 中的实验设置。总体而言，\cref{fig:early_scaleflops_data_mixtures} 显示，不同的混合比例遵循相似的缩放趋势；然而，缩放系数会因混合比例而异（\cref{tab:scaling_laws_coeffs_data_mixtures}）。有趣的是，增加图像描述数据的比例（混合 1 和混合 2）会导致较低的 $a$ 和较高的 $b$，而增加交错文本和纯文本数据的比例（混合 3 和混合 4）则产生相反的效果。值得注意的是，图像描述数据中包含的图像标记多于文本标记；因此，提高其比例会带来更多的图像标记，而增加交错文本和纯文本数据则会增加文本标记数量。这表明，当图像标记占主导时，延长训练时长比扩大模型规模更能有效降低损失。我们还发现，在模型规模固定的情况下，提高纯文本和交错文本数据的比例有利于早期融合 \cref{fig:early_vs_late_datatype_interleaved_text_main}。
\subsection{原生多模态预训练 \textbf{\vs} 持续训练LLMs}
\label{sec:native_vs_continual}
在本节中，我们比较了从头开始的原生训练与从预训练LLM初始化后的持续训练。我们从DCLM-1B~\citep{fang2023data}初始化模型，该模型在超过2T的标记上进行了训练。\Cref{fig:early_vs_early_init_scaledata} 显示了原生多模态模型在训练更长时间后可以缩小与初始化模型之间的差距。具体而言，在图像标注数据上，该模型需要不到100B的多模态标记即可达到可比的性能。然而，在交替和文本数据上，模型可能需要更长的训练时间——最多达到1T标记。考虑到预训练的成本，这些结果表明，原生训练可能是实现相同多模态基准性能的更高效方法。

\input{figs/early_vs_early_init_scaledata}
\section{\edit{朝向多模态专业化}}

此前，我们展示了在固定计算预算下，早期融合模型的表现与晚期融合模型相当。然而，多模态数据本质上是异质的，训练一个统一的模型来适应如此多样化的分布可能是次优的。在这里，我们主张在统一架构内实现多模态专业化。理想情况下，模型应自动适应每种模态，例如，通过学习特定模态的权重或专门的专家。MoEs 是这一方法的有力候选，已在大规模语言模型（LLMs）中证明了其有效性。在本节中，我们强调稀疏早期融合模型相较于其密集对手的优势。

\cpar{设置。} 我们的稀疏模型基于~\citet{gale2023megablocks}的无丢弃 MoE 实现，该实现消除了由于专家容量限制而在训练过程中丢弃标记的问题。我们采用了一个 top-$k$ 专家选择路由机制，每个标记从 $E$ 个可用专家中选择其 top-$k$ 专家。具体而言，我们设置 $k=1$ 和 $E=8$，因为我们发现这一配置效果显著。此外，我们还结合了一个辅助负载平衡损失~\citep{shazeer2017outrageously}，其权重为 0.01，以确保专家的平衡利用。遵循~\citet{abnar2025parameters}，我们将训练过程的 FLOPs 计算为 $6ND$，其中 $N$ 代表活跃参数的数量。
\subsection{稀疏与密集NMM在扩展FLOPs时的比较}
我们通过训练具有不同数量的激活参数和不同数量训练令牌的模型，将稀疏MoE模型与其密集对应模型进行比较。 \cref{fig:dense_vs_moe_scaledata} 显示，在相同的推理成本（或激活参数数量）下，MoE显著优于密集模型。
有趣的是，这种性能差距在较小的模型尺寸下更为显著。这表明，MoE使得模型能够更有效地处理异质数据，并在不同的模态中进行专业化。然而，随着密集模型变得足够大，两种架构之间的差距逐渐缩小。

\vspace{15pt}
\subsection{稀疏早期融合模型的尺度定律}
我们训练了不同的模型（参数量从3亿到34亿不等），使用不同数量的标记（从2.5亿到6000亿不等），并在\cref{fig:early_scaleflops_moe_avg}中报告了最终的损失值。我们将幂律拟合到计算量（FLOPs）作为函数的最低损失的凸包上。有趣的是，指数（$-0.047$）接近密集型NMMs（$-0.049$）的指数，这表明这两种架构的扩展方式相似。然而，MoEs的乘法常数（$26.287$）比密集模型的（$29.574$）要小，这表明MoEs具有较低的损失。此外，MoEs需要比密集模型更长的训练时间才能达到饱和（有关更多细节，请参见\cref{app:scaling_laws}）。 \edit{我们还通过\edit{考虑$N$作为活跃参数的数量}来预测\cref{eq:scaling_laws}的系数。\Cref{tab:early_vs_late_coeffs}显示，与密集模型相比，$\alpha$显著较高。有趣的是，$b$显著高于$a$，这表明在训练稀疏NMMs时，训练标记应该以比参数数量更高的速率进行缩放。我们还实验了一种考虑稀疏性的尺度定律~\citep{abnar2025parameters}，并得出了类似的结论，详见\Cref{app:scaling_laws_moes}。}}
\subsection{基于模态的路由 \vs 无关模态的路由}

MoEs的另一种替代方案是基于模态的路由，其中多模态的token根据其模态分配给专家，类似于先前的工作~\citep{bao2021vlmo,wang2022image}。我们训练具有不同图像和文本专家的模型，专家的形式为FFN，其中图像token仅由图像FFN处理，文本token仅由文本FFN处理。与基于模态的路由相比，MoEs在图像-文本说明和交错数据上的表现显著更好，如~\cref{fig:hard_vs_moe_scaledata}所示。

\begin{figure}[t!]
    \begin{minipage}[t]{0.58\textwidth}
        \input{figs/dense_vs_moe_scaledata}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.38\textwidth}
        \input{figs/scaling_laws_moes}
    \end{minipage}
\end{figure}
\subsection{专家专精化与共享性的出现}  
\label{sec:specialization}  
我们研究了 MoE 架构中的多模态专精化现象。在~\cref{fig:tokens_assignment} 中，我们可视化了在各层中分配给每个专家的文本与图像 token 的归一化数量。为了量化这种专精化，我们计算了一个专精化得分，该得分定义为某一层中所有专家的 $1-H(p)$ 的平均值，其中 $H$ 是每个专家文本/图像 token 分布的二进制熵函数。我们在~\cref{fig:tokens_specialization} 中绘制了这一专精化得分。较高的专精化得分表示专家倾向于专注于文本或图像 token，而较低的得分则表示存在共享行为。这些可视化结果清晰地表明存在模态特定的专家，尤其是在前几层中。此外，随着层数的增加，专精化得分先是下降，然后在最后几层再次上升。这表明早期与末尾的层在模态专精化方面表现更强，而中间层则较弱。这一行为是合理的，因为中间层被认为包含可以跨模态泛化的高层特征， \edit{并且这也与 \citep{shukor2024implicit} 中的发现一致，该研究表明不同模态在层级中表现出越来越强的一致性}。我们在模态无关的 MoE 中观察到专家专精化与跨模态共享的共同出现，表明相比于模态感知稀疏性方法，该方法可能更具优势。此处展示的所有数据均来自一个早期融合的 MoE 模型，该模型包含 10 亿个激活参数，并在 3000 亿个 token 上进行了训练。

\input{tables/sft_results}

\vspace{-1cm}
\section{基于SFT的下游任务评估}
根据先前关于扩展法则的研究，我们主要依赖验证损失。然而，我们通常发现，这种评估与下游任务的表现高度相关。为了验证这一点，我们在LLaVA混合模型上进行了一阶段的多模态指令调优（SFT）\citep{liu2024improvedllava}，并报告了多个VQA和图像字幕生成任务中的准确率和CIDEr分数。 \cref{tab:sft} 确认了不同模型配置的排名。具体而言，早期融合优于晚期融合，MoEs优于密集型模型。然而，由于这些模型相对较小（1.5B规模），从头开始训练，并且在小型数据集上进行了微调，因此总体分数低于当前的最先进水平。更多的实现细节可以在~\Cref{app:implementation_details} 中找到。

\begin{figure}[t!]
    \begin{minipage}[t]{0.58\textwidth}
        \input{figs/soft_vs_hard_routing}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.38\textwidth}
        \input{figs/moe_entropy}
    \end{minipage}
    \vspace{3mm}
\end{figure}
