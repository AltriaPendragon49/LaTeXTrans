\vspace{-0.5cm}
\begin{abstract}

构建能够通过多模态信号有效感知世界的通用模型一直是一个长期目标。当前的方法包括整合分别预训练的组件，例如将视觉编码器连接到大语言模型（LLM）并\edit{继续进行多模态训练}。虽然这些方法展现出了显著的样本效率，但是否这种后融合架构本质上更优仍然是一个未解的问题。在这项工作中，我们重新审视了原生多模态模型（NMMs）的架构设计——这些模型从零开始在所有模态上进行训练——并进行了一项广泛的扩展法则研究，涵盖了457个具有不同架构和训练混合的训练模型。我们的研究表明，后融合架构并不比早期融合架构具有固有优势，后者不依赖于图像编码器。相反，早期融合在较低的参数量下展现出了更强的性能，训练效率更高，且更易于部署。受早期融合架构强大表现的启发，我们展示了通过引入专家混合模型（MoEs），能够\edit{让模型学习模态特定的权重，从而显著提高性能。}

\end{abstract}
