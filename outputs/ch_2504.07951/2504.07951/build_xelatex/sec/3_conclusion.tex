\section{\edit{讨论与}限制}

\cpar{\edit{多模态数据混合的扩展规律。}}我们的扩展规律研究涵盖了不同的模型配置和训练混合方式。虽然结果表明，在不同混合方式下扩展规律的系数在很大程度上保持一致，但仍需要更广泛地探索混合方式的变化，以验证这一观察并建立一个能够考虑该因素的统一扩展规律。

\cpar{\edit{扩展规律与下游任务性能的关系。}}与以往的扩展规律研究类似，我们的分析侧重于通过验证损失衡量的预训练性能。然而，这些发现能在多大程度上转化为下游任务的性能仍是一个悬而未决的问题，需要进一步研究。

\cpar{\edit{向更大规模的外推。}}随着 FLOPs 的增加，扩展规律预测的准确性也有所提升~\cref{app:scaling_laws}。\edit{此外，我们在外推到更大模型规模时验证了这些规律（\cref{sec:scaling_laws_evaluation}）。}然而，这些规律是否能够可靠地外推到极大模型规模仍是一个未解之题。

\cpar{\edit{高分辨率与早期融合模型。}}使用高分辨率输入训练早期融合模型会显著增加视觉 token 的数量。虽然池化技术已被广泛应用于后期融合模型，但早期融合可能需要采用替代方法。\edit{鉴于早期融合模型与大型语言模型的相似性，延长上下文长度的技术可能会带来益处。}

\cpar{\edit{多模态 MoE 模型的扩展规律。}}对于 MoE，我们仅考虑了单一配置（8 个专家的 top-1 路由）。\edit{我们发现该配置在我们的设置中表现良好，并遵循了标准的 MoE 实现。}然而，当对 MoE 架构进行进一步优化，或探索不同的负载均衡、路由策略\edit{或不同的专家实现}时，研究结果可能会有所不同。
\section{结论}
我们探讨了本地多模态模型的计算最优预训练的各种策略。我们发现，本地多模态模型（NMMs）遵循与大规模语言模型（LLMs）相似的扩展规律。与普遍看法相反，我们发现采用晚融合架构并没有比早融合架构具有固有优势。虽然这两种架构表现出相似的扩展特性，但早融合模型在训练上更为高效，并且在较低计算预算下优于晚融合模型。此外，我们还展示了稀疏架构鼓励模态特定的专业化，从而在保持相同推理成本的同时提高性能。
\section*{\edit{致谢}} 我们感谢 Philipp Dufter, Samira Abnar, Xiujun Li, Zhe Gan, Alexander Toshev, Yinfei Yang, Dan Busbridge 和 Jason Ramapuram 的许多富有成效的讨论。我们感谢 Denise Hui 和 Samy Bengio 提供的基础设施和计算支持。最后，我们感谢 Louis Béthune, Pierre Ablin, Marco Cuturi 和 Apple MLR 团队在整个项目中的支持。
