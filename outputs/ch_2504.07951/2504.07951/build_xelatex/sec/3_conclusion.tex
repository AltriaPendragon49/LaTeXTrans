\section{\edit{讨论与}局限性}

\cpar{\edit{多模态数据混合的缩放规律。}} 我们的缩放规律研究涵盖了不同的模型配置和训练混合。尽管结果表明缩放规律系数在不同混合之间大致保持一致，但仍需要更广泛地探索混合变体，以验证这一观察结果并建立一个统一的缩放规律来考虑这一因素。

\cpar{\edit{缩放规律与下游任务的表现。}} 类似于先前的缩放规律研究，我们的分析侧重于通过验证损失来衡量的预训练表现。然而，这些发现是否能够转化为下游任务的表现仍然是一个未解的问题，需要进一步的研究。

\cpar{\edit{外推到更大规模。}} 随着FLOPs的增加，缩放规律预测的准确性得到了提升~\cref{app:scaling_laws}。\edit{此外，我们在外推到更大的模型尺寸时验证了我们的规律（\cref{sec:scaling_laws_evaluation}）。} 然而，这些规律是否可以可靠地外推到极大的模型尺寸仍然是一个未解的问题。

\cpar{\edit{高分辨率和早期融合模型。}} 使用高分辨率输入训练早期融合模型会显著增加视觉token的数量。虽然池化技术已经广泛应用于晚期融合模型，但对于早期融合，可能需要采用不同的方法。\edit{鉴于早期融合模型与LLM的相似性，延长上下文长度的技术似乎是有益的。}

\cpar{\edit{多模态MoEs模型的缩放规律。}} 对于MoEs，我们只考虑单一配置（使用8个专家的top-1路由）。\edit{我们发现这种配置在我们的设置中效果较好，并遵循标准的MoEs实现。} 然而，研究结果可能会因优化\edit{更多} MoE架构或探索不同的负载平衡、路由策略\edit{或不同专家的实现}而有所不同。
\section{结论}
我们探讨了原生多模态模型计算最优预训练的各种策略。我们发现，原生多模态模型遵循与大规模语言模型相似的扩展规律。与普遍观点相反，我们发现采用后融合架构并没有比早期融合架构具有固有的优势。尽管这两种架构表现出相似的扩展特性，但早期融合模型在训练上更加高效，并且在较低的计算预算下超越后融合模型。此外，我们展示了稀疏架构促进了模态特定的专业化，在保持相同推理成本的情况下，带来了性能的提升。
\section*{\edit{致谢}} 我们感谢 Philipp Dufter、Samira Abnar、Xiujun Li、Zhe Gan、Alexander Toshev、Yinfei Yang、Dan Busbridge 和 Jason Ramapuram 的许多富有成效的讨论。我们感谢 Denise Hui 和 Samy Bengio 提供的基础设施和计算支持。最后，我们感谢 Louis Béthune、Pierre Ablin、Marco Cuturi 以及 Apple 的 MLR 团队在整个项目中的支持。
