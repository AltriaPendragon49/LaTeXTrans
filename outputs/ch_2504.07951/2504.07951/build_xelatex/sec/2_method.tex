\section{扩展原生多模态模型}

在本节中，我们对原生多模态模型进行了扩展规律研究，探讨了不同的架构选择~\cref{sec:scaling_laws_early}，探索了不同的数据混合方式~\cref{sec:scaling_data_mix}，分析了早融合与晚融合原生多模态模型之间的实际权衡，并比较了原生预训练与持续预训练在原生多模态模型中的表现~\cref{sec:native_vs_continual}。

\cpar{设置。}我们训练了激活参数规模从 0.3B 到 4B 的模型，在保持深度不变的前提下扩展模型宽度。对于较小的训练 token 预算，我们将预热阶段缩短为 1K 步，而对于较大的预算，则保持为 5K 步。  
按照~\citet{hagele2024scaling}的方法，模型采用恒定学习率进行训练，随后进入冷却阶段，使用反平方根调度器。冷却阶段占据了恒定学习率阶段总训练步数的 20\%。  
为了估计 \Cref{eq:scaling_laws} 中的扩展系数，我们采用 L-BFGS 算法~\citep{lbfgs} 和 Huber 损失函数~\citep{Huber1992}（其中 $\delta = 10^{-3}$），并在初始化范围上进行网格搜索。

\input{tables/scaling_laws_coeffs}

\input{figs/latr_vs_early_equal_flops}

\vspace{-0.5cm}
\subsection{NMMs的扩展法则}
\label{sec:scaling_laws_early}

\cpar{早期融合和后期融合模型的扩展法则。}
\Cref{fig:early_vs_late_scaleflops_3d}~(左图)展示了早期融合NMMs在交替、图像-标题和文本数据集上的最终损失平均值。最低损失的前沿遵循一个关于FLOP的幂律。拟合幂律得到表达式$L \propto C^{-0.049}$，表明随着计算量的增加，损失的改善速率。分析每种数据类型（例如，图像-标题、交替、文本）的扩展法则时，我们观察到指数值有所不同（见\cref{tab:early_vs_late_coeffs}）。例如，与交替文档$(L \propto C^{-0.046}$)相比，图像-标题数据模型在$(L \propto C^{-0.061})$上的改善速率更高。

为了将损失建模为训练标记数$D$和模型参数$N$的函数，我们拟合了\cref{eq:scaling_laws}中的参数函数，得到扩展指数$\alpha = 0.301$和$\beta = 0.335$。这些描述了在扩展模型参数和训练标记数时的改善速率。假设计算量、$N$和$D$之间存在线性关系（即，$C \propto ND$），我们推导出与计算预算相关的模型参数法则（详细信息见\cref{app:scaling_laws}）。具体而言，对于给定的计算预算$C$，我们在对数间隔的$D$值下计算对应的模型大小$N$，并确定$N_{opt}$，即最小化损失的参数数量。通过在不同FLOP值下重复这一过程，生成$(C, N_{opt})$的数据集，并拟合幂律来预测计算最优模型大小与计算量之间的关系：$N^* \propto C^{0.526}$。

类似地，我们拟合了幂律来估算计算最优训练数据集大小与计算量和模型大小之间的关系：
\[
D_{opt} \propto C^{0.473}, \quad D_{opt} \propto N^{0.899}.
\]
这些关系使得实践者能够在固定计算预算下确定最优的模型和数据集大小。通过按数据类型分析，我们发现，交替数据比图像-标题数据更受益于更大的模型（$a=0.532$），而训练标记的趋势则相反。

\input{figs/data_mixtures_scaling}

我们对后期融合模型进行类似的研究，如~\cref{fig:early_vs_late_scaleflops_3d}~(右图)所示，观察到类似的扩展行为。特别地，损失的扩展指数（$c = -0.0494$）与早期融合模型（$c = -0.0492$）几乎相同。这一趋势在\cref{fig:early_vs_late_scaleflops}中也有所体现，其中早期融合在较小的模型规模上优于后期融合，而两种架构在较大的模型规模上趋向于相似的性能。我们还观察到，当变化后期融合配置（例如，使用较小的视觉编码器和较大的文本解码器）时，也出现了类似的趋势，详细信息见\cref{app:late_vs_early}。

\begin{figure}[t!]
    \begin{minipage}[t]{0.48\textwidth}
        \input{figs/late_vs_early_efficiency}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \input{figs/training_mixtures}
    \end{minipage}
\end{figure}

\cpar{NMMs的扩展法则\textit{vs} LLMs。}
在将我们的NMMs的扩展法则系数与文本专用LLMs（例如，GPT-3、Chinchilla）报告的系数进行比较时，我们发现它们处于相似范围内。特别是，对于计算量函数的损失预测，GPT-3~\citep{brown2020language}遵循$L \propto C^{-0.048}$，而我们的模型遵循$L \propto C^{-0.049}$，表明NMMs的性能遵循与LLMs相似的扩展法则。同样，我们在\cref{eq:scaling_laws}中估算的$\alpha$和$\beta$参数（$\alpha=0.301$，$\beta=0.335$）与~\citet{hoffmann2022training}报告的（$\alpha=0.339$，$\beta=0.285$）非常接近。类似地，我们计算得到的$a=0.526$和$b=0.473$与~\citet{hoffmann2022training}中的$a=0.46$和$b=0.54$非常吻合，进一步证明对于原生多模态模型，训练标记数和模型参数应按比例扩展。然而，由于$a$和$b$之间的差距小于LLMs中的差距，这一原则在NMMs中更加显著。此外，由于我们这里的$a=0.526$大于$b=0.473$，因此在给定固定计算预算的情况下，NMMs的最优模型大小大于LLMs，而最优训练标记数则较低。

\cpar{早期融合\textit{vs.}后期融合NMMs的计算最优权衡。}
尽管后期融合和早期融合模型在增加FLOP时以相似的速率减少损失，但我们观察到它们的计算最优模型之间存在明显的权衡。具体而言，后期融合模型的$N_{opt}$更大，而早期融合模型的$D_{opt}$更大。这表明，在固定计算预算下，后期融合模型需要更多的参数，而早期融合模型则更受益于更多的训练标记。这一趋势也反映在早期融合模型的$\frac{N_{opt}}{D_{opt}} \propto C^{0.053}$较低，而后期融合模型为$\frac{N_{opt}}{D_{opt}} \propto C^{0.076}$。如\cref{fig:teaser}~(右图)所示，当扩展FLOP时，早期融合模型的参数数量显著减少，这对于降低推理成本以及部署后的服务成本至关重要。

\input{tables/scaling_laws_coeffs_datamixture}

\cpar{早期融合在训练中更高效。}
我们比较了后期融合和早期融合架构的训练效率。如\cref{fig:early_vs_late_efficiency}所示，早期融合模型在相同计算预算下消耗更少的内存，并且训练速度更快。随着计算量的增加，这一优势更加明显，突显了早期融合在保持与后期融合相似的性能的同时，具有更优的训练效率。值得注意的是，在相同的FLOP下，后期融合模型的参数数量和有效深度（即，除了解码器层外，还包括额外的视觉编码器层）高于早期融合模型。

\begin{figure*}[h!]
    \centering
    \captionsetup{type=figure}

    \begin{minipage}[t]{0.55\linewidth}
        \centering
        \input{graphs/early_late/pred_loss_vs_loss_early_extrapolation}
        \caption{\textbf{观测损失与预测损失对比。} 我们可视化了由我们的缩放律 \cref{eq:scaling_laws} 预测的损失，以及每次运行实际达到的损失。我们能够可靠地预测参数量更大（8B 参数）的模型的性能，即使这些模型未被用于拟合缩放律。}
        \label{fig:observed_vs_predicted_loss_extrapolation}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.4\linewidth}
        \begin{minipage}[t]{\linewidth}
            \centering
            \vspace{-5.5cm}
            \setlength{\tabcolsep}{8pt}
            \renewcommand{\arraystretch}{1}
            \resizebox{1.0\linewidth}{!}{
            \begin{tabular}{lccc}
                Parameter & MSE & R2 & MAE (\%) \\
                \shline
                Held-in      & 0.0029  & 0.9807 & 0.8608 \\
                Held-out     & 0.0004 & 0.9682 & 0.5530 \\
            \end{tabular}
            }
            \captionof{table}{\textbf{Scaling laws prediction errors.} We report the mean square error, R2 and mean absolute error for the loss prediction for held-in and held-out (8B model) data.}
            \label{tab:scaling_laws_errors_main}
        \end{minipage}

        \begin{minipage}[t]{\linewidth}
            \centering
            \vspace{-2.8cm}
            \setlength{\tabcolsep}{12pt}
            \renewcommand{\arraystretch}{1}
            \resizebox{\linewidth}{!}{
            \begin{tabular}{lcc}
                Parameter & Avg & Std \\
                \shline
                $E$      & 1.80922  & 0.33811  \\
                $\alpha$ & 0.29842  & 0.10101  \\
                $\beta$  & 0.33209  & 0.02892  \\
                $a$      & 0.54302  & 0.08813  \\
                $b$      & 0.48301  & 0.05787  \\
                $d$      & 0.92375  & 0.23296  \\
            \end{tabular}
            }
            \captionof{table}{\textbf{Scaling laws sensitivity.} We report the mean and standard deviation after bootstrapping with 100 iterations.}
            \label{tab:scaling_laws_sensitivity_main}
        \end{minipage}
    \end{minipage}
\end{figure*}
\subsection{\edit{尺度定律评估}}
\label{sec:scaling_laws_evaluation}
对于每个模型大小和训练标记数，我们使用\cref{eq:scaling_laws}中估计的函数形式计算损失，并将其与我们运行中观察到的实际损失进行比较。\Cref{fig:observed_vs_predicted_loss_extrapolation}和\Cref{tab:scaling_laws_errors_main}可视化了这些比较，显示我们的估计非常准确，特别是在较低损失值和更大FLOPs下。我们还在外推设置中评估了我们的尺度定律，预测超出拟合所使用的模型大小的性能。值得注意的是，我们的方法能够合理准确地估计8B模型的性能。

此外，我们还使用自助法进行敏感性分析。具体来说，我们使用替换抽样方法抽取\( P \)个点（\( P \)是训练模型的总数），并重新估计尺度定律系数。此过程重复100次，我们报告每个系数的均值和标准差。\Cref{tab:scaling_laws_sensitivity_main}显示，我们对于\(\beta\)的估计比\(\alpha\)的估计更为精确，主要是因为相对于用于推导尺度定律的不同标记数量，模型大小的数量较少。
\subsection{不同数据混合比例的缩放规律}  
\label{sec:scaling_data_mix}  
我们研究训练数据混合比例的变化如何影响原生多模态模型的缩放规律。为此，我们考察了四种不同的数据混合方式，这些方式反映了社区中的常见实践~\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}，其图文描述-交错文本-纯文本的比例分别为 \colorbox{blue!10}{45-45-10}（我们的默认设置）、\colorbox{red!10}{30-30-40}、\colorbox{green!10}{40-20-40} 和 \colorbox{orange!10}{20-40-40}。  
对于每种混合方式，我们按照 \cref{sec:scaling_laws_early} 中的设置，分别训练了 76 个模型进行独立的缩放研究。总体来看，\cref{fig:early_scaleflops_data_mixtures} 显示不同的数据混合方式遵循相似的缩放趋势；然而，缩放系数会随着混合比例的不同而发生变化（见 \cref{tab:scaling_laws_coeffs_data_mixtures}）。有趣的是，提高图文描述数据的比例（混合方式 1 和 2）会导致 $a$ 较低、$b$ 较高，而增加交错文本和纯文本数据的比例（混合方式 3 和 4）则呈现相反的趋势。  
值得注意的是，图文描述数据包含的图像 token 多于文本 token；因此，增加其比例会带来更多的图像 token，而增加交错文本和纯文本数据则会增加文本 token 的数量。这表明，在图像 token 占主导的情况下，相较于增大模型规模，延长训练时间能更快地降低损失。  
我们还发现，在模型规模固定的条件下，提高纯文本和交错文本数据的比例更有利于早期融合策略 \cref{fig:early_vs_late_datatype_interleaved_text_main}。
\subsection{原生多模态预训练 \textbf{\vs} 连续训练的 LLMs}
\label{sec:native_vs_continual}
在本节中，我们将从头开始进行原生训练与从预训练的 LLM 初始化后进行连续训练进行比较。我们从 DCLM-1B~\citep{fang2023data} 初始化模型，该模型在超过 2T 的 tokens 上进行了训练。 \Cref{fig:early_vs_early_init_scaledata} 显示了原生多模态模型在训练更长时间后，可以缩小与初始化模型之间的差距。具体来说，在图像字幕数据上，模型只需少于 100B 的多模态 tokens 就可以达到相当的性能。然而，在交织数据和文本数据上，模型可能需要更长时间的训练——最多需要 1T 的 tokens。考虑到预训练的成本，这些结果表明，原生训练可能是实现相同性能的多模态基准的更高效方法。

\input{figs/early_vs_early_init_scaledata}
\section{\edit{面向多模态专业化}}

之前，我们展示了在固定计算预算下，早期融合模型的表现与后期融合模型相当。然而，多模态数据本质上是异质的，训练一个统一的模型以适应如此多样的分布可能并不是最优的。在这里，我们主张在统一架构中进行多模态专业化。理想情况下，模型应当能够隐式地适应每种模态，例如通过学习模态特定的权重或专门的专家。MoEs（混合专家模型）是这一方法的有力候选，已经在大型语言模型（LLMs）中展示了其有效性。在本节中，我们重点介绍稀疏早期融合模型相较于其密集对手的优势。

\cpar{设置。} 我们的稀疏模型基于~\citet{gale2023megablocks} 的无丢弃 MoE 实现，该实现消除了由于专家容量约束导致的训练过程中丢弃 token 的问题。我们采用了一个 top-$k$ 专家选择路由机制，其中每个 token 在 $E$ 个可用专家中选择其 top-$k$ 专家。具体来说，我们设置 $k=1$ 和 $E=8$，因为我们发现这种配置效果良好。此外，我们结合了一个辅助负载均衡损失~\citep{shazeer2017outrageously}，其权重为 0.01，以确保专家的平衡使用。按照~\citet{abnar2025parameters} 的方法，我们计算训练的 FLOP 数量为 $6ND$，其中 $N$ 代表活跃参数的数量。
\subsection{稀疏与密集型NMMs在扩展FLOPs时的比较}
我们通过训练具有不同数量活跃参数和不同数量训练标记的模型，将稀疏MoE模型与其密集型对比。 \cref{fig:dense_vs_moe_scaledata} 显示，在相同的推理成本（或活跃参数数量）下，MoE模型明显优于密集型模型。
有趣的是，这一性能差距在较小模型的情况下更加明显。这表明MoE能够更有效地处理异构数据，并在不同模态中进行专门化。然而，随着密集型模型变得足够大，两种架构之间的差距逐渐缩小。

\vspace{15pt}
\subsection{稀疏早期融合模型的缩放规律}
我们在不同规模的模型上进行训练（从 3 亿到 34 亿活跃参数不等），并使用不同数量的标记（从 2.5 亿到 6000 亿不等），最终的损失结果见 \cref{fig:early_scaleflops_moe_avg}。我们将一个幂律拟合到最低损失的凸包上，并将其作为计算量（FLOPs）的函数。值得注意的是，指数（$-0.047$）接近于密集型 NMM（$-0.049$），表明这两种架构的扩展性相似。然而，MoEs 的乘法常数较小（$26.287$），与密集型模型（$29.574$）相比，显示出较低的损失。此外，MoEs 需要比密集型模型更长的训练时间才能达到饱和（更多细节见 \cref{app:scaling_laws}）。\edit{我们还通过 \edit{将 $N$ 视为活跃参数的数量} 来预测 \cref{eq:scaling_laws} 的系数。\Cref{tab:early_vs_late_coeffs} 显示，MoEs 相较于密集型模型，$\alpha$ 显著更高。有趣的是，$b$ 显著高于 $a$，这表明在训练稀疏型 NMM 时，训练标记的扩展速度应高于参数数量的扩展速度。我们还进行了一个考虑稀疏性的缩放规律实验~\citep{abnar2025parameters}，并得出了类似的结论 \Cref{app:scaling_laws_moes}。}}
\subsection{具备模态感知的路由 \vs 不具模态感知的路由}

另一种替代门控专家模型（MoEs）的方法是具备模态感知的路由，在这种方法中，多模态 token 会根据其模态被分配给不同的专家，这与先前的研究类似~\citep{bao2021vlmo,wang2022image}。我们训练了具有图像和文本两个不同专家的模型，这些专家以前馈神经网络（FFNs）的形式存在，其中图像 token 仅由图像 FFN 处理，而文本 token 仅由文本 FFN 处理。与具备模态感知的路由相比，MoEs 在图像-文本描述任务和交错数据上的表现都显著更好，如~\cref{fig:hard_vs_moe_scaledata} 所示。

\begin{figure}[t!]
    \begin{minipage}[t]{0.58\textwidth}
        \input{figs/dense_vs_moe_scaledata}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.38\textwidth}
        \input{figs/scaling_laws_moes}
    \end{minipage}
\end{figure}
\subsection{专家专精化与共享的出现}  
\label{sec:specialization}  
我们研究了 MoE 架构中的多模态专精化。在~\cref{fig:tokens_assignment} 中，我们可视化了在各层中分配给每个专家的文本和图像 token 的归一化数量。为了量化这种专精化，我们计算了一个专精化得分，其定义为某一层中所有专家的平均值，即 $1-H(p)$，其中 $H$ 表示每个专家的文本/图像 token 分布的二元熵。我们在~\cref{fig:tokens_specialization} 中绘制了该专精化得分。更高的专精化得分表示专家倾向于专注于文本或图像 token，而较低的得分则表示存在共享行为。这些可视化结果清晰地表明了模态特定专家的存在，特别是在前几层中。此外，随着层数的增加，专精化得分先是下降，随后在最后几层再次上升。这表明前层和后层相比中间层具有更高的模态专精化。这一行为是直观的，因为中间层通常包含更高级别的特征，这些特征可能在不同模态之间具有通用性，\edit{并且这一点与 \citep{shukor2024implicit} 的研究结果一致，该研究表明随着层数的增加，不同模态之间的对齐程度也在增强}。我们在模态无关的 MoE 中观察到专家专精化和跨模态共享的同时出现，表明该方法相比模态感知稀疏性可能是一种更优的选择。此处展示的所有数据均来自一个使用早期融合的 MoE 模型，该模型拥有 10 亿激活参数，并在 3000 亿个 token 上进行了训练。

\input{tables/sft_results}  

\vspace{-1cm}
\section{使用SFT在下游任务上的评估}
根据先前关于规模法则的研究，我们主要依赖验证损失。然而，我们通常发现这一评估与下游任务的表现有很好的相关性。为了验证这一点，我们在LLaVA混合模型 \citep{liu2024improvedllava} 上进行多模态指令调优阶段（SFT），并报告在多个VQA和图像描述任务中的准确率和CIDEr分数。\cref{tab:sft} 确认了不同模型配置的排名。具体而言，早期融合优于晚期融合，MoEs优于密集模型。然而，由于这些模型相对较小（1.5B规模），是从头开始训练的，并且在一个小数据集上进行了微调，因此整体得分低于当前的最先进水平。更多实现细节可以在~\Cref{app:implementation_details} 中找到。

\begin{figure}[t!]
    \begin{minipage}[t]{0.58\textwidth}
        \input{figs/soft_vs_hard_routing}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.38\textwidth}
        \input{figs/moe_entropy}
    \end{minipage}
    \vspace{3mm}
\end{figure}
