\section{引言}
\label{sec:intro}

多模态提供了丰富的信号，用于感知和理解世界。
视觉方面的进展
\citep{radford2021learning,oquab2023dinov2,zhai2023sigmoidsiglip,fini2024multimodalaimv2}， 
\edit{音频方面的进展 \citep{huang2022masked,elizalde2023clap,chen2022wavlm,hsu2021hubert}} 
以及语言模型方面的进展 \citep{achiam2023gpt4,team2023gemini,dubey2024llama3} 
使得强大的多模态模型得以发展，这些模型能够理解语言、图像和音频。 
一种常见的方法是将分别经过预训练的单模态模型进行拼接，\edit{例如，将视觉编码器连接到LLM的输入层~\citep{laurenccon2024mattersidefics2,shukor2023epalm,alayrac2022flamingo,
xue2024xgenblip3,beyer2024paligemma,wang2024qwen2,liu2024improvedllava,zhang2023videollama,kong2024audioflam,defossez2024moshi}.}

尽管这种方法看起来很方便，但是否这种后融合策略本质上是最优的 \edit{用于理解多模态信号} 仍然是一个悬而未决的问题。 
此外，随着大量多模态数据的可用，从单模态预训练初始化可能会适得其反，因为这可能引入偏见，阻止模型 \edit{充分利用跨模态的共同依赖性}。 
另一个挑战是扩展这些系统；每个组件（例如，视觉编码器、LLM）都有自己的超参数集，\edit{预训练数据混合物}，以及与数据量和计算量相关的 \edit{扩展属性}。 
一种更灵活的架构可能允许模型动态地在各个模态之间分配其容量，从而简化扩展工作。

在本研究中，我们重点研究了从头开始在多模态数据上训练的本地多模态模型的扩展属性。 
我们首先调查 \edit{常见的} 后融合架构是否具有内在优势，通过将其与早期融合模型进行比较，后者在处理原始多模态输入时不依赖于 \edit{专用视觉编码器}。
我们对早期和后期融合架构进行了扩展实验，推导出了扩展规律，以预测它们的性能和计算最优配置。
我们的研究结果表明，从头开始训练时，后融合并没有提供固有的优势。 相反，早期融合模型更高效，且更容易扩展。 
此外，我们观察到，本地多模态模型遵循与LLM相似的扩展规律~\citep{hoffmann2022training}，尽管在不同模态和数据集之间，扩展系数略有变化。 
我们的结果表明，模型参数和训练令牌应 \edit{大致相等地} 扩展，以实现最佳性能。 
此外，我们发现，不同的 \edit{多模态} 训练混合物表现出相似的整体趋势，表明我们的发现很可能能够推广到更广泛的设置。

虽然我们的研究结果偏向早期融合，但多模态数据本质上是异质的，这表明某种程度的参数专业化仍可能带来好处。 
为了 \edit{探讨} 这一点，我们 \edit{探索利用} 专家混合（MoEs）~\citep{shazeer2017outrageously}，这是一种使模型能够在各个模态之间以对称和并行的方式动态分配专用参数的技术， 
与后融合模型不同，后者是非对称的，并且按顺序处理数据。 
使用MoEs训练本地多模态模型显著提高了性能，并且 \edit{因此，} 加速了收敛。 
\edit{我们关于MoEs的扩展规律表明，训练令牌的扩展比活动参数的数量更为重要。 这种不平衡的扩展与密集模型的观察结果不同，因为稀疏模型的总参数数量较高。} 
\edit{此外，}我们的分析表明，专家往往会专注于不同的模态，特别是在早期和最后几层中，这种专业化尤为突出。

\input{figs/teaser}
\subsection{我们的研究发现总结}
我们的研究发现可以总结如下：

\cpar{原生早期融合和晚期融合表现相当：} \edit{从头开始训练的早期融合模型}与晚期融合模型表现相当，在低计算预算下，早期融合模型略有优势 (\cref{fig:early_vs_early_init_scaledata})。此外，我们的扩展规律研究表明，随着计算预算的增加，早期和晚期融合的计算最优模型表现相似~(\cref{fig:teaser} Left)。

\cpar{原生多模态模型与大型语言模型的扩展规律相似：} 原生多模态模型的扩展规律与仅文本的大型语言模型遵循类似的规律，尽管根据目标数据类型和训练混合的不同，扩展指数略有差异 (\cref{tab:early_vs_late_coeffs})。

\cpar{晚期融合需要更多的参数：} 与早期融合模型相比，计算最优的晚期融合模型需要更高的参数与数据比例 (\cref{fig:teaser} Right)。

\cpar{稀疏性显著提升早期融合的原生多模态模型：} 与其密集型模型相比，稀疏型多模态模型在相同推理成本下表现出显著的改进~(\cref{fig:dense_vs_moe_scaledata})。此外，它们在训练时会隐式地学习模态特定的权重~(\cref{fig:app_moes_specialization})。 \edit{此外，随着计算预算的增长，计算最优的模型更依赖于扩大训练令牌的数量，而非激活参数的数量 (\cref{fig:teaser} Right)。}

\cpar{对于稀疏型原生多模态模型，模态无关的路由优于模态感知路由：} 使用模态无关路由训练稀疏专家混合模型始终优于使用模态感知路由的模型 (\cref{fig:hard_vs_moe_scaledata})。

\vspace{-5pt}
