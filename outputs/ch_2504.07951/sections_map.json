[
    {
        "section": "-1",
        "content": "\n\\documentclass[10pt,letterpaper]{article}\n\n\\usepackage[preprint]{arxiv}\n\n\\widowpenalty10000\n\\clubpenalty10000\n\\setlength{\\textfloatsep}{1pt}\n\\setlength{\\abovecaptionskip}{2pt}\n\\setlength{\\belowcaptionskip}{2pt}\n\n\\setlength{\\abovedisplayskip}{1pt}\n\\setlength{\\belowdisplayskip}{1pt}\n\\setlength{\\abovedisplayshortskip}{1pt}\n\\setlength{\\belowdisplayshortskip}{1pt}\n\n\\setcitestyle{square, comma, numbers,sort&compress, super}\n\\usepackage{enumitem}\n\\usepackage{algpseudocode}\n\\usepackage[font=small,labelfont=bf]{caption}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{booktabs}\n\\usepackage{algorithm}\n\\usepackage{subcaption}\n\\usepackage[normalem]{ulem}\n\\usepackage{xparse}\n\\usepackage{pifont}\n\\usepackage{bm}\n\\usepackage{threeparttable}\n\\usepackage{etoolbox}\n\n\\usepackage{listings}\n\\usepackage{mwe}\n\\usepackage{makecell}\n\\usepackage{color, colortbl}\n\\usepackage{tabularx}\n\\usepackage{pifont}\n\\usepackage[accsupp]{axessibility}\n\\usepackage[symbol]{footmisc}\n\\usepackage{pgfplots}\n\\usetikzlibrary{spy}\n\\usepackage[scaled=0.85]{DejaVuSansMono}\n\\usepackage{minitoc}\n\n<PLACEHOLDER_preamble_begin><PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n\n<PLACEHOLDER_NEWCOMMAND_2>\n<PLACEHOLDER_NEWCOMMAND_3>\n\n<PLACEHOLDER_NEWCOMMAND_4>\n\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n\n<PLACEHOLDER_NEWCOMMAND_7>\n<PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n\n<PLACEHOLDER_NEWCOMMAND_11>\n\n\\newlength\\savewidth<PLACEHOLDER_NEWCOMMAND_12>\n\n\\newlength\\thinwidth<PLACEHOLDER_NEWCOMMAND_13>\n\n\\definecolor{Gray}{gray}{0.92}\n\\definecolor{DarkGray}{gray}{0.5}\n\\definecolor{LightLateColor}{rgb}{0.88,1,1}\n\\definecolor{altRowColor}{gray}{0.92}\n\\definecolor{highlightRowColor}{rgb}{0.9, 0.9, 1}\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n\n\\definecolor{GrayNumber}{gray}{0.5}\n\\definecolor{GrayXMark}{gray}{0.7}\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n\n<PLACEHOLDER_NEWCOMMAND_28>\n\n\\expandafter\\def\\expandafter\\normalsize\\expandafter{\n    \\normalsize\n    \\setlength\\abovedisplayskip{2pt}\n    \\setlength\\belowdisplayskip{8pt}\n    \\setlength\\abovedisplayshortskip{-5pt}\n    \\setlength\\belowdisplayshortskip{5pt}\n}\n\n\\captionsetup[table]{skip=7pt}\n\\captionsetup[table]{belowskip=15pt}\n<PLACEHOLDER_NEWCOMMAND_29>\n\n<PLACEHOLDER_NEWCOMMAND_30>\n\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.3}\n\\usepgfplotslibrary{external}\n\\usepgfplotslibrary{groupplots}\n\\usepgfplotslibrary{statistics}\n\\usetikzlibrary{spy}\n\n\\definecolor{LateGradStart}{HTML}{185D79}\n\\definecolor{LateGradEnd}{HTML}{7AC5E9}\n\n\\definecolor{EarlyGradStart}{HTML}{fd4b2f}\n\\definecolor{EarlyGradEnd}{HTML}{f19e18}\n\n\\definecolor{LateColor}{HTML}{1F78B4}\n\\definecolor{EarlyColor}{HTML}{D32F2F}\n\n\\definecolor{MOEGradStart}{HTML}{0a3431}\n\\definecolor{MOEGradEnd}{HTML}{43c197}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\\definecolor{CustomB}{HTML}{d68910}\n\\definecolor{CustomC}{HTML}{c75f25}\n\\definecolor{CustomD}{HTML}{2980b9}\n\n\\definecolor{DarkOrange}{HTML}{c75f25}\n\\definecolor{Brown}{HTML}{d68910}\n\\definecolor{Gold}{HTML}{b89d53}\n\\definecolor{Blue}{HTML}{2980b9}\n\n\\definecolor{Blue}{HTML}{0072B2}\n\\definecolor{Orange}{HTML}{D55E00}\n\n\\definecolor{Purple}{HTML}{6A3D9A}\n\\definecolor{Gold}{HTML}{E6AB02}\n\n\\definecolor{Teal}{HTML}{117A65}\n\\definecolor{Coral}{HTML}{E67E22}\n\n\\definecolor{NavyBlue}{HTML}{253494}\n\\definecolor{SoftYellow}{HTML}{FDD835}\n\n\\definecolor{DarkGreen}{HTML}{1B5E20}\n\n\\definecolor{SteelGray}{HTML}{37474F}\n\\definecolor{AquaBlue}{HTML}{26C6DA}\n\n\\definecolor{Burgundy}{HTML}{7B1FA2}\n\\definecolor{SkyBlue}{HTML}{4FC3F7}\n\n\\definecolor{OliveGreen}{HTML}{558B2F}\n\\definecolor{Tangerine}{HTML}{FF9800}\n\n\\definecolor{MidnightBlue}{HTML}{1A237E}\n\\definecolor{Peach}{HTML}{FFAB91}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Light3}{HTML}{f5d15d}\n\\definecolor{CustomA_Light2}{HTML}{f9e282}\n\\definecolor{CustomA_Light1}{HTML}{fce94f}\n\n\\definecolor{CustomA_Base}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Dark1}{HTML}{e1b10f}\n\\definecolor{CustomA_Dark2}{HTML}{d09e0f}\n\\definecolor{CustomA_Dark3}{HTML}{b88a0e}\n\n\\definecolor{CustomG_Light3}{HTML}{77b55c}\n\\definecolor{CustomG_Light2}{HTML}{7dbf65}\n\\definecolor{CustomG_Light1}{HTML}{85d373}\n\n\\definecolor{CustomG_Base}{HTML}{218838}\n\n\\definecolor{CustomG_Dark1}{HTML}{1b6b2e}\n\\definecolor{CustomG_Dark2}{HTML}{17562a}\n\\definecolor{CustomG_Dark3}{HTML}{124627}\n\n\\definecolor{CustomC_Light3}{HTML}{c97f4d}\n\\definecolor{CustomC_Light2}{HTML}{d98e60}\n\\definecolor{CustomC_Light1}{HTML}{e89e73}\n\n\\definecolor{CustomC_Base}{HTML}{b75b20}\n\n\\definecolor{CustomC_Dark1}{HTML}{9c4a1d}\n\\definecolor{CustomC_Dark2}{HTML}{8b4219}\n\\definecolor{CustomC_Dark3}{HTML}{7b3a15}\n\n\\definecolor{CustomD_Light3}{HTML}{46a8d4}\n\\definecolor{CustomD_Light2}{HTML}{63b6de}\n\\definecolor{CustomD_Light1}{HTML}{7ac5e9}\n\n\\definecolor{CustomD_Base}{HTML}{2980b9}\n\n\\definecolor{CustomD_Dark1}{HTML}{1f6a8a}\n\\definecolor{CustomD_Dark2}{HTML}{175d79}\n\\definecolor{CustomD_Dark3}{HTML}{134f66}\n\n\\definecolor{CustomP_Light3}{HTML}{b077d4}\n\\definecolor{CustomP_Light2}{HTML}{c08ae0}\n\\definecolor{CustomP_Light1}{HTML}{d1a0eb}\n\n\\definecolor{CustomP_Base}{HTML}{8e44ad}\n\n\\definecolor{CustomP_Dark1}{HTML}{732d91}\n\\definecolor{CustomP_Dark2}{HTML}{5e2377}\n\\definecolor{CustomP_Dark3}{HTML}{4b1b5f}\n\n\\pgfplotsset{\n    legend early_same_llm style/.style={\n        solid,\n        mark=*,\n        CustomA_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_same_params style/.style={\n        solid,\n        mark=*,\n        CustomB,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=diamond,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend early_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomC_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_8b style/.style={\n        solid,\n        mark=star,\n        black,\n        mark options={solid},\n        line width=1pt,\n        mark size=2pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomC_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomC_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomC_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style_dashed/.style={\n        dashed,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill style/.style={\n        solid,\n        mark=triangle,\n        yellow,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill_distdown style/.style={\n        solid,\n        mark=triangle,\n        red,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init_aimv2 style/.style={\n        solid,\n        mark=triangle,\n        brown,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early bars style/.style={\n        ybar,\n        bar width=0.4cm,\n        solid,\n        mark=none,\n        fill=CustomC_Base,\n        line width=0pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late style/.style={\n        solid,\n        mark=diamond,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomD_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomD_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomD_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomD_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomG_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_shape style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b_shape style/.style={\n        solid,\n        mark=pentagon,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b_shape style/.style={\n        solid,\n        mark=square,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b_shape style/.style={\n        solid,\n        mark=o,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomG_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend hard style/.style={\n        solid,\n        mark=*,\n        CustomP_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_2_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_vision style/.style={\n        dotted,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}<PLACEHOLDER_preamble_end>\n\n\\definecolor{citecolor}{HTML}{0071bc}\n\n\\crefname{section}{\\S}{\\S\\S}\n\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\n    Mustafa Shukor\\thanks{Work done during an internship at Apple.} \\\\\n    Sorbonne University\n    \\And\n    Enrico Fini \\\\\n    Apple\n    \\And\n    Victor Guilherme Turrisi da Costa \\\\\n    Apple\n    \\And\n    Matthieu Cord \\\\\n    Sorbonne University\n    \\And\n    Joshua Susskind \\\\\n    Apple\n    \\And\n    Alaaeldin El-Nouby \\\\\n    Apple\n}\n\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>",
        "trans_content": "\n\\documentclass[10pt,letterpaper]{article}\n\n\\usepackage[preprint]{arxiv}\n\n\\widowpenalty10000\n\\clubpenalty10000\n\\setlength{\\textfloatsep}{1pt}\n\\setlength{\\abovecaptionskip}{2pt}\n\\setlength{\\belowcaptionskip}{2pt}\n\n\\setlength{\\abovedisplayskip}{1pt}\n\\setlength{\\belowdisplayskip}{1pt}\n\\setlength{\\abovedisplayshortskip}{1pt}\n\\setlength{\\belowdisplayshortskip}{1pt}\n\n\\setcitestyle{square, comma, numbers,sort&compress, super}\n\\usepackage{enumitem}\n\\usepackage{algpseudocode}\n\\usepackage[font=small,labelfont=bf]{caption}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{booktabs}\n\\usepackage{algorithm}\n\\usepackage{subcaption}\n\\usepackage[normalem]{ulem}\n\\usepackage{xparse}\n\\usepackage{pifont}\n\\usepackage{bm}\n\\usepackage{threeparttable}\n\\usepackage{etoolbox}\n\n\\usepackage{listings}\n\\usepackage{mwe}\n\\usepackage{makecell}\n\\usepackage{color, colortbl}\n\\usepackage{tabularx}\n\\usepackage{pifont}\n\\usepackage[accsupp]{axessibility}\n\\usepackage[symbol]{footmisc}\n\\usepackage{pgfplots}\n\\usetikzlibrary{spy}\n\\usepackage[scaled=0.85]{DejaVuSansMono}\n\\usepackage{minitoc}\n\n<PLACEHOLDER_preamble_begin><PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n\n<PLACEHOLDER_NEWCOMMAND_2>\n<PLACEHOLDER_NEWCOMMAND_3>\n\n<PLACEHOLDER_NEWCOMMAND_4>\n\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n\n<PLACEHOLDER_NEWCOMMAND_7>\n<PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n\n<PLACEHOLDER_NEWCOMMAND_11>\n\n\\newlength\\savewidth<PLACEHOLDER_NEWCOMMAND_12>\n\n\\newlength\\thinwidth<PLACEHOLDER_NEWCOMMAND_13>\n\n\\definecolor{Gray}{gray}{0.92}\n\\definecolor{DarkGray}{gray}{0.5}\n\\definecolor{LightLateColor}{rgb}{0.88,1,1}\n\\definecolor{altRowColor}{gray}{0.92}\n\\definecolor{highlightRowColor}{rgb}{0.9, 0.9, 1}\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n\n\\definecolor{GrayNumber}{gray}{0.5}\n\\definecolor{GrayXMark}{gray}{0.7}\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n\n<PLACEHOLDER_NEWCOMMAND_28>\n\n\\expandafter\\def\\expandafter\\normalsize\\expandafter{\n    \\normalsize\n    \\setlength\\abovedisplayskip{2pt}\n    \\setlength\\belowdisplayskip{8pt}\n    \\setlength\\abovedisplayshortskip{-5pt}\n    \\setlength\\belowdisplayshortskip{5pt}\n}\n\n\\captionsetup[table]{skip=7pt}\n\\captionsetup[table]{belowskip=15pt}\n<PLACEHOLDER_NEWCOMMAND_29>\n\n<PLACEHOLDER_NEWCOMMAND_30>\n\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.3}\n\\usepgfplotslibrary{external}\n\\usepgfplotslibrary{groupplots}\n\\usepgfplotslibrary{statistics}\n\\usetikzlibrary{spy}\n\n\\definecolor{LateGradStart}{HTML}{185D79}\n\\definecolor{LateGradEnd}{HTML}{7AC5E9}\n\n\\definecolor{EarlyGradStart}{HTML}{fd4b2f}\n\\definecolor{EarlyGradEnd}{HTML}{f19e18}\n\n\\definecolor{LateColor}{HTML}{1F78B4}\n\\definecolor{EarlyColor}{HTML}{D32F2F}\n\n\\definecolor{MOEGradStart}{HTML}{0a3431}\n\\definecolor{MOEGradEnd}{HTML}{43c197}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\\definecolor{CustomB}{HTML}{d68910}\n\\definecolor{CustomC}{HTML}{c75f25}\n\\definecolor{CustomD}{HTML}{2980b9}\n\n\\definecolor{DarkOrange}{HTML}{c75f25}\n\\definecolor{Brown}{HTML}{d68910}\n\\definecolor{Gold}{HTML}{b89d53}\n\\definecolor{Blue}{HTML}{2980b9}\n\n\\definecolor{Blue}{HTML}{0072B2}\n\\definecolor{Orange}{HTML}{D55E00}\n\n\\definecolor{Purple}{HTML}{6A3D9A}\n\\definecolor{Gold}{HTML}{E6AB02}\n\n\\definecolor{Teal}{HTML}{117A65}\n\\definecolor{Coral}{HTML}{E67E22}\n\n\\definecolor{NavyBlue}{HTML}{253494}\n\\definecolor{SoftYellow}{HTML}{FDD835}\n\n\\definecolor{DarkGreen}{HTML}{1B5E20}\n\n\\definecolor{SteelGray}{HTML}{37474F}\n\\definecolor{AquaBlue}{HTML}{26C6DA}\n\n\\definecolor{Burgundy}{HTML}{7B1FA2}\n\\definecolor{SkyBlue}{HTML}{4FC3F7}\n\n\\definecolor{OliveGreen}{HTML}{558B2F}\n\\definecolor{Tangerine}{HTML}{FF9800}\n\n\\definecolor{MidnightBlue}{HTML}{1A237E}\n\\definecolor{Peach}{HTML}{FFAB91}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Light3}{HTML}{f5d15d}\n\\definecolor{CustomA_Light2}{HTML}{f9e282}\n\\definecolor{CustomA_Light1}{HTML}{fce94f}\n\n\\definecolor{CustomA_Base}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Dark1}{HTML}{e1b10f}\n\\definecolor{CustomA_Dark2}{HTML}{d09e0f}\n\\definecolor{CustomA_Dark3}{HTML}{b88a0e}\n\n\\definecolor{CustomG_Light3}{HTML}{77b55c}\n\\definecolor{CustomG_Light2}{HTML}{7dbf65}\n\\definecolor{CustomG_Light1}{HTML}{85d373}\n\n\\definecolor{CustomG_Base}{HTML}{218838}\n\n\\definecolor{CustomG_Dark1}{HTML}{1b6b2e}\n\\definecolor{CustomG_Dark2}{HTML}{17562a}\n\\definecolor{CustomG_Dark3}{HTML}{124627}\n\n\\definecolor{CustomC_Light3}{HTML}{c97f4d}\n\\definecolor{CustomC_Light2}{HTML}{d98e60}\n\\definecolor{CustomC_Light1}{HTML}{e89e73}\n\n\\definecolor{CustomC_Base}{HTML}{b75b20}\n\n\\definecolor{CustomC_Dark1}{HTML}{9c4a1d}\n\\definecolor{CustomC_Dark2}{HTML}{8b4219}\n\\definecolor{CustomC_Dark3}{HTML}{7b3a15}\n\n\\definecolor{CustomD_Light3}{HTML}{46a8d4}\n\\definecolor{CustomD_Light2}{HTML}{63b6de}\n\\definecolor{CustomD_Light1}{HTML}{7ac5e9}\n\n\\definecolor{CustomD_Base}{HTML}{2980b9}\n\n\\definecolor{CustomD_Dark1}{HTML}{1f6a8a}\n\\definecolor{CustomD_Dark2}{HTML}{175d79}\n\\definecolor{CustomD_Dark3}{HTML}{134f66}\n\n\\definecolor{CustomP_Light3}{HTML}{b077d4}\n\\definecolor{CustomP_Light2}{HTML}{c08ae0}\n\\definecolor{CustomP_Light1}{HTML}{d1a0eb}\n\n\\definecolor{CustomP_Base}{HTML}{8e44ad}\n\n\\definecolor{CustomP_Dark1}{HTML}{732d91}\n\\definecolor{CustomP_Dark2}{HTML}{5e2377}\n\\definecolor{CustomP_Dark3}{HTML}{4b1b5f}\n\n\\pgfplotsset{\n    legend early_same_llm style/.style={\n        solid,\n        mark=*,\n        CustomA_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_same_params style/.style={\n        solid,\n        mark=*,\n        CustomB,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=diamond,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend early_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomC_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_8b style/.style={\n        solid,\n        mark=star,\n        black,\n        mark options={solid},\n        line width=1pt,\n        mark size=2pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomC_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomC_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomC_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style_dashed/.style={\n        dashed,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill style/.style={\n        solid,\n        mark=triangle,\n        yellow,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill_distdown style/.style={\n        solid,\n        mark=triangle,\n        red,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init_aimv2 style/.style={\n        solid,\n        mark=triangle,\n        brown,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early bars style/.style={\n        ybar,\n        bar width=0.4cm,\n        solid,\n        mark=none,\n        fill=CustomC_Base,\n        line width=0pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late style/.style={\n        solid,\n        mark=diamond,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomD_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomD_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomD_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomD_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomG_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_shape style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b_shape style/.style={\n        solid,\n        mark=pentagon,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b_shape style/.style={\n        solid,\n        mark=square,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b_shape style/.style={\n        solid,\n        mark=o,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomG_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend hard style/.style={\n        solid,\n        mark=*,\n        CustomP_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_2_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_vision style/.style={\n        dotted,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}<PLACEHOLDER_preamble_end>\n\n\\definecolor{citecolor}{HTML}{0071bc}\n\n\\crefname{section}{\\S}{\\S\\S}\n\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\n    Mustafa Shukor\\thanks{Work done during an internship at Apple.} \\\\\n    Sorbonne University\n    \\And\n    Enrico Fini \\\\\n    Apple\n    \\And\n    Victor Guilherme Turrisi da Costa \\\\\n    Apple\n    \\And\n    Matthieu Cord \\\\\n    Sorbonne University\n    \\And\n    Joshua Susskind \\\\\n    Apple\n    \\And\n    Alaaeldin El-Nouby \\\\\n    Apple\n}\n\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>"
    },
    {
        "section": "0",
        "content": "\\begin{document}\n\\setcounter{tocdepth}{2}\n\\doparttoc\n<PLACEHOLDER_NEWCOMMAND_36>\n\\faketableofcontents\n\n\\maketitle\n\n<PLACEHOLDER_sec/0_abstract_begin>\\vspace{-0.5cm}\n<PLACEHOLDER_ENV_1><PLACEHOLDER_sec/0_abstract_end>\n<PLACEHOLDER_sec/1_intro_begin>",
        "trans_content": "\\begin{document}\n\\setcounter{tocdepth}{2}\n\\doparttoc\n<PLACEHOLDER_NEWCOMMAND_36>\n\\faketableofcontents\n\n\\maketitle\n\n<PLACEHOLDER_sec/0_abstract_begin>\\vspace{-0.5cm}\n<PLACEHOLDER_ENV_1><PLACEHOLDER_sec/0_abstract_end>\n<PLACEHOLDER_sec/1_intro_begin>"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\n\\label{sec:intro}\n\nMultimodality provides a rich signal for perceiving and understanding the world.\nAdvances in vision\n\\citep{radford2021learning,oquab2023dinov2,zhai2023sigmoidsiglip,fini2024multimodalaimv2},\n\\edit{audio \\citep{huang2022masked,elizalde2023clap,chen2022wavlm,hsu2021hubert}}\nand language models \\citep{achiam2023gpt4,team2023gemini,dubey2024llama3}\nhave enabled the development of powerful multimodal models that understand\nlanguage, images, and audio. A common approach involves grafting separately\npre-trained unimodal models, \\edit{such as connecting a vision encoder to the input\nlayer of an\nLLM~\\citep{laurenccon2024mattersidefics2,shukor2023epalm,alayrac2022flamingo,\nxue2024xgenblip3,beyer2024paligemma,wang2024qwen2,liu2024improvedllava,zhang2023videollama,kong2024audioflam,defossez2024moshi}.}\n\nAlthough this seems like a convenient approach, it remains an open question\nwhether such late-fusion strategies are inherently optimal \\edit{for\nunderstanding multimodal signals}.  Moreover, with abundant multimodal data\navailable, initializing from unimodal pre-training is potentially detrimental,\nas it may introduce biases that prevent the model from \\edit{fully leveraging\ncross-modality co-dependancies}.\nAn additional challenge is scaling such systems;  each component (e.g., vision\nencoder, LLM) has its own set of hyperparameters, \\edit{pre-training data\nmixtues}, and \\edit{scaling properties with respect to the amount} of data and\ncompute applied. A more flexible architecture might allow the model to\ndynamically allocate its capacity across modalities, simplifying scaling\nefforts.\n\nIn this work, we focus on the scaling properties of native multimodal models\ntrained from the ground up on multimodal data. We first investigate whether\n\\edit{the commonly adopted} late-fusion architectures hold an intrinsic\nadvantage by comparing them to early-fusion models, which process raw multimodal\ninputs without relying on \\edit{dedicated vision encoders}.\nWe conduct scaling experiments on early and late fusion architectures, deriving\nscaling laws to predict their performance and compute-optimal configurations.\nOur findings indicate that late fusion offers no inherent advantage when\n\\edit{trained} from scratch. Instead, early-fusion models are more efficient and\nare easier to scale. Furthermore, we observe that native multimodal models\nfollow scaling laws similar to those of LLMs~\\citep{hoffmann2022training},\nalbeit with slight variations in scaling coefficients across modalities and\ndatasets. Our results suggest that model parameters and training tokens should\nbe scaled \\edit{roughly equally} for optimal performance.\nMoreover, we find that different \\edit{multimodal} training mixtures exhibit\nsimilar overall trends, indicating that our findings are likely to generalize to\na broader range of settings.\n\nWhile our findings favor early fusion, multimodal data is inherently\nheterogeneous, suggesting that some degree of parameter specialization may still\noffer benefits. To \\edit{investigate} this, we \\edit{explore leveraging} Mixture\nof Experts (MoEs)~\\citep{shazeer2017outrageously}, a technique that enables the\nmodel to dynamically allocate specialized parameters across modalities in a\nsymmetric and parallel manner, in contrast to late-fusion models, which are\nasymmetric and process data sequentially. Training native multimodal models with\nMoEs results in significantly improved performance and \\edit{therefore,} faster\nconvergence. \\edit{Our scaling laws for MoEs suggest that scaling number of\ntraining tokens is more important the number of active parameters. This\nunbalanced scaling is different from what is observed for dense models, due to\nthe higher number of total parameters for sparse models.} \\edit{In addition,\n}our analysis reveals that experts tend to specialize in different modalities,\nwith this specialization being particularly prominent in the early and last\nlayers.\n\n<PLACEHOLDER_figs/teaser_begin>\n<PLACEHOLDER_ENV_2><PLACEHOLDER_figs/teaser_end>",
        "trans_content": "\\section{引言}\n\\label{sec:intro}\n\n多模态提供了丰富的信号，用于感知和理解世界。\n视觉的进展\n\\citep{radford2021learning,oquab2023dinov2,zhai2023sigmoidsiglip,fini2024multimodalaimv2}，\n\\edit{音频 \\citep{huang2022masked,elizalde2023clap,chen2022wavlm,hsu2021hubert}} \n以及语言模型 \\citep{achiam2023gpt4,team2023gemini,dubey2024llama3}\n促使了强大多模态模型的开发，这些模型能够理解语言、图像和音频。一种常见的方法是将分别预训练的单模态模型连接起来，\n\\edit{例如将视觉编码器连接到LLM的输入层~\\citep{laurenccon2024mattersidefics2,shukor2023epalm,alayrac2022flamingo,\nxue2024xgenblip3,beyer2024paligemma,wang2024qwen2,liu2024improvedllava,zhang2023videollama,kong2024audioflam,defossez2024moshi}。}\n\n尽管这种方法看起来很方便，但是否这种后期融合策略在本质上是最优的 \\edit{用于理解多模态信号}，仍然是一个未解的问题。此外，随着大量多模态数据的可用，从单模态预训练初始化可能具有潜在的负面影响，\n因为它可能引入偏差，阻止模型 \\edit{充分利用跨模态的共同依赖性}。\n另一个挑战是扩展这样的系统；每个组件（例如视觉编码器、LLM）都有自己的超参数集，\\edit{预训练数据混合物}，以及 \\edit{关于数据和计算资源应用的} 扩展特性。更灵活的架构可能允许模型在各模态之间动态分配其能力，从而简化扩展工作。\n\n在这项工作中，我们专注于从头开始训练的原生多模态模型的扩展特性。我们首先研究 \\edit{常用的} 后期融合架构是否在本质上具有优势，方法是将它们与早期融合模型进行比较，后者处理原始多模态输入，而无需依赖 \\edit{专用的视觉编码器}。\n我们对早期和后期融合架构进行了扩展实验，推导出扩展定律以预测它们的性能和计算最优配置。\n我们的研究结果表明，后期融合在 \\edit{从零开始训练} 时并没有固有的优势。相反，早期融合模型更高效，更易于扩展。此外，我们观察到，原生多模态模型遵循类似于LLMs的扩展定律~\\citep{hoffmann2022training}，尽管在跨模态和数据集的扩展系数上略有变化。我们的结果表明，模型参数和训练令牌应该 \\edit{大致相等} 地扩展，以获得最佳性能。\n此外，我们发现，不同的 \\edit{多模态} 训练混合物表现出类似的整体趋势，表明我们的研究结果可能适用于更广泛的设置。\n\n尽管我们的发现倾向于早期融合，但多模态数据本质上是异质的，这表明某种程度的参数专业化可能仍然会带来好处。为 \\edit{探讨} 这一点，我们 \\edit{尝试利用} 专家混合（Mixture of Experts, MoEs）~\\citep{shazeer2017outrageously}，这是一种使模型能够在各模态之间对称并行地动态分配专业化参数的技术，区别于后期融合模型，后者是非对称的，并且顺序地处理数据。使用MoEs训练原生多模态模型显著提高了性能，\\edit{因此}，收敛速度也更快。 \\edit{我们的MoEs扩展定律表明，训练令牌的数量比活跃参数的数量更为重要。这种不平衡的扩展与密集模型所观察到的不同，因为稀疏模型的总参数数量更高。} \\edit{此外，}我们的分析表明，专家通常会在不同的模态中进行专业化，这种专业化在早期和最后几层尤为显著。\n\n<PLACEHOLDER_figs/teaser_begin>\n<PLACEHOLDER_ENV_2><PLACEHOLDER_figs/teaser_end>"
    },
    {
        "section": "1_1",
        "content": "\\subsection{Summary of our findings}\nOur findings can be summarized as follows:\n\n\\cpar{Native early and late fusion perform on par:} \\edit{Early fusion models trained\nfrom scratch}\nperform on par with their late-fusion counterparts, with a\nslight advantage to early-fusion models for low compute budgets\n(\\cref{fig:early_vs_early_init_scaledata}). Furthermore, our scaling laws study\nindicates that the compute-optimal models for early and late fusion perform\nsimilarly as the compute budget increases~(\\cref{fig:teaser} Left).\n\n\\cpar{NMMs scale similarly to LLMs:} The scaling laws of native multimodal\nmodels follow similar laws as text-only LLMs with slightly varying  scaling\nexponents depending on the target data type and training mixture\n(\\cref{tab:early_vs_late_coeffs}).\n\n\\cpar{Late-fusion requires more parameters:}\nCompute-optimal late-fusion models require a higher parameters-to-data ratio\nwhen compared to early-fusion (\\cref{fig:teaser} Right).\n\n\\cpar{Sparsity significantly benefits early-fusion NMMs:} Sparse NMMs exhibit\nsignificant improvements compared to their dense counterparts at the same\ninference cost~(\\cref{fig:dense_vs_moe_scaledata}). Furthermore, they implicitly\nlearn modality-specific weights when trained with\nsparsity~(\\cref{fig:app_moes_specialization}). \\edit{In addition,\ncompute-optimal models rely more on scaling the number of training tokens than\nthe number of active parameters as the compute-budget grows (\\cref{fig:teaser} Right).}\n\n\\cpar{Modality-agnostic routing beats Modality-aware routing for Sparse NMMs:}\nTraining sparse mixture of experts with modality-agnostic routing consistently\noutperforms models with modality-aware routing\n(\\cref{fig:hard_vs_moe_scaledata}).\n\n\\vspace{-5pt}\n\n<PLACEHOLDER_sec/1_intro_end>\n<PLACEHOLDER_sec/1.5_preliminaries_begin>",
        "trans_content": "\\subsection{我们的研究发现总结}\n我们的研究发现可以总结如下：\n\n\\cpar{原生早期和晚期融合效果相当：} \\edit{从头训练的早期融合模型}\n与其晚期融合的对应模型表现相当，且在低计算预算下，早期融合模型稍微占优\n(\\cref{fig:early_vs_early_init_scaledata})。此外，我们的扩展法则研究表明，随着计算预算的增加，早期和晚期融合的计算最优模型表现相似~(\\cref{fig:teaser} 左图)。\n\n\\cpar{原生多模态模型的扩展与大语言模型相似：} 原生多模态模型的扩展法则与仅文本的大语言模型遵循相似的规律，只是根据目标数据类型和训练混合的不同，扩展指数略有不同\n(\\cref{tab:early_vs_late_coeffs})。\n\n\\cpar{晚期融合需要更多的参数：} 与早期融合相比，计算最优的晚期融合模型需要更高的参数与数据比率\n(\\cref{fig:teaser} 右图)。\n\n\\cpar{稀疏性显著提升早期融合原生多模态模型表现：} 在相同推理成本下，稀疏的原生多模态模型相比其密集型对照模型表现显著提升~(\\cref{fig:dense_vs_moe_scaledata})。此外，经过稀疏性训练后，它们会隐式地学习到模态特定的权重~(\\cref{fig:app_moes_specialization})。 \\edit{此外，计算最优模型在计算预算增加时，更依赖于扩展训练令牌的数量，而非活跃参数的数量 (\\cref{fig:teaser} 右图)。}\n\n\\cpar{对于稀疏的原生多模态模型，无模态感知路由优于有模态感知路由：} 训练稀疏的专家混合模型时，使用无模态感知路由的表现始终优于使用有模态感知路由的模型\n(\\cref{fig:hard_vs_moe_scaledata})。\n\n\\vspace{-5pt}\n\n<PLACEHOLDER_sec/1_intro_end>\n<PLACEHOLDER_sec/1.5_preliminaries_begin>"
    },
    {
        "section": "2+2_1",
        "content": "\\section{Preliminaries}\n\n\n\\subsection{Definitions}\n\n\\cpar{Native Multimodal Models (NMMs):}\nModels that are trained from scratch on all modalities simultaneously without\nrelying on pre-trained LLMs or vision encoders. Our focus is on the\nrepresentative image and text modalities, where the model processes both text\nand images as input and generates text as output.\n\n\\cpar{Early fusion:} Enabling multimodal interaction from the beginning, using\nalmost no modality-specific parameters (\\eg, except a linear layer to patchify\nimages). Using a single transformer model, this approach processes raw\nmultimodal \\edit{input—tokenized text and continuous image patches—with no\nimage discretization.} \\edit{We} refer to the main transformer as\nthe decoder.\n\n\\cpar{Late fusion:} Delaying the multimodal interaction \\edit{to} deeper layers,\ntypically after separate unimodal components \\edit{has processed} each modality\nindependently (\\eg, a vision encoder connected to an LLM).\n\n\\cpar{Modality-agnostic routing:} In sparse mixture-of-experts, \\edit{modality-agnostic} routing\nrefers to relying on a learned router module that is trained jointly with the\nmodel.\n\n\\cpar{Modality-aware routing:} Routing based on pre-defined rules such as\nrouting based on the modality type (\\eg, vision-tokens, token-tokens).\n\n<PLACEHOLDER_tables/notations_begin><PLACEHOLDER_ENV_3><PLACEHOLDER_tables/notations_end>",
        "trans_content": "\\section{预备知识}\n\n\\subsection{定义}\n\n\\cpar{原生多模态模型 (NMMs):}\n从头开始同时训练所有模态的模型，而不依赖于预训练的LLMs或视觉编码器。我们的重点是代表性的图像和文本模态，其中模型处理文本和图像作为输入，并生成文本作为输出。\n\n\\cpar{早期融合:} 从一开始就启用多模态交互，几乎不使用任何特定于模态的参数（例如，除了一个用于图像分块的线性层）。使用单一的变压器模型，这种方法处理原始的多模态 \\edit{输入——标记化文本和连续的图像块——而不进行图像离散化。} \\edit{我们} 将主要的变压器称为解码器。\n\n\\cpar{后期融合:} 将多模态交互 \\edit{推迟到} 更深层次，通常是在单独的单模态组件 \\edit{处理} 每个模态后独立进行（例如，连接到LLM的视觉编码器）。\n\n\\cpar{与模态无关的路由:} 在稀疏专家混合模型中，\\edit{与模态无关的}路由指的是依赖于一个与模型联合训练的学习路由模块。\n\n\\cpar{与模态相关的路由:} 基于预定义规则的路由，例如基于模态类型的路由（例如，视觉标记、标记标记）。\n\n<PLACEHOLDER_tables/notations_begin><PLACEHOLDER_ENV_3><PLACEHOLDER_tables/notations_end>"
    },
    {
        "section": "2_2",
        "content": "\\subsection{Scaling Laws}\nWe aim to understand the scaling properties of NMMs and how different\narchitectural choices influence trade-offs. To this end, we analyze our models\nwithin the scaling laws framework proposed by~\\citet{kaplan2020scaling,\nhoffmann2022training}.\nWe compute FLOPs based on the total number of parameters, using the\napproximation \\(C = 6ND\\), as adopted in prior\nwork~\\citep{hoffmann2022training,abnar2025parameters}. However, we modify this\nestimation to suit our setup: for late-fusion models, FLOPs is computed as\n\\(6(N_vD_v + ND)\\).\nWe consider a setup where, given a compute budget \\(C\\), our goal is to predict\nthe model’s final \\edit{loss}, as well as determine the optimal number of\nparameters \\edit{and} number of training tokens. Consistent with prior studies on LLM\nscaling~\\citep{hoffmann2022training}, we assume a power-law relationship between\nthe final model loss and both model size (\\(N\\)) and training tokens (\\(D\\)):\n\n<PLACEHOLDER_ENV_4>\n\n\\noindent Here, \\(E\\) represents the lowest achievable loss on the dataset,\nwhile \\(\\frac{A}{N^{\\alpha}}\\) captures the effect of increasing the number of\nparameters, where a larger model leads to lower loss, with the rate of\nimprovement governed by \\(\\alpha\\). Similarly, \\(\\frac{B}{D^{\\beta}}\\) accounts\nfor the benefits of a higher number of tokens, with \\(\\beta\\) determining the\nrate of improvement. Additionally, we assume a linear relationship between\ncompute budget (FLOPs) and both \\(N\\) and \\(D\\) (\\(C \\propto ND\\)). This further\nleads to power-law relationships detailed in \\cref{tab:power_laws}.\n\n<PLACEHOLDER_figs/scaling_laws_early_vs_late_begin>\n<PLACEHOLDER_ENV_5><PLACEHOLDER_figs/scaling_laws_early_vs_late_end>\n\n<PLACEHOLDER_ENV_6>",
        "trans_content": "\\subsection{尺度规律}\n我们旨在理解NMM的尺度属性以及不同架构选择如何影响权衡。为此，我们在~\\citet{kaplan2020scaling, hoffmann2022training}提出的尺度规律框架内分析我们的模型。我们根据总参数数量计算FLOPs，使用了先前工作中采用的近似公式 \\(C = 6ND\\)~\\citep{hoffmann2022training,abnar2025parameters}。然而，我们对这一估算进行了修改，以适应我们的设置：对于后融合模型，FLOPs的计算公式为 \\(6(N_vD_v + ND)\\)。\n\n我们考虑一种设置，其中，在给定计算预算 \\(C\\) 的情况下，我们的目标是预测模型的最终\\edit{损失}，并确定最佳的参数数量\\edit{和}训练令牌数量。与先前关于LLM尺度研究的一致~\\citep{hoffmann2022training}，我们假设最终模型损失与模型大小（\\(N\\)）和训练令牌（\\(D\\)）之间存在幂律关系：\n\n<PLACEHOLDER_ENV_4>\n\n\\noindent 其中，\\(E\\)表示数据集上可实现的最低损失，而 \\(\\frac{A}{N^{\\alpha}}\\) 捕捉了增加参数数量的效果，其中更大的模型导致较低的损失，改进的速率由 \\(\\alpha\\) 控制。类似地，\\(\\frac{B}{D^{\\beta}}\\) 说明了更多令牌的好处，\\(\\beta\\) 决定了改进的速率。此外，我们假设计算预算（FLOPs）与 \\(N\\) 和 \\(D\\) 之间存在线性关系（\\(C \\propto ND\\)）。这进一步导致了在\\cref{tab:power_laws}中详细描述的幂律关系。\n\n<PLACEHOLDER_figs/scaling_laws_early_vs_late_begin>\n<PLACEHOLDER_ENV_5><PLACEHOLDER_figs/scaling_laws_early_vs_late_end>\n\n<PLACEHOLDER_ENV_6>"
    },
    {
        "section": "2_3",
        "content": "\\subsection{Experimental setup}\n\\edit{Our models} are based on the autoregressive transformer\narchitecture~\\citep{vaswani2017attention} with SwiGLU\nFFNs~\\citep{shazeer2020glu} and QK-Norm~\\citep{dehghani2023scaling}\nfollowing~\\citet{li2024datacomp}. In early-fusion models, image patches are\nlinearly projected to match the text token dimension, while late-fusion follows\nthe CLIP architecture~\\citep{radford2021learning}. We adopt causal attention for\ntext tokens and bidirectional attention for image tokens, we found this to work\nbetter. Training is conducted on a mixture of public and private multimodal\ndatasets, including DCLM \\citep{li2024datacomp}, Obelics\n\\citep{laurenccon2024obelics}, DFN \\citep{fang2023data}, COYO\n\\citep{kakaobrain2022coyo700m}, and a private collection of High-Quality\nImage-Text Pairs (HQITP) (see \\cref{tab:pretraining_datasets}). Images are\n\\edit{resized} to 224×224 resolution with a 14×14 patch size. We use a context\nlength of 1k for the multimodal sequences. For training efficiency, we train our\nmodels with \\texttt{bfloat16},  Fully Sharded Data Parallel (FSDP)\n\\citep{zhao2023pytorch}, \\edit{activation} checkpointing, and gradient accumulation. We\nalso use sequence packing for the image captioning dataset to reduce the amount\nof padded tokens. Similar to previous\nworks~\\citep{hoffmann2022training,aghajanyan2023scalingmm,abnar2025parameters},\nwe evaluate performance on a held-out subsets of interleaved (Obelics),\nImage-caption (HQITP), and text-only data (DCLM). Further implementation details\nare provided in~\\cref{app:implementation_details}.<PLACEHOLDER_sec/1.5_preliminaries_end>\n<PLACEHOLDER_sec/2_method_begin>",
        "trans_content": "\\subsection{实验设置}\n\\edit{我们的模型} 基于自回归变换器架构~\\citep{vaswani2017attention}，使用SwiGLU FFN~\\citep{shazeer2020glu} 和QK-Norm~\\citep{dehghani2023scaling}，遵循~\\citet{li2024datacomp}。在早期融合模型中，图像块被线性映射以匹配文本标记维度，而后期融合遵循CLIP架构~\\citep{radford2021learning}。我们对文本标记采用因果注意力，对图像标记采用双向注意力，发现这样效果更好。训练在公开和私人多模态数据集的混合上进行，包括DCLM \\citep{li2024datacomp}、Obelics \\citep{laurenccon2024obelics}、DFN \\citep{fang2023data}、COYO \\citep{kakaobrain2022coyo700m}，以及一个私有的高质量图像-文本对（HQITP）集合（见\\cref{tab:pretraining_datasets}）。图像被\\edit{调整大小}至224×224分辨率，图像块大小为14×14。我们使用1k的上下文长度来处理多模态序列。为了提高训练效率，我们使用\\texttt{bfloat16}、完全分片数据并行（FSDP）\\citep{zhao2023pytorch}、\\edit{激活}检查点和梯度累积。我们还使用序列打包来处理图像标注数据集，以减少填充标记的数量。与之前的工作~\\citep{hoffmann2022training,aghajanyan2023scalingmm,abnar2025parameters}类似，我们在交替的（Obelics）、图像-标注（HQITP）和仅文本数据（DCLM）上评估性能。更多的实现细节请参见~\\cref{app:implementation_details}。<PLACEHOLDER_sec/1.5_preliminaries_end>\n<PLACEHOLDER_sec/2_method_begin>"
    },
    {
        "section": "3",
        "content": "\\section{Scaling  native multimodal models}\n\nIn this section, we present a scaling laws study of native multimodal models,\nexamining various architectural choices~\\cref{sec:scaling_laws_early}, exploring\ndifferent data mixtures~\\cref{sec:scaling_data_mix}, analyzing the practical\ntrade-offs between late and early fusion\nNMMs, and comparing the performance of native\npre-training and continual pre-training of NMMs~\\cref{sec:native_vs_continual}.\n\n\\cpar{Setup.} We train models ranging from 0.3B to 4B active parameters,\nscaling the width while keeping the depth constant. For smaller training token\nbudgets, we reduce the warm-up phase to 1K steps while maintaining 5K steps for\nlarger budgets.\nFollowing~\\citet{hagele2024scaling}, models are trained with a constant learning\nrate, followed by a cool-down phase using an inverse square root scheduler. The\ncool-down phase spans 20\\% of the total steps spent at the constant learning\nrate.  To estimate the scaling coefficients in \\Cref{eq:scaling_laws}, we apply the\nL-BFGS algorithm~\\citep{lbfgs} and Huber loss~\\citep{Huber1992} (with $\\delta =\n10^{-3}$), performing a grid search over initialization ranges.\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_begin><PLACEHOLDER_ENV_7><PLACEHOLDER_tables/scaling_laws_coeffs_end>\n\n<PLACEHOLDER_figs/latr_vs_early_equal_flops_begin><PLACEHOLDER_ENV_8><PLACEHOLDER_figs/latr_vs_early_equal_flops_end>\n\n\\vspace{-0.5cm}",
        "trans_content": "\\section{扩展原生多模态模型}\n\n在本节中，我们对原生多模态模型进行了扩展规律研究，探讨了不同的架构选择~\\cref{sec:scaling_laws_early}，探索了不同的数据混合方式~\\cref{sec:scaling_data_mix}，分析了早融合与晚融合原生多模态模型之间的实际权衡，并比较了原生预训练与持续预训练在原生多模态模型中的表现~\\cref{sec:native_vs_continual}。\n\n\\cpar{设置。}我们训练了激活参数规模从 0.3B 到 4B 的模型，在保持深度不变的前提下扩展模型宽度。对于较小的训练 token 预算，我们将预热阶段缩短为 1K 步，而对于较大的预算，则保持为 5K 步。  \n按照~\\citet{hagele2024scaling}的方法，模型采用恒定学习率进行训练，随后进入冷却阶段，使用反平方根调度器。冷却阶段占据了恒定学习率阶段总训练步数的 20\\%。  \n为了估计 \\Cref{eq:scaling_laws} 中的扩展系数，我们采用 L-BFGS 算法~\\citep{lbfgs} 和 Huber 损失函数~\\citep{Huber1992}（其中 $\\delta = 10^{-3}$），并在初始化范围上进行网格搜索。\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_begin><PLACEHOLDER_ENV_7><PLACEHOLDER_tables/scaling_laws_coeffs_end>\n\n<PLACEHOLDER_figs/latr_vs_early_equal_flops_begin><PLACEHOLDER_ENV_8><PLACEHOLDER_figs/latr_vs_early_equal_flops_end>\n\n\\vspace{-0.5cm}"
    },
    {
        "section": "3_1",
        "content": "\\subsection{Scaling laws of NMMs}\n\\label{sec:scaling_laws_early}\n\n\\cpar{Scaling laws for early-fusion and late-fusion models.}\n\\Cref{fig:early_vs_late_scaleflops_3d}~(left) presents the final loss averaged\nacross interleaved, image-caption, and text datasets for early-fusion NMMs. The\nlowest-loss frontier follows a power law as a function of FLOPs. Fitting the\npower law yields the expression $L \\propto C^{-0.049}$, indicating the rate of\nimprovement with increasing compute. When analyzing the scaling laws per data\ntype (\\eg, image-caption, interleaved, text), we observe that the exponent\nvaries (\\cref{tab:early_vs_late_coeffs}). For instance, the model achieves a\nhigher rate of improvement for image-caption data $(L \\propto C^{-0.061})$ when\ncompared to interleaved documents $(L \\propto C^{-0.046}$).\n\nTo model the loss as a function of the number of training tokens $D$ and model\nparameters $N$, we fit the parametric function in \\cref{eq:scaling_laws}, obtaining\nscaling exponents $\\alpha = 0.301$ and $\\beta = 0.335$. These describe the rates\nof improvement when scaling the number of model parameters and training tokens, respectively.\nAssuming a linear relationship between compute, $N$, and $D$ (\\ie, $C \\propto ND$),\nwe derive the law relating model parameters to the compute budget (see\n\\cref{app:scaling_laws} for details). Specifically, for a given compute budget\n$C$, we compute the corresponding model size $N$ at logarithmically spaced $D$\nvalues and determine $N_{opt}$, the parameter count that minimizes loss.\nRepeating this across different FLOPs values produces a dataset of $(C,\nN_{opt})$, to which we fit a power law predicting the compute-optimal model size\nas a function of compute: $N^* \\propto C^{0.526}.$\n\nSimilarly, we fit power laws to estimate the compute-optimal training dataset\nsize as a function of compute and model size:\n\\[\nD_{opt} \\propto C^{0.473}, \\quad D_{opt} \\propto N^{0.899}.\n\\]\nThese relationships allow practitioners to determine the optimal model and\ndataset size given a fixed compute budget. When analyzing by data type, we find\nthat interleaved data benefits more from larger models ($a=0.532$) compared to\nimage-caption data ($a=0.520$), whereas the opposite trend holds for training\ntokens.\n\n<PLACEHOLDER_figs/data_mixtures_scaling_begin>\n<PLACEHOLDER_ENV_9>\n<PLACEHOLDER_figs/data_mixtures_scaling_end>\n\nWe conduct a similar study on late-fusion models\nin~\\cref{fig:early_vs_late_scaleflops_3d}~(right) and observe comparable scaling\nbehaviors. In particular, the loss scaling exponent ($c = -0.0494$) is nearly\nidentical to that of early fusion ($c = -0.0492$).\nThis trend is evident in \\cref{fig:early_vs_late_scaleflops}, where early fusion\noutperforms late fusion at smaller model scales, while both architectures\nconverge to similar performance at larger model sizes. We also observe similar\ntrends when varying late-fusion configurations, such as using a smaller vision\nencoder with a larger text decoder~\\cref{app:late_vs_early}.\n\n<PLACEHOLDER_ENV_10>\n\n\\cpar{Scaling laws of NMMs \\textit{vs} LLMs.}\nUpon comparing the scaling law coefficients of our NMMs to those reported for\ntext-only LLMs (\\eg, GPT-3, Chinchilla), we find them to be within similar\nranges. In particular, for predicting the loss as a function of compute,\nGPT-3~\\citep{brown2020language} follows $L \\propto C^{-0.048}$, while our models\nfollow $L \\propto C^{-0.049}$, suggesting that the performance of NMMs adheres\nto similar scaling laws as LLMs.\nSimilarly, our estimates of the $\\alpha$ and $\\beta$ parameters in\n\\cref{eq:scaling_laws} ($\\alpha=0.301$, $\\beta=0.335$) closely match those\nreported by~\\citet{hoffmann2022training} ($\\alpha=0.339$, $\\beta=0.285$).\nLikewise, our computed values of $a=0.526$ and $b=0.473$ align closely with\n$a=0.46$ and $b=0.54$ from~\\citet{hoffmann2022training}, reinforcing the idea\nthat, for native multimodal models, the number of training tokens and model\nparameters should be scaled proportionally.\nHowever, since the gap between $a$ and $b$ is smaller than in LLMs, this\nprinciple holds even more strongly for NMMs. Additionally, as $a=0.526$ is\ngreater than $b=0.473$ in our case, the optimal model size for NMMs is larger\nthan that of LLMs, while the optimal number of training tokens is lower, given\na fixed compute budget.\n\n\\cpar{Compute-optimal trade-offs for early \\textit{vs.} late fusion NMMs.}\nWhile late- and early-fusion models reduce loss at similar rates with increasing\nFLOPs, we observe distinct trade-offs in their compute-optimal models.\nSpecifically, $N_{opt}$ is larger for late-fusion models, whereas $D_{opt}$ is\nlarger for early-fusion models. This indicates that, given a fixed compute\nbudget, late-fusion models require a higher number of parameters, while early-fusion\nmodels benefit more from a higher number of training tokens.\nThis trend is also reflected in the lower $\\frac{N_{opt}}{D_{opt}} \\propto\nC^{0.053}$ for early fusion compared to $\\frac{N_{opt}}{D_{opt}} \\propto\nC^{0.076}$ for late fusion. As shown in \\cref{fig:teaser}~(right), when scaling FLOPs,\nthe number of parameters of early fusion models becomes significantly lower, which is crucial\nfor reducing inference costs and, consequently, lowering serving costs after\ndeployment.\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_begin><PLACEHOLDER_ENV_11><PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_end>\n\n\\cpar{Early-fusion is more efficient to train.}\nWe compare the training efficiency of late- and early-fusion architectures. As shown in \\cref{fig:early_vs_late_efficiency}, early-fusion models consume less memory and train faster under the same compute budget. This advantage becomes even more pronounced as compute increases, highlighting the superior training efficiency of early fusion while maintaining comparable performance to late fusion at scale. Notably, for the same FLOPs, late-fusion models have a higher parameter count and higher effective depth (\\ie, additional vision encoder layers alongside decoder layers) compared to early-fusion models.\n\n<PLACEHOLDER_ENV_12>",
        "trans_content": "\\subsection{NMMs的扩展法则}\n\\label{sec:scaling_laws_early}\n\n\\cpar{早期融合和后期融合模型的扩展法则。}\n\\Cref{fig:early_vs_late_scaleflops_3d}~(左图)展示了早期融合NMMs在交替、图像-标题和文本数据集上的最终损失平均值。最低损失的前沿遵循一个关于FLOP的幂律。拟合幂律得到表达式$L \\propto C^{-0.049}$，表明随着计算量的增加，损失的改善速率。分析每种数据类型（例如，图像-标题、交替、文本）的扩展法则时，我们观察到指数值有所不同（见\\cref{tab:early_vs_late_coeffs}）。例如，与交替文档$(L \\propto C^{-0.046}$)相比，图像-标题数据模型在$(L \\propto C^{-0.061})$上的改善速率更高。\n\n为了将损失建模为训练标记数$D$和模型参数$N$的函数，我们拟合了\\cref{eq:scaling_laws}中的参数函数，得到扩展指数$\\alpha = 0.301$和$\\beta = 0.335$。这些描述了在扩展模型参数和训练标记数时的改善速率。假设计算量、$N$和$D$之间存在线性关系（即，$C \\propto ND$），我们推导出与计算预算相关的模型参数法则（详细信息见\\cref{app:scaling_laws}）。具体而言，对于给定的计算预算$C$，我们在对数间隔的$D$值下计算对应的模型大小$N$，并确定$N_{opt}$，即最小化损失的参数数量。通过在不同FLOP值下重复这一过程，生成$(C, N_{opt})$的数据集，并拟合幂律来预测计算最优模型大小与计算量之间的关系：$N^* \\propto C^{0.526}$。\n\n类似地，我们拟合了幂律来估算计算最优训练数据集大小与计算量和模型大小之间的关系：\n\\[\nD_{opt} \\propto C^{0.473}, \\quad D_{opt} \\propto N^{0.899}.\n\\]\n这些关系使得实践者能够在固定计算预算下确定最优的模型和数据集大小。通过按数据类型分析，我们发现，交替数据比图像-标题数据更受益于更大的模型（$a=0.532$），而训练标记的趋势则相反。\n\n<PLACEHOLDER_figs/data_mixtures_scaling_begin>\n<PLACEHOLDER_ENV_9>\n<PLACEHOLDER_figs/data_mixtures_scaling_end>\n\n我们对后期融合模型进行类似的研究，如~\\cref{fig:early_vs_late_scaleflops_3d}~(右图)所示，观察到类似的扩展行为。特别地，损失的扩展指数（$c = -0.0494$）与早期融合模型（$c = -0.0492$）几乎相同。这一趋势在\\cref{fig:early_vs_late_scaleflops}中也有所体现，其中早期融合在较小的模型规模上优于后期融合，而两种架构在较大的模型规模上趋向于相似的性能。我们还观察到，当变化后期融合配置（例如，使用较小的视觉编码器和较大的文本解码器）时，也出现了类似的趋势，详细信息见\\cref{app:late_vs_early}。\n\n<PLACEHOLDER_ENV_10>\n\n\\cpar{NMMs的扩展法则\\textit{vs} LLMs。}\n在将我们的NMMs的扩展法则系数与文本专用LLMs（例如，GPT-3、Chinchilla）报告的系数进行比较时，我们发现它们处于相似范围内。特别是，对于计算量函数的损失预测，GPT-3~\\citep{brown2020language}遵循$L \\propto C^{-0.048}$，而我们的模型遵循$L \\propto C^{-0.049}$，表明NMMs的性能遵循与LLMs相似的扩展法则。同样，我们在\\cref{eq:scaling_laws}中估算的$\\alpha$和$\\beta$参数（$\\alpha=0.301$，$\\beta=0.335$）与~\\citet{hoffmann2022training}报告的（$\\alpha=0.339$，$\\beta=0.285$）非常接近。类似地，我们计算得到的$a=0.526$和$b=0.473$与~\\citet{hoffmann2022training}中的$a=0.46$和$b=0.54$非常吻合，进一步证明对于原生多模态模型，训练标记数和模型参数应按比例扩展。然而，由于$a$和$b$之间的差距小于LLMs中的差距，这一原则在NMMs中更加显著。此外，由于我们这里的$a=0.526$大于$b=0.473$，因此在给定固定计算预算的情况下，NMMs的最优模型大小大于LLMs，而最优训练标记数则较低。\n\n\\cpar{早期融合\\textit{vs.}后期融合NMMs的计算最优权衡。}\n尽管后期融合和早期融合模型在增加FLOP时以相似的速率减少损失，但我们观察到它们的计算最优模型之间存在明显的权衡。具体而言，后期融合模型的$N_{opt}$更大，而早期融合模型的$D_{opt}$更大。这表明，在固定计算预算下，后期融合模型需要更多的参数，而早期融合模型则更受益于更多的训练标记。这一趋势也反映在早期融合模型的$\\frac{N_{opt}}{D_{opt}} \\propto C^{0.053}$较低，而后期融合模型为$\\frac{N_{opt}}{D_{opt}} \\propto C^{0.076}$。如\\cref{fig:teaser}~(右图)所示，当扩展FLOP时，早期融合模型的参数数量显著减少，这对于降低推理成本以及部署后的服务成本至关重要。\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_begin><PLACEHOLDER_ENV_11><PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_end>\n\n\\cpar{早期融合在训练中更高效。}\n我们比较了后期融合和早期融合架构的训练效率。如\\cref{fig:early_vs_late_efficiency}所示，早期融合模型在相同计算预算下消耗更少的内存，并且训练速度更快。随着计算量的增加，这一优势更加明显，突显了早期融合在保持与后期融合相似的性能的同时，具有更优的训练效率。值得注意的是，在相同的FLOP下，后期融合模型的参数数量和有效深度（即，除了解码器层外，还包括额外的视觉编码器层）高于早期融合模型。\n\n<PLACEHOLDER_ENV_12>"
    },
    {
        "section": "3_2",
        "content": "\\subsection{\\edit{Scaling laws evaluation}}\n\\label{sec:scaling_laws_evaluation}\nFor each model size and number of training tokens, we compute the loss using the\nestimated functional form in \\cref{eq:scaling_laws} and compare it to the actual\nloss observed in our runs. \\Cref{fig:observed_vs_predicted_loss_extrapolation}\nand \\Cref{tab:scaling_laws_errors_main} visualizes these comparisons, showing\nthat our estimation is highly accurate, particularly for lower loss values and\nlarger FLOPs. We also assess our scaling laws in an extrapolation setting,\npredicting performance beyond the model sizes used for fitting. Notably, our\napproach estimates the performance of an 8B model with reasonable accuracy.\n\nAdditionally, we conduct a sensitivity analysis using bootstrapping.\nSpecifically, we sample \\( P \\) points with replacement (\\( P \\) being the total\nnumber of trained models) and re-estimate the scaling law coefficients. This\nprocess is repeated 100 times, and we report the mean and standard deviation of\neach coefficient. \\Cref{tab:scaling_laws_sensitivity_main} shows that our\nestimation is more precise for \\(\\beta\\) than for \\(\\alpha\\), primarily due to\nthe smaller number of model sizes relative to the number of different token\ncounts used to derive the scaling laws.",
        "trans_content": "\\subsection{\\edit{尺度定律评估}}\n\\label{sec:scaling_laws_evaluation}\n对于每个模型大小和训练标记数，我们使用\\cref{eq:scaling_laws}中估计的函数形式计算损失，并将其与我们运行中观察到的实际损失进行比较。\\Cref{fig:observed_vs_predicted_loss_extrapolation}和\\Cref{tab:scaling_laws_errors_main}可视化了这些比较，显示我们的估计非常准确，特别是在较低损失值和更大FLOPs下。我们还在外推设置中评估了我们的尺度定律，预测超出拟合所使用的模型大小的性能。值得注意的是，我们的方法能够合理准确地估计8B模型的性能。\n\n此外，我们还使用自助法进行敏感性分析。具体来说，我们使用替换抽样方法抽取\\( P \\)个点（\\( P \\)是训练模型的总数），并重新估计尺度定律系数。此过程重复100次，我们报告每个系数的均值和标准差。\\Cref{tab:scaling_laws_sensitivity_main}显示，我们对于\\(\\beta\\)的估计比\\(\\alpha\\)的估计更为精确，主要是因为相对于用于推导尺度定律的不同标记数量，模型大小的数量较少。"
    },
    {
        "section": "3_3",
        "content": "\\subsection{Scaling laws for different data mixtures}\n\\label{sec:scaling_data_mix}\nWe investigate how variations in the training mixture affect the scaling laws of\nnative multimodal models. To this end, we study four different mixtures that\nreflect common community\npractices~\\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila},\nwith Image Caption-Interleaved-Text ratios of \\colorbox{blue!10}{45-45-10} (our default setup),\n\\colorbox{red!10}{30-30-40}, \\colorbox{green!10}{40-20-40}, and \\colorbox{orange!10}{20-40-40}.\nFor each mixture, we conduct a separate scaling study by training 76 different\nmodels, following our setup in \\cref{sec:scaling_laws_early}. Overall,\n\\cref{fig:early_scaleflops_data_mixtures} shows that different mixtures follow\nsimilar scaling trends; however, the scaling coefficients vary depending on the\nmixture (\\cref{tab:scaling_laws_coeffs_data_mixtures}). Interestingly,\nincreasing the proportion of image-caption data (mixtures 1 and 2) leads to\nlower $a$ and higher $b$, whereas increasing the ratio of interleaved and text\ndata (mixtures 3 and 4) have the opposite effect.\nNotably, image-caption data contains more image tokens than text\ntokens; therefore, increasing its proportion results in more\nimage tokens, while increasing interleaved and text data increases text token\ncounts. This suggests that, when image tokens are prevalent, training for longer decreases the loss faster than increasing the model size.\nWe also found that for a fixed model size, increasing text-only and interleaved data ratio is in favor of\nearly-fusion \\cref{fig:early_vs_late_datatype_interleaved_text_main}.",
        "trans_content": "\\subsection{不同数据混合比例的缩放规律}  \n\\label{sec:scaling_data_mix}  \n我们研究训练数据混合比例的变化如何影响原生多模态模型的缩放规律。为此，我们考察了四种不同的数据混合方式，这些方式反映了社区中的常见实践~\\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}，其图文描述-交错文本-纯文本的比例分别为 \\colorbox{blue!10}{45-45-10}（我们的默认设置）、\\colorbox{red!10}{30-30-40}、\\colorbox{green!10}{40-20-40} 和 \\colorbox{orange!10}{20-40-40}。  \n对于每种混合方式，我们按照 \\cref{sec:scaling_laws_early} 中的设置，分别训练了 76 个模型进行独立的缩放研究。总体来看，\\cref{fig:early_scaleflops_data_mixtures} 显示不同的数据混合方式遵循相似的缩放趋势；然而，缩放系数会随着混合比例的不同而发生变化（见 \\cref{tab:scaling_laws_coeffs_data_mixtures}）。有趣的是，提高图文描述数据的比例（混合方式 1 和 2）会导致 $a$ 较低、$b$ 较高，而增加交错文本和纯文本数据的比例（混合方式 3 和 4）则呈现相反的趋势。  \n值得注意的是，图文描述数据包含的图像 token 多于文本 token；因此，增加其比例会带来更多的图像 token，而增加交错文本和纯文本数据则会增加文本 token 的数量。这表明，在图像 token 占主导的情况下，相较于增大模型规模，延长训练时间能更快地降低损失。  \n我们还发现，在模型规模固定的条件下，提高纯文本和交错文本数据的比例更有利于早期融合策略 \\cref{fig:early_vs_late_datatype_interleaved_text_main}。"
    },
    {
        "section": "3_4",
        "content": "\\subsection{Native multimodal pre-training \\textbf{\\vs} continual training of\nLLMs}\n\\label{sec:native_vs_continual}\nIn this section, we compare training natively from scratch to continual training\nafter initializing from a pre-trained LLM. We initialize the model from DCLM-1B~\\citep{fang2023data} that is trained on more than 2T tokens.\n\\Cref{fig:early_vs_early_init_scaledata} shows that native multimodal models can\nclose the gap with initialized models when trained for longer.\nSpecifically, on image captioning data, the model requires fewer than 100B\nmultimodal tokens to reach comparable performance. However, on interleaved and\ntext data, the model may need longer training—up to 1T tokens.\nConsidering the cost of pre-training, these results suggest that training\nnatively could be a more efficient approach for achieving the same performance on multimodal benchmarks.\n\n<PLACEHOLDER_figs/early_vs_early_init_scaledata_begin>\n<PLACEHOLDER_ENV_13><PLACEHOLDER_figs/early_vs_early_init_scaledata_end>",
        "trans_content": "\\subsection{原生多模态预训练 \\textbf{\\vs} 连续训练的 LLMs}\n\\label{sec:native_vs_continual}\n在本节中，我们将从头开始进行原生训练与从预训练的 LLM 初始化后进行连续训练进行比较。我们从 DCLM-1B~\\citep{fang2023data} 初始化模型，该模型在超过 2T 的 tokens 上进行了训练。 \\Cref{fig:early_vs_early_init_scaledata} 显示了原生多模态模型在训练更长时间后，可以缩小与初始化模型之间的差距。具体来说，在图像字幕数据上，模型只需少于 100B 的多模态 tokens 就可以达到相当的性能。然而，在交织数据和文本数据上，模型可能需要更长时间的训练——最多需要 1T 的 tokens。考虑到预训练的成本，这些结果表明，原生训练可能是实现相同性能的多模态基准的更高效方法。\n\n<PLACEHOLDER_figs/early_vs_early_init_scaledata_begin>\n<PLACEHOLDER_ENV_13><PLACEHOLDER_figs/early_vs_early_init_scaledata_end>"
    },
    {
        "section": "4",
        "content": "\\section{\\edit{Towards multimodal specialization}}\nPreviously, we demonstrated that early-fusion models achieve performance on par\nwith late-fusion models under a fixed compute budget. However, multimodal data\nis inherently heterogeneous, and training a unified model to fit such diverse\ndistributions may be suboptimal.\nHere, we argue for multimodal specialization within a unified architecture.\nIdeally, the model should implicitly adapt to each modality, for instance, by\nlearning modality-specific weights or specialized experts. MoEs is\na strong candidate for this approach, having demonstrated effectiveness in LLMs.\nIn this section, we highlight the advantages of sparse early-fusion models over\ntheir dense counterparts.\n\n\\cpar{Setup.} Our sparse models are based on the dropless-MoE implementation\nof~\\citet{gale2023megablocks}, which eliminates token dropping during training\ncaused by expert capacity constraints. We employ a top-$k$ expert-choice routing\nmechanism, where each token selects its top-$k$ experts among the $E$ available\nexperts. Specifically, we set $k=1$ and $E=8$, as we find this configuration to\nwork effectively.\nAdditionally, we incorporate an auxiliary load-balancing\nloss~\\citep{shazeer2017outrageously} with a weight of 0.01 to ensure a balanced\nexpert utilization. Following~\\citet{abnar2025parameters}, we compute training\nFLOPs as $6ND$, where $N$ represents the number of active parameters.",
        "trans_content": "\\section{\\edit{面向多模态专业化}}\n\n之前，我们展示了在固定计算预算下，早期融合模型的表现与后期融合模型相当。然而，多模态数据本质上是异质的，训练一个统一的模型以适应如此多样的分布可能并不是最优的。在这里，我们主张在统一架构中进行多模态专业化。理想情况下，模型应当能够隐式地适应每种模态，例如通过学习模态特定的权重或专门的专家。MoEs（混合专家模型）是这一方法的有力候选，已经在大型语言模型（LLMs）中展示了其有效性。在本节中，我们重点介绍稀疏早期融合模型相较于其密集对手的优势。\n\n\\cpar{设置。} 我们的稀疏模型基于~\\citet{gale2023megablocks} 的无丢弃 MoE 实现，该实现消除了由于专家容量约束导致的训练过程中丢弃 token 的问题。我们采用了一个 top-$k$ 专家选择路由机制，其中每个 token 在 $E$ 个可用专家中选择其 top-$k$ 专家。具体来说，我们设置 $k=1$ 和 $E=8$，因为我们发现这种配置效果良好。此外，我们结合了一个辅助负载均衡损失~\\citep{shazeer2017outrageously}，其权重为 0.01，以确保专家的平衡使用。按照~\\citet{abnar2025parameters} 的方法，我们计算训练的 FLOP 数量为 $6ND$，其中 $N$ 代表活跃参数的数量。"
    },
    {
        "section": "4_1",
        "content": "\\subsection{Sparse vs dense NMMs when scaling FLOPs}\nWe compare sparse MoE models to their dense counterparts by training models with different numbers of active parameters and varying amounts of training tokens. \\cref{fig:dense_vs_moe_scaledata} shows that, under the same inference cost (or number of active parameters), MoEs significantly outperform dense models.\nInterestingly, this performance gap is more pronounced for smaller model sizes. This suggests that MoEs enable models to handle heterogeneous data more effectively and specialize in different modalities. However, as dense models become sufficiently large, the gap between the two architectures gradually closes.\n\n\\vspace{15pt}",
        "trans_content": "\\subsection{稀疏与密集型NMMs在扩展FLOPs时的比较}\n我们通过训练具有不同数量活跃参数和不同数量训练标记的模型，将稀疏MoE模型与其密集型对比。 \\cref{fig:dense_vs_moe_scaledata} 显示，在相同的推理成本（或活跃参数数量）下，MoE模型明显优于密集型模型。\n有趣的是，这一性能差距在较小模型的情况下更加明显。这表明MoE能够更有效地处理异构数据，并在不同模态中进行专门化。然而，随着密集型模型变得足够大，两种架构之间的差距逐渐缩小。\n\n\\vspace{15pt}"
    },
    {
        "section": "4_2",
        "content": "\\subsection{Scaling laws for sparse early-fusion models}\nWe train different models (ranging from 300M to 3.4B active parameters) on\nvarying amounts of tokens (ranging from 250M to 600B) and report the final loss\nin \\cref{fig:early_scaleflops_moe_avg}. We fit a power law to the convex hull of\nthe lowest loss as a function of compute (FLOPs). Interestingly, the exponent\n($-0.047$) is close to that of dense NMMs ($-0.049$), indicating that both\narchitectures scale similarly. However, the multiplicative constant is smaller\nfor MoEs ($26.287$) compared to dense models ($29.574$), revealing lower loss.\nAdditionally, MoEs require longer training to reach saturation compared to dense\nmodels (\\cref{app:scaling_laws} for more details). \\edit{We also predict the\ncoefficients of \\cref{eq:scaling_laws} by \\edit{considering $N$ as the number of\nactive parameters. \\Cref{tab:early_vs_late_coeffs} shows significantly higher\n$\\alpha$ compared to dense models. Interestingly, $b$ is significantly higher\nthan $a$, revealing that the training tokens should be scaled at a higher rate\nthan the number of parameters when training sparse NMMs. We also experiment with a\nscaling law that takes into account the sparsity~\\citep{abnar2025parameters} and\nreached similar conclusions \\Cref{app:scaling_laws_moes}.}}",
        "trans_content": "\\subsection{稀疏早期融合模型的缩放规律}\n我们在不同规模的模型上进行训练（从 3 亿到 34 亿活跃参数不等），并使用不同数量的标记（从 2.5 亿到 6000 亿不等），最终的损失结果见 \\cref{fig:early_scaleflops_moe_avg}。我们将一个幂律拟合到最低损失的凸包上，并将其作为计算量（FLOPs）的函数。值得注意的是，指数（$-0.047$）接近于密集型 NMM（$-0.049$），表明这两种架构的扩展性相似。然而，MoEs 的乘法常数较小（$26.287$），与密集型模型（$29.574$）相比，显示出较低的损失。此外，MoEs 需要比密集型模型更长的训练时间才能达到饱和（更多细节见 \\cref{app:scaling_laws}）。\\edit{我们还通过 \\edit{将 $N$ 视为活跃参数的数量} 来预测 \\cref{eq:scaling_laws} 的系数。\\Cref{tab:early_vs_late_coeffs} 显示，MoEs 相较于密集型模型，$\\alpha$ 显著更高。有趣的是，$b$ 显著高于 $a$，这表明在训练稀疏型 NMM 时，训练标记的扩展速度应高于参数数量的扩展速度。我们还进行了一个考虑稀疏性的缩放规律实验~\\citep{abnar2025parameters}，并得出了类似的结论 \\Cref{app:scaling_laws_moes}。}}"
    },
    {
        "section": "4_3",
        "content": "\\subsection{Modality-aware \\vs Modality-agnostic routing}\n\nAnother alternative to MoEs is modality-aware routing, where multimodal tokens are assigned to experts based on their modalities, similar\nto previous works~\\citep{bao2021vlmo,wang2022image}. We train models with\ndistinct image and text experts in the form of FFNs, where image tokens are\nprocessed only by the image FFN and text tokens only by the text FFN. Compared to modality-aware routing, MoEs exhibit significantly better performance on both image-caption and interleaved data as presented in~\\cref{fig:hard_vs_moe_scaledata}.\n\n<PLACEHOLDER_ENV_14>",
        "trans_content": "\\subsection{具备模态感知的路由 \\vs 不具模态感知的路由}\n\n另一种替代门控专家模型（MoEs）的方法是具备模态感知的路由，在这种方法中，多模态 token 会根据其模态被分配给不同的专家，这与先前的研究类似~\\citep{bao2021vlmo,wang2022image}。我们训练了具有图像和文本两个不同专家的模型，这些专家以前馈神经网络（FFNs）的形式存在，其中图像 token 仅由图像 FFN 处理，而文本 token 仅由文本 FFN 处理。与具备模态感知的路由相比，MoEs 在图像-文本描述任务和交错数据上的表现都显著更好，如~\\cref{fig:hard_vs_moe_scaledata} 所示。\n\n<PLACEHOLDER_ENV_14>"
    },
    {
        "section": "4_4",
        "content": "\\subsection{Emergence of expert specialization and sharing}\n\\label{sec:specialization}\nWe investigate multimodal specialization in MoE architectures. In~\\cref{fig:tokens_assignment}, we visualize the normalized number of text and image tokens assigned to each expert across layers.  To quantify this specialization, we compute a specialization score, defined as the average, across all experts within a layer, of $1-H(p)$, where $H$ is the binary entropy of each expert's text/image token distribution. We plot this specialization score in~\\cref{fig:tokens_specialization}.  Higher specialization scores indicate a tendency for experts to focus on either text or image tokens, while lower scores indicate a shared behavior.  These visualizations provide clear evidence of modality-specific experts, particularly in the early layers. Furthermore, the specialization score decreases as the number of layers increases, before rising again in the last layers. This suggests that early and final layers exhibit higher modality specialization compared to mid-layers. This behavior is intuitive, as middle layers are expected to hold higher-level features that may generalize across modalities, \\edit{and consistent with findings in \\citep{shukor2024implicit} that shows increasing alignment between modalities across layers}. The emergence of both expert specialization and cross-modality sharing in our modality-agnostic MoE, suggests it may be a preferable approach compared to modality-aware sparsity. All data displayed here is from an early-fusion MoE model with 1B active parameters trained for 300B tokens.\n\n<PLACEHOLDER_tables/sft_results_begin>\n<PLACEHOLDER_ENV_15><PLACEHOLDER_tables/sft_results_end>\n\n\\vspace{-1cm}",
        "trans_content": "\\subsection{专家专精化与共享的出现}  \n\\label{sec:specialization}  \n我们研究了 MoE 架构中的多模态专精化。在~\\cref{fig:tokens_assignment} 中，我们可视化了在各层中分配给每个专家的文本和图像 token 的归一化数量。为了量化这种专精化，我们计算了一个专精化得分，其定义为某一层中所有专家的平均值，即 $1-H(p)$，其中 $H$ 表示每个专家的文本/图像 token 分布的二元熵。我们在~\\cref{fig:tokens_specialization} 中绘制了该专精化得分。更高的专精化得分表示专家倾向于专注于文本或图像 token，而较低的得分则表示存在共享行为。这些可视化结果清晰地表明了模态特定专家的存在，特别是在前几层中。此外，随着层数的增加，专精化得分先是下降，随后在最后几层再次上升。这表明前层和后层相比中间层具有更高的模态专精化。这一行为是直观的，因为中间层通常包含更高级别的特征，这些特征可能在不同模态之间具有通用性，\\edit{并且这一点与 \\citep{shukor2024implicit} 的研究结果一致，该研究表明随着层数的增加，不同模态之间的对齐程度也在增强}。我们在模态无关的 MoE 中观察到专家专精化和跨模态共享的同时出现，表明该方法相比模态感知稀疏性可能是一种更优的选择。此处展示的所有数据均来自一个使用早期融合的 MoE 模型，该模型拥有 10 亿激活参数，并在 3000 亿个 token 上进行了训练。\n\n<PLACEHOLDER_tables/sft_results_begin>  \n<PLACEHOLDER_ENV_15><PLACEHOLDER_tables/sft_results_end>  \n\n\\vspace{-1cm}"
    },
    {
        "section": "5",
        "content": "\\section{Evaluation on downstream tasks with SFT}\nFollowing previous work on scaling laws, we primarily rely on validation losses. However, we generally find that this evaluation correlates well\nwith performance on downstream tasks. To validate this, we conduct a multimodal\ninstruction tuning stage (SFT) on the LLaVA mixture \\citep{liu2024improvedllava} and report\naccuracy and CIDEr scores across several VQA and captioning tasks.\n\\cref{tab:sft} confirms the ranking of different model configurations.\nSpecifically, early fusion outperforms late fusion, and MoEs outperform dense\nmodels. However, since the models are relatively small (1.5B scale), trained\nfrom scratch, and fine-tuned on a small dataset, the overall scores\nare lower than the current state of the art.  Further implementation\ndetails can be found in~\\Cref{app:implementation_details}.\n\n<PLACEHOLDER_ENV_16>\n<PLACEHOLDER_sec/2_method_end>\n<PLACEHOLDER_sec/2_related_work_begin>",
        "trans_content": "\\section{使用SFT在下游任务上的评估}\n根据先前关于规模法则的研究，我们主要依赖验证损失。然而，我们通常发现这一评估与下游任务的表现有很好的相关性。为了验证这一点，我们在LLaVA混合模型 \\citep{liu2024improvedllava} 上进行多模态指令调优阶段（SFT），并报告在多个VQA和图像描述任务中的准确率和CIDEr分数。\\cref{tab:sft} 确认了不同模型配置的排名。具体而言，早期融合优于晚期融合，MoEs优于密集模型。然而，由于这些模型相对较小（1.5B规模），是从头开始训练的，并且在一个小数据集上进行了微调，因此整体得分低于当前的最先进水平。更多实现细节可以在~\\Cref{app:implementation_details} 中找到。\n\n<PLACEHOLDER_ENV_16>\n<PLACEHOLDER_sec/2_method_end>\n<PLACEHOLDER_sec/2_related_work_begin>"
    },
    {
        "section": "6",
        "content": "\\section{Related work}\n\n\\cpar{Large multimodal models.} A long-standing research goal has been to develop models capable of perceiving the world through multiple modalities, akin to human sensory experience.  Recent progress in vision and language processing has shifted the research focus from smaller, task-specific models toward large, generalist models that can handle diverse inputs \\citep{team2023gemini,hurst2024gpt4o}.  Crucially, pre-trained vision and language backbones often require surprisingly little adaptation to enable effective cross-modal communication \\citep{tsimpoukelli2021multimodalfrozen,shukor2023epalm,vallaeys2024improveddepalm,merullo2023linearly,koh2023grounding}.  Simply integrating a vision encoder with either an encoder-decoder architecture \\citep{shukor2023unival,wang2022ofa,lu2022unified,mizrahi20234m} or a decoder-only LLM has yielded highly capable multimodal systems \\citep{laurenccon2024mattersidefics2,alayrac2022flamingo,liu2024improvedllava,wang2024qwen2,xue2024xgenblip3,chen2024internvl,zhu2024minigpt,abdin2024phi3,dai2024nvlm,beyer2024paligemma,moon2024anymal}. This late-fusion approach, where modalities are processed separately before being combined, is now well-understood, with established best practices for training effective models \\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}.  In contrast, early-fusion models \\citep{fuyu8b,team2024chameleon,diao2024unveiling}, which combine modalities at an earlier stage, remain relatively unexplored, with only a limited number of publicly released models \\citep{fuyu8b,diao2024unveiling}.  Unlike \\citep{diao2024unveiling,team2024chameleon}, our models utilize only a single linear layer and rely exclusively on a next-token prediction loss. Furthermore, we train our models from scratch on all modalities without image tokenization.\n\n\\cpar{Native Multimodal Models.} We define native multimodal models as those trained from scratch on all modalities simultaneously \\citep{team2023gemini} rather than adapting LLMs to accommodate additional modalities. Due to the high cost of training such models, they remain relatively underexplored, with most relying on late-fusion architectures \\citep{kosmoshuang2023language,yu2022coca}. Some multimodal models trained from scratch \\citep{aghajanyan2022cm3,team2024chameleon,wang2024emu3} relax this constraint by utilizing pre-trained image tokenizers such as \\citep{vqgan,vqvae} to convert images into discrete tokens, integrating them into the text vocabulary. This approach enables models to understand and generate text and images, facilitating a more seamless multimodal learning process.\n\n\\cpar{Scaling laws.} Scaling law studies aim to predict how model\nperformance scales with training compute. Early works\n\\citep{kaplan2020scaling,hoffmann2022training} found that LLM performance follows\na power-law relationship with compute, enabling the compute-optimal estimation of the number of model parameters and training tokens at scale for a given budget. Similar research has\nextended these findings to sparse Mixture of Experts (MoE) models, considering\nfactors such as sparsity, number of experts, and routing granularity\n\\citep{krajewski2024scalingmoe,clark2022unifiedscalingmoe,wangscalingmoe}.\nScaling laws have also been observed across various domains, including image\nmodels \\citep{fini2024multimodalaimv2}, video models\n\\citep{rajasegaran2025empirical}, protein LLMs \\citep{scalingprotein}, and\nimitation learning \\citep{pearce2024scaling}. However, few studies have\ninvestigated scaling laws for multimodal models.\nNotably,~\\citet{aghajanyan2023scalingmm} examined multimodal models that tokenize\nmodalities into discrete tokens and include multimodal generation. In contrast,\nwe focus on studying early-fusion models that take raw multimodal inputs and\nare trained on interleaved multimodal data.\n\n\\cpar{Mixture of experts (MoEs).} Mixture of Experts~\\citep{shazeer2017outrageously} enables scaling model capacity by decoupling\nmodel size from per-sample compute. This is done through sparsely\nactivating a small number of parameters. This approach has led to large\nsparse models that rival dense counterparts while being more efficient during\ntraining and inference\n\\citep{fedus2022switch,sun2024hunyuan,jiang2024mixtral,liu2024deepseekv3,wei2024skywork}.\nMany studies have explored improving MoE LLMs across various aspects, such as\nload balancing, routing, stability, scaling, and granularity\n\\citep{lewis2021base,zoph2022st,lepikhin2020gshard}. However, there is limited\nresearch on adopting MoEs for multimodal models, with some work focusing on contrastive\nimage-text models~\\citep{mustafa2022multimodal} and late-fusion multimodal LLMs~\\citep{lin2024moe,li2024aria}. Additionally, some studies investigate\npredefined expert routing, where certain parameters are reserved to process specific modalities~\\cite {bao2021vlmo,chen2024eve,shen2023scaling}. We focus on studying MoEs for native early-fusion models\nrather than proposing new architectures.\n\n<PLACEHOLDER_figs/moe_specialization_begin>\n\n<PLACEHOLDER_ENV_17><PLACEHOLDER_figs/moe_specialization_end><PLACEHOLDER_sec/2_related_work_end>\n<PLACEHOLDER_sec/3_conclusion_begin>",
        "trans_content": "\\section{相关工作}\n\n\\cpar{大型多模态模型。} 一个长期的研究目标是开发能够通过多种模态感知世界的模型，这类似于人类的感官体验。最近，在视觉和语言处理方面的进展使得研究焦点从较小的任务特定模型转向可以处理多种输入的大型通用模型 \\citep{team2023gemini,hurst2024gpt4o}。至关重要的是，经过预训练的视觉和语言骨干通常只需很少的适应，就能有效实现跨模态通信 \\citep{tsimpoukelli2021multimodalfrozen,shukor2023epalm,vallaeys2024improveddepalm,merullo2023linearly,koh2023grounding}。仅仅将视觉编码器与编码-解码架构 \\citep{shukor2023unival,wang2022ofa,lu2022unified,mizrahi20234m} 或解码器-only LLM 结合，就能构建出功能强大的多模态系统 \\citep{laurenccon2024mattersidefics2,alayrac2022flamingo,liu2024improvedllava,wang2024qwen2,xue2024xgenblip3,chen2024internvl,zhu2024minigpt,abdin2024phi3,dai2024nvlm,beyer2024paligemma,moon2024anymal}。这种后融合方法，在模态先分别处理再结合的过程中，现已得到充分理解，并且有了训练有效模型的最佳实践 \\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}。相比之下，早期融合模型 \\citep{fuyu8b,team2024chameleon,diao2024unveiling}，即在更早阶段就将模态结合，仍然相对较少被探索，只有有限数量的公开发布模型 \\citep{fuyu8b,diao2024unveiling}。与 \\citep{diao2024unveiling,team2024chameleon} 不同，我们的模型仅使用一个线性层，并完全依赖于下一个令牌预测损失。此外，我们的模型从零开始训练所有模态，而无需图像标记化。\n\n\\cpar{原生多模态模型。} 我们将原生多模态模型定义为那些同时从零开始在所有模态上进行训练的模型 \\citep{team2023gemini}，而不是将LLM适配到其他模态。由于训练此类模型的成本很高，它们仍然相对未被深入研究，大多数模型依赖于后融合架构 \\citep{kosmoshuang2023language,yu2022coca}。一些从零开始训练的多模态模型 \\citep{aghajanyan2022cm3,team2024chameleon,wang2024emu3} 通过使用如 \\citep{vqgan,vqvae} 这样的预训练图像标记器，将图像转换为离散令牌，并将其集成到文本词汇中，从而放宽了这一约束。这种方法使得模型能够理解并生成文本和图像，促进了更顺畅的多模态学习过程。\n\n\\cpar{扩展定律。} 扩展定律研究旨在预测模型性能如何随着训练计算资源的增加而变化。早期的研究 \\citep{kaplan2020scaling,hoffmann2022training} 发现，LLM性能与计算资源呈幂律关系，这使得能够基于计算资源预算来估算模型参数数量和训练令牌数量的计算最优值。类似的研究将这些发现扩展到了稀疏专家混合（MoE）模型，考虑了稀疏性、专家数量和路由粒度等因素 \\citep{krajewski2024scalingmoe,clark2022unifiedscalingmoe,wangscalingmoe}。扩展定律还在多个领域得到了观察，包括图像模型 \\citep{fini2024multimodalaimv2}、视频模型 \\citep{rajasegaran2025empirical}、蛋白质LLM \\citep{scalingprotein} 和模仿学习 \\citep{pearce2024scaling}。然而，关于多模态模型的扩展定律的研究较少。特别是，\\citet{aghajanyan2023scalingmm} 考察了将模态标记为离散令牌并包括多模态生成的多模态模型。相比之下，我们重点研究的是早期融合模型，它们接受原始多模态输入，并在交错的多模态数据上进行训练。\n\n\\cpar{专家混合（MoEs）。} 专家混合（MoE） \\citep{shazeer2017outrageously} 通过将模型容量与每个样本的计算资源解耦，能够扩展模型容量。这是通过稀疏激活少量参数来实现的。这种方法使得大型稀疏模型能够与密集模型竞争，同时在训练和推理过程中更为高效 \\citep{fedus2022switch,sun2024hunyuan,jiang2024mixtral,liu2024deepseekv3,wei2024skywork}。许多研究探讨了改进MoE LLM的各个方面，如负载平衡、路由、稳定性、扩展性和粒度 \\citep{lewis2021base,zoph2022st,lepikhin2020gshard}。然而，对于将MoE应用于多模态模型的研究较为有限，一些工作集中在对比图像-文本模型 \\citep{mustafa2022multimodal} 和后融合多模态LLM \\citep{lin2024moe,li2024aria} 上。此外，一些研究探讨了预定义的专家路由，其中某些参数专门处理特定的模态 \\citep{bao2021vlmo,chen2024eve,shen2023scaling}。我们专注于研究MoE在原生早期融合模型中的应用，而不是提出新的架构。\n\n<PLACEHOLDER_figs/moe_specialization_begin>\n\n<PLACEHOLDER_ENV_17><PLACEHOLDER_figs/moe_specialization_end><PLACEHOLDER_sec/2_related_work_end>\n<PLACEHOLDER_sec/3_conclusion_begin>"
    },
    {
        "section": "7",
        "content": "\\section{\\edit{Discussion and }Limitations}\n\n\\cpar{\\edit{Scaling laws for multimodal data mixtures.}} Our scaling laws study\nspans different model configurations and training mixtures. While results\nsuggest that the scaling law coefficients remain largely consistent across\nmixtures, a broader exploration of mixture variations is needed to validate this\nobservation and establish a unified scaling law that accounts for this factor.\n\n\\cpar{\\edit{Scaling laws and performance on downstream tasks.}} Similar to\nprevious scaling law studies, our analysis focuses on pretraining performance as\nmeasured by the validation loss. However, the extent to which these findings\ntranslate to downstream performance remains an open question and requires\nfurther investigation.\n\n\\cpar{\\edit{Extrapolation to larger scales.}} The accuracy of scaling law\npredictions improves with increasing FLOPs~\\cref{app:scaling_laws}.\n\\edit{Furthermore, we validate our laws when extrapolating to larger model sizes\n(\\cref{sec:scaling_laws_evaluation}).} However, whether these laws can be reliably\nextrapolated to extremely large model sizes remains an open question.\n\n\\cpar{\\edit{High resolution and early-fusion models.}} Training early-fusion\nmodels with high-resolution inputs leads to a significant increase in vision\ntokens. While pooling techniques have been widely adopted for late-fusion\nmodels, alternative approaches may be necessary for early fusion. \\edit{Given\nthe similarity of early-fusion models to LLMs, it appears that techniques for\nextending context length could be beneficial.}\n\n\\cpar{\\edit{Scaling laws for multimodal MoEs models.}} For MoEs, we consider\nonly a single configuration (top-1 routing with 8 experts). \\edit{We found this\nconfiguration to work reasonably well in our setup, and follow a standard MoEs\nimplementation}. However, the findings may vary when optimizing \\edit{more} the\nMoE architecture or exploring different load-balancing, routing strategies\n\\edit{or different experts implementations}.",
        "trans_content": "\\section{\\edit{讨论与}限制}\n\n\\cpar{\\edit{多模态数据混合的扩展规律。}}我们的扩展规律研究涵盖了不同的模型配置和训练混合方式。虽然结果表明，在不同混合方式下扩展规律的系数在很大程度上保持一致，但仍需要更广泛地探索混合方式的变化，以验证这一观察并建立一个能够考虑该因素的统一扩展规律。\n\n\\cpar{\\edit{扩展规律与下游任务性能的关系。}}与以往的扩展规律研究类似，我们的分析侧重于通过验证损失衡量的预训练性能。然而，这些发现能在多大程度上转化为下游任务的性能仍是一个悬而未决的问题，需要进一步研究。\n\n\\cpar{\\edit{向更大规模的外推。}}随着 FLOPs 的增加，扩展规律预测的准确性也有所提升~\\cref{app:scaling_laws}。\\edit{此外，我们在外推到更大模型规模时验证了这些规律（\\cref{sec:scaling_laws_evaluation}）。}然而，这些规律是否能够可靠地外推到极大模型规模仍是一个未解之题。\n\n\\cpar{\\edit{高分辨率与早期融合模型。}}使用高分辨率输入训练早期融合模型会显著增加视觉 token 的数量。虽然池化技术已被广泛应用于后期融合模型，但早期融合可能需要采用替代方法。\\edit{鉴于早期融合模型与大型语言模型的相似性，延长上下文长度的技术可能会带来益处。}\n\n\\cpar{\\edit{多模态 MoE 模型的扩展规律。}}对于 MoE，我们仅考虑了单一配置（8 个专家的 top-1 路由）。\\edit{我们发现该配置在我们的设置中表现良好，并遵循了标准的 MoE 实现。}然而，当对 MoE 架构进行进一步优化，或探索不同的负载均衡、路由策略\\edit{或不同的专家实现}时，研究结果可能会有所不同。"
    },
    {
        "section": "8",
        "content": "\\section{Conclusion}\nWe explore various strategies for compute-optimal pretraining of native\nmultimodal models. We found the NMMs follow similar scaling laws to those of\nLLMs. Contrary to common belief, we find no inherent advantage in adopting\nlate-fusion architectures over early-fusion ones. While both architectures\nexhibit similar scaling properties, early-fusion models are more efficient to\ntrain and outperform late-fusion models at lower compute budgets. Furthermore,\nwe show that sparse architectures encourage modality-specific specialization,\nleading to performance improvements while maintaining the same inference cost.",
        "trans_content": "\\section{结论}\n我们探讨了本地多模态模型的计算最优预训练的各种策略。我们发现，本地多模态模型（NMMs）遵循与大规模语言模型（LLMs）相似的扩展规律。与普遍看法相反，我们发现采用晚融合架构并没有比早融合架构具有固有优势。虽然这两种架构表现出相似的扩展特性，但早融合模型在训练上更为高效，并且在较低计算预算下优于晚融合模型。此外，我们还展示了稀疏架构鼓励模态特定的专业化，从而在保持相同推理成本的同时提高性能。"
    },
    {
        "section": "9",
        "content": "\\section*{\\edit{Acknowledgment}} We thank Philipp Dufter, Samira Abnar, Xiujun\nLi, Zhe Gan, Alexander Toshev, Yinfei Yang, Dan Busbridge, and Jason Ramapuram\nfor many fruitful discussions. We thank Denise Hui, and Samy Bengio for infra\nand compute support. Finally, we thank, Louis Béthune, Pierre Ablin, Marco\nCuturi, and the MLR team at Apple for their support throughout the project.<PLACEHOLDER_sec/3_conclusion_end>\n{\n    \\small\n    \\bibliographystyle{ieeenat_fullname}\n    \\bibliography{main}\n}\n\\clearpage\n\\appendix\n\\addcontentsline{toc}{section}{Appendix}\n<PLACEHOLDER_NEWCOMMAND_45>\n\\part{}\n\\parttoc\n\\clearpage\n<PLACEHOLDER_sec/X_suppl_begin>",
        "trans_content": "\\section*{\\edit{致谢}} 我们感谢 Philipp Dufter, Samira Abnar, Xiujun Li, Zhe Gan, Alexander Toshev, Yinfei Yang, Dan Busbridge 和 Jason Ramapuram 的许多富有成效的讨论。我们感谢 Denise Hui 和 Samy Bengio 提供的基础设施和计算支持。最后，我们感谢 Louis Béthune, Pierre Ablin, Marco Cuturi 和 Apple MLR 团队在整个项目中的支持。<PLACEHOLDER_sec/3_conclusion_end>\n{\n    \\small\n    \\bibliographystyle{ieeenat_fullname}\n    \\bibliography{main}\n}\n\\clearpage\n\\appendix\n\\addcontentsline{toc}{section}{附录}\n<PLACEHOLDER_NEWCOMMAND_45>\n\\part{}\n\\parttoc\n\\clearpage\n<PLACEHOLDER_sec/X_suppl_begin>"
    },
    {
        "section": "10",
        "content": "\\section{Experimental setup}\n\\label{app:implementation_details}\n\nIn \\Cref{tab:scaling_laws_hparams}, we show the pre-training hyperparameters for different model configurations used to derive the scaling laws. The number of parameters ranges from 275M to 3.7B, with model width increasing accordingly, while the depth remains fixed at 24 layers. Learning rates vary by model size, decreasing as the model scales up. Based on empirical experiments and estimates similar to \\citep{mckinzie2025mm1}, we found these values to be effective in our setup. Training is optimized using a fully decoupled AdamW optimizer with momentum values $\\beta_1=0.9$, $\\beta_2=0.95$, and a weight decay of $1\\text{e}{-4}$. The batch size is set to 2k samples, which account for 2M tokens, given a 1k context length.  Gradient clipping is set to 1.0, with a maximum warmup duration of 5k iterations, adjusted for shorter training runs: 1k and 2.5k warmup steps for models trained between 1k–4k and 5k–15k steps, respectively. For MoEs, we found that a longer warmup is significantly better, so we adopt a 2.5k warmup for all runs under 20k steps. We use a constant learning rate schedule with cooldown during the final 20\\% of training, gradually reducing to zero following an inverse square root schedule. For vision processing, image inputs are divided into $(14,14)$ patches, with augmentations including Random Resized Crop (resizing images to 224px with a scale range of [0.4, 1.0]) and Random Horizontal Flip with a probability of 0.5.  We train our models on mixture of interleaved, image captions and text only data \\Cref{tab:pretraining_datasets}.\nFor late fusion models, we found that using smaller learning rate for the vision encoder significantly boost the performance \\Cref{tab:late_scaler_scratch}, and when both the encoder and decoder are initialized (\\Cref{sec:app_init_early_late}) we found that freezing the vision encoder works best \\Cref{tab:late_scaler_init}.\n\n<PLACEHOLDER_ENV_18>\n\n<PLACEHOLDER_ENV_19>\n\n<PLACEHOLDER_ENV_20>\n\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_end>\n\n<PLACEHOLDER_ENV_22>",
        "trans_content": "\\section{实验设置}\n\\label{app:implementation_details}\n\n在 \\Cref{tab:scaling_laws_hparams} 中，我们展示了用于推导扩展规律的不同模型配置的预训练超参数。参数数量从 275M 到 3.7B 不等，随着模型宽度的增加而增长，而深度固定为 24 层。学习率随模型规模而变化，随着模型的扩大而降低。基于类似 \\citep{mckinzie2025mm1} 的实证实验与估计，我们发现这些数值在我们的设置中是有效的。训练采用完全解耦的 AdamW 优化器进行优化，动量参数为 $\\beta_1=0.9$、$\\beta_2=0.95$，权重衰减为 $1\\text{e}{-4}$。批量大小设置为 2k 个样本，在上下文长度为 1k 的情况下，相当于 2M 个 token。梯度裁剪设置为 1.0，最大预热时长为 5k 步，针对较短训练过程进行调整：对于训练步数在 1k–4k 和 5k–15k 之间的模型，分别使用 1k 和 2.5k 的预热步数。对于 MoE 模型，我们发现较长的预热期显著更优，因此对所有小于 20k 步的训练过程采用 2.5k 的预热。我们使用常数学习率调度策略，在训练后 20\\% 阶段进行冷却，按照反平方根曲线逐渐降至零。在视觉处理方面，图像输入被划分为 $(14,14)$ 的 patch，采用的增强方法包括随机缩放裁剪（将图像缩放至 224px，比例范围为 [0.4, 1.0]）以及以 0.5 的概率进行的随机水平翻转。我们在交错混合的图像描述与纯文本数据上训练模型，见 \\Cref{tab:pretraining_datasets}。\n\n对于后融合模型，我们发现对视觉编码器使用更小的学习率会显著提升性能 \\Cref{tab:late_scaler_scratch}；当编码器与解码器均进行初始化时（见 \\Cref{sec:app_init_early_late}），我们发现冻结视觉编码器效果最佳 \\Cref{tab:late_scaler_init}。\n\n<PLACEHOLDER_ENV_18>\n\n<PLACEHOLDER_ENV_19>\n\n<PLACEHOLDER_ENV_20>\n\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_end>\n\n<PLACEHOLDER_ENV_22>"
    },
    {
        "section": "11+11_1",
        "content": "\\section{Late vs early fusion}\n\\label{app:late_vs_early}\nThis section provides additional comparison between early and late fusion models.\n\n\n\\subsection{Scaling FLOPs} \\Cref{fig:early_vs_late_scaledata_main} compares early-fusion and late-fusion models when scaling FLOPs. Specifically, for each model size, we train multiple models using different amounts of training tokens. The performance gap between the two approaches mainly decreases due to increasing model sizes rather than increasing the number of training tokens. Despite the decreasing gap, across all the models that we train, early-fusion consistently outperform late-fusion.",
        "trans_content": "\\section{晚融合与早融合}\n\\label{app:late_vs_early}\n本节提供了早融合和晚融合模型的额外比较。\n\n\\subsection{FLOPs扩展} \\Cref{fig:early_vs_late_scaledata_main} 比较了在扩展FLOPs时的早融合与晚融合模型。具体来说，对于每个模型大小，我们使用不同数量的训练标记训练多个模型。两种方法之间的性能差距主要是由于增加模型大小而非增加训练标记的数量所导致的。尽管差距在减小，但在我们训练的所有模型中，早融合始终优于晚融合。"
    },
    {
        "section": "11_2",
        "content": "\\subsection{Changing the training data mixture} We analyze how the performance gap between early and late fusion models changes with variations in the training data mixture. As shown in \\Cref{fig:early_vs_late_textratio} and \\Cref{fig:early_vs_late_datatype_sameflops}, when fixing the model size, increasing the ratio of text and interleaved data favors early fusion. Interestingly, the gap remains largely unchanged for other data types. We also observe interference effects between different data types. Specifically, increasing the amount of interleaved data negatively impacts performance on image captions and vice versa. Additionally, increasing the proportion of text-only data slightly improves interleaved performance but increases loss on image captions. Overall, we find that text-only and interleaved data are correlated across different setups.\n\n<PLACEHOLDER_ENV_23>\n\n<PLACEHOLDER_figs/early_vs_late_imageres_begin><PLACEHOLDER_ENV_24><PLACEHOLDER_figs/early_vs_late_imageres_end>",
        "trans_content": "\\subsection{改变训练数据混合} 我们分析了在训练数据混合变化的情况下，早期融合模型和晚期融合模型之间的性能差距如何变化。如\\Cref{fig:early_vs_late_textratio}和\\Cref{fig:early_vs_late_datatype_sameflops}所示，当固定模型大小时，增加文本和交错数据的比例有利于早期融合。有趣的是，对于其他数据类型，性能差距基本保持不变。我们还观察到不同数据类型之间的干扰效应。具体来说，增加交错数据的量会对图像描述的性能产生负面影响，反之亦然。此外，增加仅文本数据的比例会稍微提高交错数据的表现，但会增加图像描述的损失。总体来说，我们发现不同设置下，仅文本数据和交错数据是相关的。\n\n<PLACEHOLDER_ENV_23>\n\n<PLACEHOLDER_figs/early_vs_late_imageres_begin><PLACEHOLDER_ENV_24><PLACEHOLDER_figs/early_vs_late_imageres_end>"
    },
    {
        "section": "11_3",
        "content": "\\subsection{Scaling image resolution is in favor of early-fusion}\n\nWe examine how both\narchitectures perform with varying image resolution. We fix the number of model parameters to 1.63B and 1.75B for early and late fusion respecively. All models are trained for 100K steps or 200B tokens. Since the patch size remains\nconstant, increasing the resolution results in a higher number of visual tokens. For all resolutions, we maintain the same number of text tokens.\nAs shown in \\Cref{fig:early_vs_late_imageres}, the early-fusion model\nconsistently outperforms the late-fusion model across resolutions, particularly\nfor multimodal data, with the performance gap widening at higher resolutions.\nAdditionally, we observe that the loss on text and interleaved data increases as\nresolution increases.\n\n\\vspace{1cm}",
        "trans_content": "\\subsection{图像分辨率的缩放有利于早期融合}\n\n我们研究了在不同图像分辨率下，两种架构的表现。我们将模型参数数目固定为1.63B和1.75B，分别对应于早期融合和后期融合。所有模型都训练了100K步或200B个token。由于patch大小保持不变，增加分辨率会导致更多的视觉token。对于所有分辨率，我们保持相同数量的文本token。如\\Cref{fig:early_vs_late_imageres}所示，早期融合模型在所有分辨率下始终优于后期融合模型，特别是在多模态数据中，性能差距在更高分辨率下扩大。此外，我们观察到，随着分辨率的提高，文本和交织数据的损失也增加。 \n\n\\vspace{1cm}"
    },
    {
        "section": "11_4",
        "content": "\\subsection{Early-fusion is consistently better when matching the late-fusion model size}\n<PLACEHOLDER_figs/early_vs_late_datatype_isoparams_begin>\n<PLACEHOLDER_ENV_25><PLACEHOLDER_figs/early_vs_late_datatype_isoparams_end>\n\nIn this section, we compare the late-fusion model with different configurations\nof early-fusion one. Specifically, we train early-fusion models that match the\nlate-fusion model in total parameters (Params), text model size (Text), and\nFLOPs (FLOPs), assuming 45-45-10 training mixture. As shown in\n\\Cref{fig:early_vs_late_datatype_isoparams}, early fusion consistently\noutperforms late fusion when normalized by total parameters, followed by\nnormalization by FLOPs. When matching the text model size, early fusion performs\nbetter at higher ratios of interleaved data.",
        "trans_content": "\\subsection{在匹配 late-fusion 模型规模时，early-fusion 始终表现更佳}  \n<PLACEHOLDER_figs/early_vs_late_datatype_isoparams_begin>  \n<PLACEHOLDER_ENV_25><PLACEHOLDER_figs/early_vs_late_datatype_isoparams_end>  \n\n在本节中，我们将不同配置的 early-fusion 模型与 late-fusion 模型进行比较。具体而言，我们训练了与 late-fusion 模型在总参数量（Params）、文本模型大小（Text）和计算量（FLOPs）方面相匹配的 early-fusion 模型，假设训练混合比例为 45-45-10。如 \\Cref{fig:early_vs_late_datatype_isoparams} 所示，在按总参数量归一化的情况下，early-fusion 的表现始终优于 late-fusion，其次是在按 FLOPs 归一化的情况下。当匹配文本模型规模时，在较高比例的交错数据下，early-fusion 的表现也更佳。"
    },
    {
        "section": "11_5",
        "content": "\\subsection{Different late-fusion configuration}\nWe examine how this scaling changes with different late-fusion configurations. Instead of scaling both the vision and text models equally, as done in the main paper, we fix the vision encoder size to 300M and scale only the text model. \\Cref{fig:early_vs_late_scalellmdata_dclm} shows that late-fusion models lag behind at smaller model sizes, with the gap closing significantly as the text model scales. This suggests that allocating more parameters to shared components is more beneficial, further supporting the choice of early-fusion models.\n\n<PLACEHOLDER_ENV_26>",
        "trans_content": "\\subsection{不同的晚融合配置}\n我们研究了在不同晚融合配置下，这种缩放是如何变化的。与主文中所做的将视觉和文本模型等比例缩放不同，我们将视觉编码器的大小固定为300M，仅缩放文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，在较小的模型规模下，晚融合模型落后，而随着文本模型的缩放，差距显著缩小。这表明，将更多的参数分配给共享组件更为有利，进一步支持了选择早融合模型的做法。\n\n<PLACEHOLDER_ENV_26>"
    },
    {
        "section": "11_6",
        "content": "\\subsection{Initializing from LLM and CLIP}\n\\label{sec:app_init_early_late}\n\nWe study the case where both late and early fusion models are initialized from pre-trained models, specifically DCLM-1B \\citep{li2024datacomp} and CLIP-ViT-L \\citep{radford2021learning} for late fusion. Interestingly, \\Cref{fig:early_vs_late_init_scaledata} shows that for text and interleaved multimodal documents, early fusion can match the performance of late fusion when trained for longer. However, closing the gap on image caption data remains more challenging. Notably, when considering the overall training cost, including that of pre-trained models, early fusion requires significantly longer training to compensate for the vision encoder’s pretraining cost.\n\n<PLACEHOLDER_figs/early_vs_late_init_scaledata_begin>\n<PLACEHOLDER_ENV_27><PLACEHOLDER_figs/early_vs_late_init_scaledata_end>",
        "trans_content": "\\subsection{从LLM和CLIP初始化}\n\\label{sec:app_init_early_late}\n\n我们研究了一个案例，其中晚期和早期融合模型都从预训练模型初始化，特别是晚期融合使用DCLM-1B \\citep{li2024datacomp}和CLIP-ViT-L \\citep{radford2021learning}。有趣的是，\\Cref{fig:early_vs_late_init_scaledata} 显示，对于文本和交错的多模态文档，经过更长时间训练后，早期融合可以与晚期融合的性能相匹配。然而，在图像描述数据上弥合差距仍然具有更大的挑战性。值得注意的是，考虑到整体训练成本，包括预训练模型的成本，早期融合需要显著更长的训练时间，以补偿视觉编码器的预训练成本。\n\n<PLACEHOLDER_figs/early_vs_late_init_scaledata_begin>\n<PLACEHOLDER_ENV_27><PLACEHOLDER_figs/early_vs_late_init_scaledata_end>"
    },
    {
        "section": "12+12_1",
        "content": "\\section{Scaling laws}\n\\label{app:scaling_laws}\n\n\n\\subsection{Fitting \\(L = F(N, D)\\)}\nFollowing \\citep{hoffmann2022training}, we determine the parameters that minimize the following objective across all our runs \\(i\\):\n<PLACEHOLDER_ENV_28>\nWe perform this optimization across various initialization ranges and select the parameters that achieve the lowest loss across all initializations. Specifically, our grid search spans \\(\\{0, 0.5, 2.5\\}\\) for \\(\\alpha\\) and \\(\\beta\\), \\(\\{0, 5, 10, ..., 30\\}\\) for \\(a\\) and \\(b\\), and \\(\\{-1, -0.5, 1, 0.5\\}\\) for \\(e\\). We use the L-BFGS algorithm with \\(\\delta=1e-3\\).",
        "trans_content": "\\section{尺度定律}\n\\label{app:scaling_laws}\n\n\\subsection{拟合 \\(L = F(N, D)\\)}\n根据 \\citep{hoffmann2022training}，我们确定最小化以下目标的参数，目标是在所有实验运行 \\(i\\) 中最小化：\n<PLACEHOLDER_ENV_28>\n我们在不同的初始化范围内执行此优化，并选择在所有初始化中实现最低损失的参数。具体来说，我们的网格搜索范围为：\\(\\{0, 0.5, 2.5\\}\\) 对于 \\(\\alpha\\) 和 \\(\\beta\\)，\\(\\{0, 5, 10, ..., 30\\}\\) 对于 \\(a\\) 和 \\(b\\)，\\(\\{-1, -0.5, 1, 0.5\\}\\) 对于 \\(e\\)。我们使用 L-BFGS 算法，\\(\\delta=1e-3\\)。"
    },
    {
        "section": "12_2",
        "content": "\\subsection{Fitting \\(N \\propto C^a\\), \\(D \\propto C^b\\) and \\(D \\propto N^d\\)}\nWhile these equations have a closed-form solution \\citep{hoffmann2022training} for early-fusion models that can be derived from \\Cref{eq:scaling_laws}, this is not the case for late-fusion models without specifying either the vision encoder or text model size. To ensure a fair comparison, we derive these equations for both models, by performing linear regression in log space. We found that the regression is very close to the coefficient found with closed-form derivation \\Cref{tab:scaling_laws_closed_form}. For instance, to derive \\(N = K_aC^a\\), given a FLOP budget \\(C\\) and a set of linearly spaced tokens \\(D_i\\) ranging from 10B to 600B, we compute the model size for each \\(D_i\\) as \\(N_i = \\frac{C}{6D}\\) for early fusion and \\(N_i = \\frac{C}{6D}+0.483*N_v\\) for late fusion (for the 45-45-10 mixture, \\(D_v=0.544D\\), thus $C=6D(0.544N_v+N_t)$). We then apply \\Cref{eq:scaling_laws} to obtain the loss for each model size and select \\(N\\) that has the minimum loss. We repeat this for all FLOP values corresponding to our runs, resulting in a set of points \\((C, N_{opt})\\) that we use to regress \\(a\\) and \\(K_a\\).  We follow a similar procedure to find \\(b\\) and \\(d\\). For late-fusion models, we regress a linear model to determine \\(N_v\\) given \\(N\\). Notably, even though we maintain a fixed width ratio for late-fusion models, this approach is more accurate, as embedding layers prevent a strictly fixed ratio between text and vision model sizes. We present the regression results in \\Cref{fig:scaling_laws_closed_form_early_late}.\n\n<PLACEHOLDER_ENV_29>",
        "trans_content": "\\subsection{拟合 \\(N \\propto C^a\\), \\(D \\propto C^b\\) 和 \\(D \\propto N^d\\)}\n虽然这些方程对于早期融合模型有闭式解 \\citep{hoffmann2022training}，该解可以从 \\Cref{eq:scaling_laws} 推导出来，但对于没有指定视觉编码器或文本模型大小的晚期融合模型并非如此。为了确保公平比较，我们通过在对数空间中执行线性回归，推导这两种模型的方程。我们发现回归结果与闭式推导中找到的系数非常接近 \\Cref{tab:scaling_laws_closed_form}。例如，为了推导 \\(N = K_aC^a\\)，给定一个FLOP预算 \\(C\\) 和一组线性间隔的标记 \\(D_i\\)，范围从10B到600B，我们为每个 \\(D_i\\) 计算模型大小，其中早期融合为 \\(N_i = \\frac{C}{6D}\\)，而晚期融合为 \\(N_i = \\frac{C}{6D}+0.483*N_v\\)（对于45-45-10混合模型，\\(D_v=0.544D\\)，因此 \\(C=6D(0.544N_v+N_t)\\)）。然后，我们应用 \\Cref{eq:scaling_laws} 来获得每个模型大小的损失，并选择具有最小损失的 \\(N\\)。我们对所有与我们的运行相对应的FLOP值重复这一过程，得到一组点 \\((C, N_{opt})\\)，我们用这些点回归 \\(a\\) 和 \\(K_a\\)。我们遵循类似的过程来找到 \\(b\\) 和 \\(d\\)。对于晚期融合模型，我们回归一个线性模型来确定给定 \\(N\\) 的 \\(N_v\\)。值得注意的是，尽管我们为晚期融合模型保持固定的宽度比，但这种方法更为准确，因为嵌入层阻止了文本和视觉模型大小之间的严格固定比例。我们在 \\Cref{fig:scaling_laws_closed_form_early_late} 中展示了回归结果。\n\n<PLACEHOLDER_ENV_29>"
    },
    {
        "section": "12_3",
        "content": "\\subsection{Fitting \\(L \\propto C^c\\)}\nTo determine the relationship between the final model loss and the compute budget \\(C\\), we begin by interpolating the points corresponding to the same model size and compute the convex hull that covers the minimum loss achieved by all runs for each FLOP. This results in a continuous mapping from the FLOPs to the lowest loss. We consider a range of FLOPs, excluding very small values ($\\leq 3e^{19}$), and construct a dataset of \\((C, L)\\) for linearly spaced compute \\(C\\). Using this data, we find the linear relationship between \\(L\\) and \\(C\\) in the log space and deduce the exponent \\(c\\). We visualize the results in \\Cref{fig:scaling_laws_early_late_moe}.\n\n<PLACEHOLDER_ENV_30>\n\n<PLACEHOLDER_ENV_31>",
        "trans_content": "\\subsection{拟合 \\(L \\propto C^c\\)}\n为了确定最终模型损失与计算预算 \\(C\\) 之间的关系，我们首先对对应于相同模型大小的点进行插值，并计算覆盖所有运行中每个 FLOP 最小损失的凸包。这将导致从 FLOP 到最低损失的连续映射。我们考虑一个 FLOP 范围，排除非常小的值（\\(\\leq 3e^{19}\\)），并为线性间隔的计算 \\(C\\) 构建一个 \\((C, L)\\) 数据集。利用这些数据，我们在对数空间中找到 \\(L\\) 和 \\(C\\) 之间的线性关系，并推导出指数 \\(c\\)。我们在 \\Cref{fig:scaling_laws_early_late_moe} 中可视化了结果。\n\n<PLACEHOLDER_ENV_30>\n\n<PLACEHOLDER_ENV_31>"
    },
    {
        "section": "12_4",
        "content": "\\subsection{Scaling laws for different target data type}\nIn \\Cref{fig:scaling_laws_early_late_moe_getty_obelics_dclm}, we derive the scaling laws for different target data types. In general, we observe that the model learns image captioning faster than interleaved data, as indicated by the higher absolute value of the scaling exponent (e.g., 0.062 vs 0.046), despite using the same data ratio for captioning and interleaved data (45\\% each). Additionally, we find that the model learns more slowly on text-only data, likely due to the smaller amount of text-only data (10\\%). Across model configurations, we find that early fusion scales similarly to late fusion on image captioning but has a lower multiplicative constant (49.99 vs 47.97). For MoEs, the model learns faster but exhibits a higher multiplicative constant. On text and interleaved data, early and late fusion models scale similarly and achieve comparable performance. However, MoEs demonstrate better overall performance while learning slightly more slowly.",
        "trans_content": "\\subsection{不同目标数据类型的缩放规律}\n在 \\Cref{fig:scaling_laws_early_late_moe_getty_obelics_dclm} 中，我们推导了不同目标数据类型的缩放规律。总体而言，我们观察到模型在图像标注任务上学习得比交织数据更快，如通过更高的缩放指数绝对值所示（例如，0.062 对比 0.046），尽管图像标注和交织数据使用相同的数据比例（各占 45\\%）。此外，我们发现模型在仅文本数据上学习较慢，这可能是由于仅文本数据的数量较少（10\\%）。在不同的模型配置中，我们发现早期融合在图像标注任务上的表现与晚期融合相似，但乘法常数较低（49.99 对比 47.97）。对于 MoE 模型，模型学习较快，但表现出更高的乘法常数。在文本和交织数据上，早期和晚期融合模型的缩放规律相似，且表现相当。然而，MoE 模型表现出更好的整体性能，尽管学习速度稍慢。"
    },
    {
        "section": "12_5",
        "content": "\\subsection{Scaling laws for different training mixtures}\n\nWe investigate how the scaling laws change when modifying the training mixtures. Specifically, we vary the ratio of image caption, interleaved, and text-only data and report the results in \\Cref{fig:app_early_scaleflops_data_mixtures}. Overall, we observe similar scaling trends, with only minor changes in the scaling coefficients. Upon closer analysis, we find that increasing the ratio of a particular data type in the training mixture, leads to a corresponding increase in its scaling exponent. For instance, increasing the ratio of image captions from 30\\% to 40\\% raises the absolute value of the exponent from 0.056 to 0.061. However, for text-only data, we do not observe significant changes in the scaling coefficients when varying its proportion in the training mixture.\n\n<PLACEHOLDER_ENV_32>",
        "trans_content": "\\subsection{不同训练混合数据的尺度律}\n\n我们研究了在修改训练混合数据时尺度律的变化。具体来说，我们改变了图像描述、交错数据和仅文本数据的比例，并在\\Cref{fig:app_early_scaleflops_data_mixtures}中报告了结果。总体而言，我们观察到相似的尺度变化趋势，只有在尺度系数上有轻微的变化。经过更详细的分析，我们发现，当某种数据类型在训练混合数据中的比例增加时，其对应的尺度指数也会随之增加。例如，将图像描述的比例从30\\%增加到40\\%时，指数的绝对值从0.056提高到0.061。然而，对于仅文本数据，当其在训练混合数据中的比例变化时，我们并没有观察到尺度系数的显著变化。\n\n<PLACEHOLDER_ENV_32>"
    },
    {
        "section": "12_6",
        "content": "\\subsection{Scaling laws evaluation and sensitivity}\n\nFor each model size and number of training tokens, we compute the loss based on the estimated functional form in \\Cref{eq:scaling_laws} and compare it with the actual loss achieved by our runs. We visualize these points in \\Cref{fig:observed_vs_predicted_loss}, demonstrating that our estimation is highly accurate, particularly for lower loss values, and hence for larger FLOPs. Additionally, we perform a sensitivity analysis using bootstrapping. Specifically, we sample with replacement \\( P \\) points (\\( P \\) being equal to the total number of trained models) and re-estimate the scaling law coefficients. This process is repeated 100 times, and we report the average and standard deviation of each coefficient. \\Cref{tab:scaling_laws_sensitivity} shows that our estimation is more precise for \\(\\beta\\) compared to \\(\\alpha\\), primarily due to the smaller number of model sizes relative to the number of different token counts used to derive the scaling laws.\n\n<PLACEHOLDER_ENV_33>",
        "trans_content": "\\subsection{缩放规律评估与敏感性分析}\n\n对于每个模型大小和训练令牌数量，我们基于\\Cref{eq:scaling_laws}中估计的函数形式计算损失，并将其与我们实验中实际获得的损失进行比较。我们在\\Cref{fig:observed_vs_predicted_loss}中展示了这些数据点，证明我们的估计非常准确，尤其是在较低损失值时，因此对于较大的FLOPs也同样适用。此外，我们通过自助法进行敏感性分析。具体来说，我们以替代抽样的方式选取\\( P \\)个点（\\( P \\)等于训练模型的总数），并重新估计缩放规律的系数。这个过程重复进行100次，我们报告每个系数的平均值和标准差。\\Cref{tab:scaling_laws_sensitivity}显示，相较于\\(\\alpha\\)，我们的估计对于\\(\\beta\\)更为精确，主要是因为模型大小的数量相对于用于推导缩放规律的不同令牌数量较少。 \n\n<PLACEHOLDER_ENV_33>"
    },
    {
        "section": "12_7",
        "content": "\\subsection{\\edit{Scaling laws for sparse NMMs.}}\n\\label{app:scaling_laws_moes}\n\nSimilar to dense models, we fit a parametric loss function (\\Cref{eq:scaling_laws}) to predict the loss of sparse NMMs based on the number of parameters and training tokens, replacing the total parameter count with the number of active parameters. While incorporating sparsity is standard when deriving scaling laws for MoEs \\citep{wangscalingmoe,krajewski2024scalingmoe,abnar2025parameters}, we focus on deriving scaling laws specific to the sparsity level used in our MoE setup. This yields coefficients that are implicitly conditioned on the sparsity configuration.\n\nWe also experiment with a sparsity-aware formulation of the scaling law as proposed in \\citep{abnar2025parameters}, and observe consistent trends (\\Cref{tab:moes_coeffs}). In particular, the exponents associated with model size ($N$) are substantially larger than those for training tokens ($\\beta$), reinforcing the importance of scaling model size in sparse architectures. Additionally, we observe that the terms governing the scaling of active parameters decompose into two components.\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_moes_begin><PLACEHOLDER_ENV_34><PLACEHOLDER_tables/scaling_laws_coeffs_moes_end>",
        "trans_content": "\\subsection{\\edit{稀疏NMM的缩放规律。}}\n\\label{app:scaling_laws_moes}\n\n与密集模型类似，我们拟合一个参数化损失函数（\\Cref{eq:scaling_laws}），根据参数数量和训练标记预测稀疏NMM的损失，替换总参数数目为活动参数的数量。虽然在推导MoE的缩放规律时，加入稀疏性是标准做法\\citep{wangscalingmoe,krajewski2024scalingmoe,abnar2025parameters}，但我们着重于推导与我们MoE设置中使用的稀疏性水平相关的缩放规律。这会得到隐式依赖于稀疏配置的系数。\n\n我们还尝试了\\citep{abnar2025parameters}提出的稀疏感知缩放规律公式，并观察到一致的趋势（\\Cref{tab:moes_coeffs}）。特别地，与模型大小（$N$）相关的指数显著大于与训练标记（$\\beta$）相关的指数，进一步强调了在稀疏架构中扩大模型大小的重要性。此外，我们还观察到，支配活动参数缩放的项分解为两个组成部分。\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_moes_begin><PLACEHOLDER_ENV_34><PLACEHOLDER_tables/scaling_laws_coeffs_moes_end>"
    },
    {
        "section": "13",
        "content": "\\section{Mixture of experts and modality-specific specialization}\n\\label{app:moes}\n\nWe investigate multimodal specialization in MoE architectures. We compute a\nspecialization score as the average difference between the number of text/images\ntokens assigned to each expert and a uniform assignment ($1/E$). Additionally,\nwe visualize the normalized number of text and image tokens assigned to each\nexpert across layers. \\Cref{fig:app_moes_specialization} shows clear modality-specific\nexperts, particularly in the early layers. Furthermore, the specialization score\ndecreases as the number of layers increases but rises again in the very last\nlayers. This suggests that early and final layers require more modality\nspecialization compared to mid-layers. Additionally, we observe several experts\nshared between text and image modalities, a phenomenon not present in\nhard-routed or predefined modality-specific experts.\n\n<PLACEHOLDER_ENV_35>\n\n<PLACEHOLDER_ENV_36>\n\n<PLACEHOLDER_ENV_37><PLACEHOLDER_sec/X_suppl_end>\n\n\\end{document}",
        "trans_content": "\\section{专家混合与特定模态的专业化}\n\\label{app:moes}\n\n我们研究了MoE架构中的多模态专业化。我们计算了一个专业化得分，即每个专家分配到的文本/图像标记与均匀分配（$1/E$）之间的平均差异。此外，我们还可视化了跨层次分配给每个专家的归一化文本和图像标记数量。 \\Cref{fig:app_moes_specialization} 显示了明显的模态特定专家，特别是在早期层次中。此外，随着层数的增加，专业化得分有所下降，但在最后几层再次上升。这表明，早期层和最后层比中间层更需要模态专业化。此外，我们还观察到多个专家在文本和图像模态之间共享，这种现象在硬路由或预定义的模态特定专家中并不存在。\n\n<PLACEHOLDER_ENV_35>\n\n<PLACEHOLDER_ENV_36>\n\n<PLACEHOLDER_ENV_37><PLACEHOLDER_sec/X_suppl_end>\n\n\\end{document}"
    }
]