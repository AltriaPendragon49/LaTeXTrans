[
    {
        "section": "-1",
        "content": "\n\\documentclass[10pt,letterpaper]{article}\n\n\\usepackage[preprint]{arxiv}\n\n\\widowpenalty10000\n\\clubpenalty10000\n\\setlength{\\textfloatsep}{1pt}\n\\setlength{\\abovecaptionskip}{2pt}\n\\setlength{\\belowcaptionskip}{2pt}\n\n\\setlength{\\abovedisplayskip}{1pt}\n\\setlength{\\belowdisplayskip}{1pt}\n\\setlength{\\abovedisplayshortskip}{1pt}\n\\setlength{\\belowdisplayshortskip}{1pt}\n\n\\setcitestyle{square, comma, numbers,sort&compress, super}\n\\usepackage{enumitem}\n\\usepackage{algpseudocode}\n\\usepackage[font=small,labelfont=bf]{caption}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{booktabs}\n\\usepackage{algorithm}\n\\usepackage{subcaption}\n\\usepackage[normalem]{ulem}\n\\usepackage{xparse}\n\\usepackage{pifont}\n\\usepackage{bm}\n\\usepackage{threeparttable}\n\\usepackage{etoolbox}\n\n\\usepackage{listings}\n\\usepackage{mwe}\n\\usepackage{makecell}\n\\usepackage{color, colortbl}\n\\usepackage{tabularx}\n\\usepackage{pifont}\n\\usepackage[accsupp]{axessibility}\n\\usepackage[symbol]{footmisc}\n\\usepackage{pgfplots}\n\\usetikzlibrary{spy}\n\\usepackage[scaled=0.85]{DejaVuSansMono}\n\\usepackage{minitoc}\n\n<PLACEHOLDER_preamble_begin><PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n\n<PLACEHOLDER_NEWCOMMAND_2>\n<PLACEHOLDER_NEWCOMMAND_3>\n\n<PLACEHOLDER_NEWCOMMAND_4>\n\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n\n<PLACEHOLDER_NEWCOMMAND_7>\n<PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n\n<PLACEHOLDER_NEWCOMMAND_11>\n\n\\newlength\\savewidth<PLACEHOLDER_NEWCOMMAND_12>\n\n\\newlength\\thinwidth<PLACEHOLDER_NEWCOMMAND_13>\n\n\\definecolor{Gray}{gray}{0.92}\n\\definecolor{DarkGray}{gray}{0.5}\n\\definecolor{LightLateColor}{rgb}{0.88,1,1}\n\\definecolor{altRowColor}{gray}{0.92}\n\\definecolor{highlightRowColor}{rgb}{0.9, 0.9, 1}\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n\n\\definecolor{GrayNumber}{gray}{0.5}\n\\definecolor{GrayXMark}{gray}{0.7}\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n\n<PLACEHOLDER_NEWCOMMAND_28>\n\n\\expandafter\\def\\expandafter\\normalsize\\expandafter{\n    \\normalsize\n    \\setlength\\abovedisplayskip{2pt}\n    \\setlength\\belowdisplayskip{8pt}\n    \\setlength\\abovedisplayshortskip{-5pt}\n    \\setlength\\belowdisplayshortskip{5pt}\n}\n\n\\captionsetup[table]{skip=7pt}\n\\captionsetup[table]{belowskip=15pt}\n<PLACEHOLDER_NEWCOMMAND_29>\n\n<PLACEHOLDER_NEWCOMMAND_30>\n\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.3}\n\\usepgfplotslibrary{external}\n\\usepgfplotslibrary{groupplots}\n\\usepgfplotslibrary{statistics}\n\\usetikzlibrary{spy}\n\n\\definecolor{LateGradStart}{HTML}{185D79}\n\\definecolor{LateGradEnd}{HTML}{7AC5E9}\n\n\\definecolor{EarlyGradStart}{HTML}{fd4b2f}\n\\definecolor{EarlyGradEnd}{HTML}{f19e18}\n\n\\definecolor{LateColor}{HTML}{1F78B4}\n\\definecolor{EarlyColor}{HTML}{D32F2F}\n\n\\definecolor{MOEGradStart}{HTML}{0a3431}\n\\definecolor{MOEGradEnd}{HTML}{43c197}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\\definecolor{CustomB}{HTML}{d68910}\n\\definecolor{CustomC}{HTML}{c75f25}\n\\definecolor{CustomD}{HTML}{2980b9}\n\n\\definecolor{DarkOrange}{HTML}{c75f25}\n\\definecolor{Brown}{HTML}{d68910}\n\\definecolor{Gold}{HTML}{b89d53}\n\\definecolor{Blue}{HTML}{2980b9}\n\n\\definecolor{Blue}{HTML}{0072B2}\n\\definecolor{Orange}{HTML}{D55E00}\n\n\\definecolor{Purple}{HTML}{6A3D9A}\n\\definecolor{Gold}{HTML}{E6AB02}\n\n\\definecolor{Teal}{HTML}{117A65}\n\\definecolor{Coral}{HTML}{E67E22}\n\n\\definecolor{NavyBlue}{HTML}{253494}\n\\definecolor{SoftYellow}{HTML}{FDD835}\n\n\\definecolor{DarkGreen}{HTML}{1B5E20}\n\n\\definecolor{SteelGray}{HTML}{37474F}\n\\definecolor{AquaBlue}{HTML}{26C6DA}\n\n\\definecolor{Burgundy}{HTML}{7B1FA2}\n\\definecolor{SkyBlue}{HTML}{4FC3F7}\n\n\\definecolor{OliveGreen}{HTML}{558B2F}\n\\definecolor{Tangerine}{HTML}{FF9800}\n\n\\definecolor{MidnightBlue}{HTML}{1A237E}\n\\definecolor{Peach}{HTML}{FFAB91}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Light3}{HTML}{f5d15d}\n\\definecolor{CustomA_Light2}{HTML}{f9e282}\n\\definecolor{CustomA_Light1}{HTML}{fce94f}\n\n\\definecolor{CustomA_Base}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Dark1}{HTML}{e1b10f}\n\\definecolor{CustomA_Dark2}{HTML}{d09e0f}\n\\definecolor{CustomA_Dark3}{HTML}{b88a0e}\n\n\\definecolor{CustomG_Light3}{HTML}{77b55c}\n\\definecolor{CustomG_Light2}{HTML}{7dbf65}\n\\definecolor{CustomG_Light1}{HTML}{85d373}\n\n\\definecolor{CustomG_Base}{HTML}{218838}\n\n\\definecolor{CustomG_Dark1}{HTML}{1b6b2e}\n\\definecolor{CustomG_Dark2}{HTML}{17562a}\n\\definecolor{CustomG_Dark3}{HTML}{124627}\n\n\\definecolor{CustomC_Light3}{HTML}{c97f4d}\n\\definecolor{CustomC_Light2}{HTML}{d98e60}\n\\definecolor{CustomC_Light1}{HTML}{e89e73}\n\n\\definecolor{CustomC_Base}{HTML}{b75b20}\n\n\\definecolor{CustomC_Dark1}{HTML}{9c4a1d}\n\\definecolor{CustomC_Dark2}{HTML}{8b4219}\n\\definecolor{CustomC_Dark3}{HTML}{7b3a15}\n\n\\definecolor{CustomD_Light3}{HTML}{46a8d4}\n\\definecolor{CustomD_Light2}{HTML}{63b6de}\n\\definecolor{CustomD_Light1}{HTML}{7ac5e9}\n\n\\definecolor{CustomD_Base}{HTML}{2980b9}\n\n\\definecolor{CustomD_Dark1}{HTML}{1f6a8a}\n\\definecolor{CustomD_Dark2}{HTML}{175d79}\n\\definecolor{CustomD_Dark3}{HTML}{134f66}\n\n\\definecolor{CustomP_Light3}{HTML}{b077d4}\n\\definecolor{CustomP_Light2}{HTML}{c08ae0}\n\\definecolor{CustomP_Light1}{HTML}{d1a0eb}\n\n\\definecolor{CustomP_Base}{HTML}{8e44ad}\n\n\\definecolor{CustomP_Dark1}{HTML}{732d91}\n\\definecolor{CustomP_Dark2}{HTML}{5e2377}\n\\definecolor{CustomP_Dark3}{HTML}{4b1b5f}\n\n\\pgfplotsset{\n    legend early_same_llm style/.style={\n        solid,\n        mark=*,\n        CustomA_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_same_params style/.style={\n        solid,\n        mark=*,\n        CustomB,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=diamond,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend early_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomC_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_8b style/.style={\n        solid,\n        mark=star,\n        black,\n        mark options={solid},\n        line width=1pt,\n        mark size=2pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomC_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomC_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomC_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style_dashed/.style={\n        dashed,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill style/.style={\n        solid,\n        mark=triangle,\n        yellow,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill_distdown style/.style={\n        solid,\n        mark=triangle,\n        red,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init_aimv2 style/.style={\n        solid,\n        mark=triangle,\n        brown,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early bars style/.style={\n        ybar,\n        bar width=0.4cm,\n        solid,\n        mark=none,\n        fill=CustomC_Base,\n        line width=0pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late style/.style={\n        solid,\n        mark=diamond,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomD_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomD_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomD_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomD_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomG_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_shape style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b_shape style/.style={\n        solid,\n        mark=pentagon,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b_shape style/.style={\n        solid,\n        mark=square,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b_shape style/.style={\n        solid,\n        mark=o,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomG_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend hard style/.style={\n        solid,\n        mark=*,\n        CustomP_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_2_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_vision style/.style={\n        dotted,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}<PLACEHOLDER_preamble_end>\n\n\\definecolor{citecolor}{HTML}{0071bc}\n\n\\crefname{section}{\\S}{\\S\\S}\n\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\n    Mustafa Shukor\\thanks{Work done during an internship at Apple.} \\\\\n    Sorbonne University\n    \\And\n    Enrico Fini \\\\\n    Apple\n    \\And\n    Victor Guilherme Turrisi da Costa \\\\\n    Apple\n    \\And\n    Matthieu Cord \\\\\n    Sorbonne University\n    \\And\n    Joshua Susskind \\\\\n    Apple\n    \\And\n    Alaaeldin El-Nouby \\\\\n    Apple\n}\n\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>",
        "trans_content": "\n\\documentclass[10pt,letterpaper]{article}\n\n\\usepackage[preprint]{arxiv}\n\n\\widowpenalty10000\n\\clubpenalty10000\n\\setlength{\\textfloatsep}{1pt}\n\\setlength{\\abovecaptionskip}{2pt}\n\\setlength{\\belowcaptionskip}{2pt}\n\n\\setlength{\\abovedisplayskip}{1pt}\n\\setlength{\\belowdisplayskip}{1pt}\n\\setlength{\\abovedisplayshortskip}{1pt}\n\\setlength{\\belowdisplayshortskip}{1pt}\n\n\\setcitestyle{square, comma, numbers,sort&compress, super}\n\\usepackage{enumitem}\n\\usepackage{algpseudocode}\n\\usepackage[font=small,labelfont=bf]{caption}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{booktabs}\n\\usepackage{algorithm}\n\\usepackage{subcaption}\n\\usepackage[normalem]{ulem}\n\\usepackage{xparse}\n\\usepackage{pifont}\n\\usepackage{bm}\n\\usepackage{threeparttable}\n\\usepackage{etoolbox}\n\n\\usepackage{listings}\n\\usepackage{mwe}\n\\usepackage{makecell}\n\\usepackage{color, colortbl}\n\\usepackage{tabularx}\n\\usepackage{pifont}\n\\usepackage[accsupp]{axessibility}\n\\usepackage[symbol]{footmisc}\n\\usepackage{pgfplots}\n\\usetikzlibrary{spy}\n\\usepackage[scaled=0.85]{DejaVuSansMono}\n\\usepackage{minitoc}\n\n<PLACEHOLDER_preamble_begin><PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n\n<PLACEHOLDER_NEWCOMMAND_2>\n<PLACEHOLDER_NEWCOMMAND_3>\n\n<PLACEHOLDER_NEWCOMMAND_4>\n\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n\n<PLACEHOLDER_NEWCOMMAND_7>\n<PLACEHOLDER_NEWCOMMAND_8>\n<PLACEHOLDER_NEWCOMMAND_9>\n<PLACEHOLDER_NEWCOMMAND_10>\n\n<PLACEHOLDER_NEWCOMMAND_11>\n\n\\newlength\\savewidth<PLACEHOLDER_NEWCOMMAND_12>\n\n\\newlength\\thinwidth<PLACEHOLDER_NEWCOMMAND_13>\n\n\\definecolor{Gray}{gray}{0.92}\n\\definecolor{DarkGray}{gray}{0.5}\n\\definecolor{LightLateColor}{rgb}{0.88,1,1}\n\\definecolor{altRowColor}{gray}{0.92}\n\\definecolor{highlightRowColor}{rgb}{0.9, 0.9, 1}\n<PLACEHOLDER_NEWCOMMAND_14>\n<PLACEHOLDER_NEWCOMMAND_15>\n<PLACEHOLDER_NEWCOMMAND_16>\n<PLACEHOLDER_NEWCOMMAND_17>\n<PLACEHOLDER_NEWCOMMAND_18>\n<PLACEHOLDER_NEWCOMMAND_19>\n<PLACEHOLDER_NEWCOMMAND_20>\n<PLACEHOLDER_NEWCOMMAND_21>\n\n\\definecolor{GrayNumber}{gray}{0.5}\n\\definecolor{GrayXMark}{gray}{0.7}\n<PLACEHOLDER_NEWCOMMAND_22>\n<PLACEHOLDER_NEWCOMMAND_23>\n\n<PLACEHOLDER_NEWCOMMAND_24>\n<PLACEHOLDER_NEWCOMMAND_25>\n<PLACEHOLDER_NEWCOMMAND_26>\n<PLACEHOLDER_NEWCOMMAND_27>\n\n<PLACEHOLDER_NEWCOMMAND_28>\n\n\\expandafter\\def\\expandafter\\normalsize\\expandafter{\n    \\normalsize\n    \\setlength\\abovedisplayskip{2pt}\n    \\setlength\\belowdisplayskip{8pt}\n    \\setlength\\abovedisplayshortskip{-5pt}\n    \\setlength\\belowdisplayshortskip{5pt}\n}\n\n\\captionsetup[table]{skip=7pt}\n\\captionsetup[table]{belowskip=15pt}\n<PLACEHOLDER_NEWCOMMAND_29>\n\n<PLACEHOLDER_NEWCOMMAND_30>\n\n\\usepackage{pgfplots}\n\\pgfplotsset{compat=1.3}\n\\usepgfplotslibrary{external}\n\\usepgfplotslibrary{groupplots}\n\\usepgfplotslibrary{statistics}\n\\usetikzlibrary{spy}\n\n\\definecolor{LateGradStart}{HTML}{185D79}\n\\definecolor{LateGradEnd}{HTML}{7AC5E9}\n\n\\definecolor{EarlyGradStart}{HTML}{fd4b2f}\n\\definecolor{EarlyGradEnd}{HTML}{f19e18}\n\n\\definecolor{LateColor}{HTML}{1F78B4}\n\\definecolor{EarlyColor}{HTML}{D32F2F}\n\n\\definecolor{MOEGradStart}{HTML}{0a3431}\n\\definecolor{MOEGradEnd}{HTML}{43c197}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\\definecolor{CustomB}{HTML}{d68910}\n\\definecolor{CustomC}{HTML}{c75f25}\n\\definecolor{CustomD}{HTML}{2980b9}\n\n\\definecolor{DarkOrange}{HTML}{c75f25}\n\\definecolor{Brown}{HTML}{d68910}\n\\definecolor{Gold}{HTML}{b89d53}\n\\definecolor{Blue}{HTML}{2980b9}\n\n\\definecolor{Blue}{HTML}{0072B2}\n\\definecolor{Orange}{HTML}{D55E00}\n\n\\definecolor{Purple}{HTML}{6A3D9A}\n\\definecolor{Gold}{HTML}{E6AB02}\n\n\\definecolor{Teal}{HTML}{117A65}\n\\definecolor{Coral}{HTML}{E67E22}\n\n\\definecolor{NavyBlue}{HTML}{253494}\n\\definecolor{SoftYellow}{HTML}{FDD835}\n\n\\definecolor{DarkGreen}{HTML}{1B5E20}\n\n\\definecolor{SteelGray}{HTML}{37474F}\n\\definecolor{AquaBlue}{HTML}{26C6DA}\n\n\\definecolor{Burgundy}{HTML}{7B1FA2}\n\\definecolor{SkyBlue}{HTML}{4FC3F7}\n\n\\definecolor{OliveGreen}{HTML}{558B2F}\n\\definecolor{Tangerine}{HTML}{FF9800}\n\n\\definecolor{MidnightBlue}{HTML}{1A237E}\n\\definecolor{Peach}{HTML}{FFAB91}\n\n\\definecolor{CustomA}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Light3}{HTML}{f5d15d}\n\\definecolor{CustomA_Light2}{HTML}{f9e282}\n\\definecolor{CustomA_Light1}{HTML}{fce94f}\n\n\\definecolor{CustomA_Base}{HTML}{f1c40f}\n\n\\definecolor{CustomA_Dark1}{HTML}{e1b10f}\n\\definecolor{CustomA_Dark2}{HTML}{d09e0f}\n\\definecolor{CustomA_Dark3}{HTML}{b88a0e}\n\n\\definecolor{CustomG_Light3}{HTML}{77b55c}\n\\definecolor{CustomG_Light2}{HTML}{7dbf65}\n\\definecolor{CustomG_Light1}{HTML}{85d373}\n\n\\definecolor{CustomG_Base}{HTML}{218838}\n\n\\definecolor{CustomG_Dark1}{HTML}{1b6b2e}\n\\definecolor{CustomG_Dark2}{HTML}{17562a}\n\\definecolor{CustomG_Dark3}{HTML}{124627}\n\n\\definecolor{CustomC_Light3}{HTML}{c97f4d}\n\\definecolor{CustomC_Light2}{HTML}{d98e60}\n\\definecolor{CustomC_Light1}{HTML}{e89e73}\n\n\\definecolor{CustomC_Base}{HTML}{b75b20}\n\n\\definecolor{CustomC_Dark1}{HTML}{9c4a1d}\n\\definecolor{CustomC_Dark2}{HTML}{8b4219}\n\\definecolor{CustomC_Dark3}{HTML}{7b3a15}\n\n\\definecolor{CustomD_Light3}{HTML}{46a8d4}\n\\definecolor{CustomD_Light2}{HTML}{63b6de}\n\\definecolor{CustomD_Light1}{HTML}{7ac5e9}\n\n\\definecolor{CustomD_Base}{HTML}{2980b9}\n\n\\definecolor{CustomD_Dark1}{HTML}{1f6a8a}\n\\definecolor{CustomD_Dark2}{HTML}{175d79}\n\\definecolor{CustomD_Dark3}{HTML}{134f66}\n\n\\definecolor{CustomP_Light3}{HTML}{b077d4}\n\\definecolor{CustomP_Light2}{HTML}{c08ae0}\n\\definecolor{CustomP_Light1}{HTML}{d1a0eb}\n\n\\definecolor{CustomP_Base}{HTML}{8e44ad}\n\n\\definecolor{CustomP_Dark1}{HTML}{732d91}\n\\definecolor{CustomP_Dark2}{HTML}{5e2377}\n\\definecolor{CustomP_Dark3}{HTML}{4b1b5f}\n\n\\pgfplotsset{\n    legend early_same_llm style/.style={\n        solid,\n        mark=*,\n        CustomA_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_same_params style/.style={\n        solid,\n        mark=*,\n        CustomB,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early style/.style={\n        solid,\n        mark=diamond,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomC_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend early_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomC_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_8b style/.style={\n        solid,\n        mark=star,\n        black,\n        mark options={solid},\n        line width=1pt,\n        mark size=2pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomC_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomC_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomC_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style/.style={\n        solid,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init style_dashed/.style={\n        dashed,\n        mark=triangle,\n        CustomC_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill style/.style={\n        solid,\n        mark=triangle,\n        yellow,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early_init_distill_distdown style/.style={\n        solid,\n        mark=triangle,\n        red,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init_aimv2 style/.style={\n        solid,\n        mark=triangle,\n        brown,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend early bars style/.style={\n        ybar,\n        bar width=0.4cm,\n        solid,\n        mark=none,\n        fill=CustomC_Base,\n        line width=0pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=0pt,\n        mark size=1.5pt,\n    }\n}\n\n\\pgfplotsset{\n    legend late style/.style={\n        solid,\n        mark=diamond,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_2b style/.style={\n        solid,\n        mark=pentagon,\n        CustomD_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_4b style/.style={\n        solid,\n        mark=square,\n        CustomD_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_0_9b style/.style={\n        solid,\n        mark=o,\n        CustomD_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend late_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomD_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_init style/.style={\n        solid,\n        mark=triangle,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_noconnect style/.style={\n        only marks,\n        mark=*,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_2_2b style/.style={\n        solid,\n        mark=triangle,\n        CustomG_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_shape style/.style={\n        solid,\n        mark=diamond,\n        CustomG_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_2b_shape style/.style={\n        solid,\n        mark=pentagon,\n        CustomG_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_4b_shape style/.style={\n        solid,\n        mark=square,\n        CustomG_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend moe_0_9b_shape style/.style={\n        solid,\n        mark=o,\n        CustomG_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend moe_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomG_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend hard style/.style={\n        solid,\n        mark=*,\n        CustomP_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_4b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_0_9b style/.style={\n        solid,\n        mark=*,\n        CustomP_Light3,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_2_2b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark1,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\\pgfplotsset{\n    legend hard_3_3b style/.style={\n        solid,\n        mark=*,\n        CustomP_Dark2,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}\n\n\\pgfplotsset{\n    legend late_vision style/.style={\n        dotted,\n        mark=*,\n        CustomD_Base,\n        mark options={solid},\n        line width=1pt,\n        mark size=1.5pt\n    }\n}<PLACEHOLDER_preamble_end>\n\n\\definecolor{citecolor}{HTML}{0071bc}\n\n\\crefname{section}{\\S}{\\S\\S}\n\n<PLACEHOLDER_NEWCOMMAND_31>\n<PLACEHOLDER_NEWCOMMAND_32>\n<PLACEHOLDER_NEWCOMMAND_33>\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\n    Mustafa Shukor\\thanks{Work done during an internship at Apple.} \\\\\n    Sorbonne University\n    \\And\n    Enrico Fini \\\\\n    Apple\n    \\And\n    Victor Guilherme Turrisi da Costa \\\\\n    Apple\n    \\And\n    Matthieu Cord \\\\\n    Sorbonne University\n    \\And\n    Joshua Susskind \\\\\n    Apple\n    \\And\n    Alaaeldin El-Nouby \\\\\n    Apple\n}\n\n<PLACEHOLDER_NEWCOMMAND_34>\n<PLACEHOLDER_NEWCOMMAND_35>"
    },
    {
        "section": "0",
        "content": "\\begin{document}\n\\setcounter{tocdepth}{2}\n\\doparttoc\n<PLACEHOLDER_NEWCOMMAND_36>\n\\faketableofcontents\n\n\\maketitle\n\n<PLACEHOLDER_sec/0_abstract_begin>\\vspace{-0.5cm}\n<PLACEHOLDER_ENV_1><PLACEHOLDER_sec/0_abstract_end>\n<PLACEHOLDER_sec/1_intro_begin>",
        "trans_content": "\\begin{document}\n\\setcounter{tocdepth}{2}\n\\doparttoc\n<PLACEHOLDER_NEWCOMMAND_36>\n\\faketableofcontents\n\n\\maketitle\n\n<PLACEHOLDER_sec/0_abstract_begin>\\vspace{-0.5cm}\n<PLACEHOLDER_ENV_1><PLACEHOLDER_sec/0_abstract_end>\n<PLACEHOLDER_sec/1_intro_begin>"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\n\\label{sec:intro}\n\nMultimodality provides a rich signal for perceiving and understanding the world.\nAdvances in vision\n\\citep{radford2021learning,oquab2023dinov2,zhai2023sigmoidsiglip,fini2024multimodalaimv2},\n\\edit{audio \\citep{huang2022masked,elizalde2023clap,chen2022wavlm,hsu2021hubert}}\nand language models \\citep{achiam2023gpt4,team2023gemini,dubey2024llama3}\nhave enabled the development of powerful multimodal models that understand\nlanguage, images, and audio. A common approach involves grafting separately\npre-trained unimodal models, \\edit{such as connecting a vision encoder to the input\nlayer of an\nLLM~\\citep{laurenccon2024mattersidefics2,shukor2023epalm,alayrac2022flamingo,\nxue2024xgenblip3,beyer2024paligemma,wang2024qwen2,liu2024improvedllava,zhang2023videollama,kong2024audioflam,defossez2024moshi}.}\n\nAlthough this seems like a convenient approach, it remains an open question\nwhether such late-fusion strategies are inherently optimal \\edit{for\nunderstanding multimodal signals}.  Moreover, with abundant multimodal data\navailable, initializing from unimodal pre-training is potentially detrimental,\nas it may introduce biases that prevent the model from \\edit{fully leveraging\ncross-modality co-dependancies}.\nAn additional challenge is scaling such systems;  each component (e.g., vision\nencoder, LLM) has its own set of hyperparameters, \\edit{pre-training data\nmixtues}, and \\edit{scaling properties with respect to the amount} of data and\ncompute applied. A more flexible architecture might allow the model to\ndynamically allocate its capacity across modalities, simplifying scaling\nefforts.\n\nIn this work, we focus on the scaling properties of native multimodal models\ntrained from the ground up on multimodal data. We first investigate whether\n\\edit{the commonly adopted} late-fusion architectures hold an intrinsic\nadvantage by comparing them to early-fusion models, which process raw multimodal\ninputs without relying on \\edit{dedicated vision encoders}.\nWe conduct scaling experiments on early and late fusion architectures, deriving\nscaling laws to predict their performance and compute-optimal configurations.\nOur findings indicate that late fusion offers no inherent advantage when\n\\edit{trained} from scratch. Instead, early-fusion models are more efficient and\nare easier to scale. Furthermore, we observe that native multimodal models\nfollow scaling laws similar to those of LLMs~\\citep{hoffmann2022training},\nalbeit with slight variations in scaling coefficients across modalities and\ndatasets. Our results suggest that model parameters and training tokens should\nbe scaled \\edit{roughly equally} for optimal performance.\nMoreover, we find that different \\edit{multimodal} training mixtures exhibit\nsimilar overall trends, indicating that our findings are likely to generalize to\na broader range of settings.\n\nWhile our findings favor early fusion, multimodal data is inherently\nheterogeneous, suggesting that some degree of parameter specialization may still\noffer benefits. To \\edit{investigate} this, we \\edit{explore leveraging} Mixture\nof Experts (MoEs)~\\citep{shazeer2017outrageously}, a technique that enables the\nmodel to dynamically allocate specialized parameters across modalities in a\nsymmetric and parallel manner, in contrast to late-fusion models, which are\nasymmetric and process data sequentially. Training native multimodal models with\nMoEs results in significantly improved performance and \\edit{therefore,} faster\nconvergence. \\edit{Our scaling laws for MoEs suggest that scaling number of\ntraining tokens is more important the number of active parameters. This\nunbalanced scaling is different from what is observed for dense models, due to\nthe higher number of total parameters for sparse models.} \\edit{In addition,\n}our analysis reveals that experts tend to specialize in different modalities,\nwith this specialization being particularly prominent in the early and last\nlayers.\n\n<PLACEHOLDER_figs/teaser_begin>\n<PLACEHOLDER_ENV_2><PLACEHOLDER_figs/teaser_end>",
        "trans_content": "\\section{引言}\n\\label{sec:intro}\n\n多模态提供了丰富的信号，用于感知和理解世界。\n视觉方面的进展\n\\citep{radford2021learning,oquab2023dinov2,zhai2023sigmoidsiglip,fini2024multimodalaimv2}， \n\\edit{音频方面的进展 \\citep{huang2022masked,elizalde2023clap,chen2022wavlm,hsu2021hubert}} \n以及语言模型方面的进展 \\citep{achiam2023gpt4,team2023gemini,dubey2024llama3} \n使得强大的多模态模型得以发展，这些模型能够理解语言、图像和音频。 \n一种常见的方法是将分别经过预训练的单模态模型进行拼接，\\edit{例如，将视觉编码器连接到LLM的输入层~\\citep{laurenccon2024mattersidefics2,shukor2023epalm,alayrac2022flamingo,\nxue2024xgenblip3,beyer2024paligemma,wang2024qwen2,liu2024improvedllava,zhang2023videollama,kong2024audioflam,defossez2024moshi}.}\n\n尽管这种方法看起来很方便，但是否这种后融合策略本质上是最优的 \\edit{用于理解多模态信号} 仍然是一个悬而未决的问题。 \n此外，随着大量多模态数据的可用，从单模态预训练初始化可能会适得其反，因为这可能引入偏见，阻止模型 \\edit{充分利用跨模态的共同依赖性}。 \n另一个挑战是扩展这些系统；每个组件（例如，视觉编码器、LLM）都有自己的超参数集，\\edit{预训练数据混合物}，以及与数据量和计算量相关的 \\edit{扩展属性}。 \n一种更灵活的架构可能允许模型动态地在各个模态之间分配其容量，从而简化扩展工作。\n\n在本研究中，我们重点研究了从头开始在多模态数据上训练的本地多模态模型的扩展属性。 \n我们首先调查 \\edit{常见的} 后融合架构是否具有内在优势，通过将其与早期融合模型进行比较，后者在处理原始多模态输入时不依赖于 \\edit{专用视觉编码器}。\n我们对早期和后期融合架构进行了扩展实验，推导出了扩展规律，以预测它们的性能和计算最优配置。\n我们的研究结果表明，从头开始训练时，后融合并没有提供固有的优势。 相反，早期融合模型更高效，且更容易扩展。 \n此外，我们观察到，本地多模态模型遵循与LLM相似的扩展规律~\\citep{hoffmann2022training}，尽管在不同模态和数据集之间，扩展系数略有变化。 \n我们的结果表明，模型参数和训练令牌应 \\edit{大致相等地} 扩展，以实现最佳性能。 \n此外，我们发现，不同的 \\edit{多模态} 训练混合物表现出相似的整体趋势，表明我们的发现很可能能够推广到更广泛的设置。\n\n虽然我们的研究结果偏向早期融合，但多模态数据本质上是异质的，这表明某种程度的参数专业化仍可能带来好处。 \n为了 \\edit{探讨} 这一点，我们 \\edit{探索利用} 专家混合（MoEs）~\\citep{shazeer2017outrageously}，这是一种使模型能够在各个模态之间以对称和并行的方式动态分配专用参数的技术， \n与后融合模型不同，后者是非对称的，并且按顺序处理数据。 \n使用MoEs训练本地多模态模型显著提高了性能，并且 \\edit{因此，} 加速了收敛。 \n\\edit{我们关于MoEs的扩展规律表明，训练令牌的扩展比活动参数的数量更为重要。 这种不平衡的扩展与密集模型的观察结果不同，因为稀疏模型的总参数数量较高。} \n\\edit{此外，}我们的分析表明，专家往往会专注于不同的模态，特别是在早期和最后几层中，这种专业化尤为突出。\n\n<PLACEHOLDER_figs/teaser_begin>\n<PLACEHOLDER_ENV_2><PLACEHOLDER_figs/teaser_end>"
    },
    {
        "section": "1_1",
        "content": "\\subsection{Summary of our findings}\nOur findings can be summarized as follows:\n\n\\cpar{Native early and late fusion perform on par:} \\edit{Early fusion models trained\nfrom scratch}\nperform on par with their late-fusion counterparts, with a\nslight advantage to early-fusion models for low compute budgets\n(\\cref{fig:early_vs_early_init_scaledata}). Furthermore, our scaling laws study\nindicates that the compute-optimal models for early and late fusion perform\nsimilarly as the compute budget increases~(\\cref{fig:teaser} Left).\n\n\\cpar{NMMs scale similarly to LLMs:} The scaling laws of native multimodal\nmodels follow similar laws as text-only LLMs with slightly varying  scaling\nexponents depending on the target data type and training mixture\n(\\cref{tab:early_vs_late_coeffs}).\n\n\\cpar{Late-fusion requires more parameters:}\nCompute-optimal late-fusion models require a higher parameters-to-data ratio\nwhen compared to early-fusion (\\cref{fig:teaser} Right).\n\n\\cpar{Sparsity significantly benefits early-fusion NMMs:} Sparse NMMs exhibit\nsignificant improvements compared to their dense counterparts at the same\ninference cost~(\\cref{fig:dense_vs_moe_scaledata}). Furthermore, they implicitly\nlearn modality-specific weights when trained with\nsparsity~(\\cref{fig:app_moes_specialization}). \\edit{In addition,\ncompute-optimal models rely more on scaling the number of training tokens than\nthe number of active parameters as the compute-budget grows (\\cref{fig:teaser} Right).}\n\n\\cpar{Modality-agnostic routing beats Modality-aware routing for Sparse NMMs:}\nTraining sparse mixture of experts with modality-agnostic routing consistently\noutperforms models with modality-aware routing\n(\\cref{fig:hard_vs_moe_scaledata}).\n\n\\vspace{-5pt}\n\n<PLACEHOLDER_sec/1_intro_end>\n<PLACEHOLDER_sec/1.5_preliminaries_begin>",
        "trans_content": "\\subsection{我们的研究发现总结}\n我们的研究发现可以总结如下：\n\n\\cpar{原生早期融合和晚期融合表现相当：} \\edit{从头开始训练的早期融合模型}与晚期融合模型表现相当，在低计算预算下，早期融合模型略有优势 (\\cref{fig:early_vs_early_init_scaledata})。此外，我们的扩展规律研究表明，随着计算预算的增加，早期和晚期融合的计算最优模型表现相似~(\\cref{fig:teaser} Left)。\n\n\\cpar{原生多模态模型与大型语言模型的扩展规律相似：} 原生多模态模型的扩展规律与仅文本的大型语言模型遵循类似的规律，尽管根据目标数据类型和训练混合的不同，扩展指数略有差异 (\\cref{tab:early_vs_late_coeffs})。\n\n\\cpar{晚期融合需要更多的参数：} 与早期融合模型相比，计算最优的晚期融合模型需要更高的参数与数据比例 (\\cref{fig:teaser} Right)。\n\n\\cpar{稀疏性显著提升早期融合的原生多模态模型：} 与其密集型模型相比，稀疏型多模态模型在相同推理成本下表现出显著的改进~(\\cref{fig:dense_vs_moe_scaledata})。此外，它们在训练时会隐式地学习模态特定的权重~(\\cref{fig:app_moes_specialization})。 \\edit{此外，随着计算预算的增长，计算最优的模型更依赖于扩大训练令牌的数量，而非激活参数的数量 (\\cref{fig:teaser} Right)。}\n\n\\cpar{对于稀疏型原生多模态模型，模态无关的路由优于模态感知路由：} 使用模态无关路由训练稀疏专家混合模型始终优于使用模态感知路由的模型 (\\cref{fig:hard_vs_moe_scaledata})。\n\n\\vspace{-5pt}\n\n<PLACEHOLDER_sec/1_intro_end>\n<PLACEHOLDER_sec/1.5_preliminaries_begin>"
    },
    {
        "section": "2+2_1",
        "content": "\\section{Preliminaries}\n\n\n\\subsection{Definitions}\n\n\\cpar{Native Multimodal Models (NMMs):}\nModels that are trained from scratch on all modalities simultaneously without\nrelying on pre-trained LLMs or vision encoders. Our focus is on the\nrepresentative image and text modalities, where the model processes both text\nand images as input and generates text as output.\n\n\\cpar{Early fusion:} Enabling multimodal interaction from the beginning, using\nalmost no modality-specific parameters (\\eg, except a linear layer to patchify\nimages). Using a single transformer model, this approach processes raw\nmultimodal \\edit{input—tokenized text and continuous image patches—with no\nimage discretization.} \\edit{We} refer to the main transformer as\nthe decoder.\n\n\\cpar{Late fusion:} Delaying the multimodal interaction \\edit{to} deeper layers,\ntypically after separate unimodal components \\edit{has processed} each modality\nindependently (\\eg, a vision encoder connected to an LLM).\n\n\\cpar{Modality-agnostic routing:} In sparse mixture-of-experts, \\edit{modality-agnostic} routing\nrefers to relying on a learned router module that is trained jointly with the\nmodel.\n\n\\cpar{Modality-aware routing:} Routing based on pre-defined rules such as\nrouting based on the modality type (\\eg, vision-tokens, token-tokens).\n\n<PLACEHOLDER_tables/notations_begin><PLACEHOLDER_ENV_3><PLACEHOLDER_tables/notations_end>",
        "trans_content": "\\section{预备知识}\n\n\\subsection{定义}\n\n\\cpar{原生多模态模型（Native Multimodal Models, NMMs）：}\n指的是从零开始在所有模态上同时训练的模型，不依赖于预训练的大型语言模型（LLMs）或视觉编码器。我们的关注点是图像和文本这两个典型模态，其中模型同时处理文本和图像作为输入，并生成文本作为输出。\n\n\\cpar{早期融合（Early fusion）：}\n从一开始就实现多模态交互，几乎不使用模态特定的参数（\\eg，除了用于图像块化的线性层）。该方法使用一个单一的 transformer 模型，处理原始的多模态 \\edit{输入——即标记化的文本和连续的图像块——而无需图像离散化。} \\edit{我们}将该主 transformer 称为解码器。\n\n\\cpar{晚期融合（Late fusion）：}\n将多模态交互延后到更深层，通常是在各自的单模态组件 \\edit{独立处理完}每个模态之后（\\eg，将视觉编码器连接到大型语言模型）。\n\n\\cpar{模态无关路由（Modality-agnostic routing）：}\n在稀疏专家混合机制中，\\edit{模态无关}的路由是指依赖于与模型联合训练的学习型路由器模块。\n\n\\cpar{模态感知路由（Modality-aware routing）：}\n基于预定义规则的路由方式，例如根据模态类型进行路由（\\eg，视觉 token、文本 token）。\n\n<PLACEHOLDER_tables/notations_begin><PLACEHOLDER_ENV_3><PLACEHOLDER_tables/notations_end>"
    },
    {
        "section": "2_2",
        "content": "\\subsection{Scaling Laws}\nWe aim to understand the scaling properties of NMMs and how different\narchitectural choices influence trade-offs. To this end, we analyze our models\nwithin the scaling laws framework proposed by~\\citet{kaplan2020scaling,\nhoffmann2022training}.\nWe compute FLOPs based on the total number of parameters, using the\napproximation \\(C = 6ND\\), as adopted in prior\nwork~\\citep{hoffmann2022training,abnar2025parameters}. However, we modify this\nestimation to suit our setup: for late-fusion models, FLOPs is computed as\n\\(6(N_vD_v + ND)\\).\nWe consider a setup where, given a compute budget \\(C\\), our goal is to predict\nthe model’s final \\edit{loss}, as well as determine the optimal number of\nparameters \\edit{and} number of training tokens. Consistent with prior studies on LLM\nscaling~\\citep{hoffmann2022training}, we assume a power-law relationship between\nthe final model loss and both model size (\\(N\\)) and training tokens (\\(D\\)):\n\n<PLACEHOLDER_ENV_4>\n\n\\noindent Here, \\(E\\) represents the lowest achievable loss on the dataset,\nwhile \\(\\frac{A}{N^{\\alpha}}\\) captures the effect of increasing the number of\nparameters, where a larger model leads to lower loss, with the rate of\nimprovement governed by \\(\\alpha\\). Similarly, \\(\\frac{B}{D^{\\beta}}\\) accounts\nfor the benefits of a higher number of tokens, with \\(\\beta\\) determining the\nrate of improvement. Additionally, we assume a linear relationship between\ncompute budget (FLOPs) and both \\(N\\) and \\(D\\) (\\(C \\propto ND\\)). This further\nleads to power-law relationships detailed in \\cref{tab:power_laws}.\n\n<PLACEHOLDER_figs/scaling_laws_early_vs_late_begin>\n<PLACEHOLDER_ENV_5><PLACEHOLDER_figs/scaling_laws_early_vs_late_end>\n\n<PLACEHOLDER_ENV_6>",
        "trans_content": "\\subsection{尺度定律}\n我们的目标是理解NMM的尺度特性，以及不同的架构选择如何影响权衡。为此，我们在~\\citet{kaplan2020scaling, hoffmann2022training} 提出的尺度定律框架内分析我们的模型。\n我们基于总参数数量计算FLOPs，使用近似值 \\(C = 6ND\\)，这一近似在之前的工作中被采用~\\citep{hoffmann2022training, abnar2025parameters}。然而，我们对这个估计进行了修改，以适应我们的设置：对于晚期融合模型，FLOPs被计算为 \\(6(N_vD_v + ND)\\)。\n我们考虑一个设置，在给定计算预算 \\(C\\) 的情况下，我们的目标是预测模型的最终 \\edit{损失}，并确定最佳的参数数量 \\edit{和} 训练令牌数量。与之前关于LLM尺度研究一致~\\citep{hoffmann2022training}，我们假设最终模型损失与模型大小 (\\(N\\)) 和训练令牌 (\\(D\\)) 之间存在幂律关系：\n\n<PLACEHOLDER_ENV_4>\n\n\\noindent 其中，\\(E\\) 代表数据集上可实现的最低损失，而 \\(\\frac{A}{N^{\\alpha}}\\) 捕捉了增加参数数量的效果，其中更大的模型会导致更低的损失，改善的速度由 \\(\\alpha\\) 控制。类似地，\\(\\frac{B}{D^{\\beta}}\\) 反映了更多训练令牌的益处，\\(\\beta\\) 决定了改善的速率。此外，我们假设计算预算（FLOPs）与 \\(N\\) 和 \\(D\\) 之间存在线性关系 (\\(C \\propto ND\\))。这进一步导致了在 \\cref{tab:power_laws} 中详细描述的幂律关系。\n\n<PLACEHOLDER_figs/scaling_laws_early_vs_late_begin>\n<PLACEHOLDER_ENV_5><PLACEHOLDER_figs/scaling_laws_early_vs_late_end>\n\n<PLACEHOLDER_ENV_6>"
    },
    {
        "section": "2_3",
        "content": "\\subsection{Experimental setup}\n\\edit{Our models} are based on the autoregressive transformer\narchitecture~\\citep{vaswani2017attention} with SwiGLU\nFFNs~\\citep{shazeer2020glu} and QK-Norm~\\citep{dehghani2023scaling}\nfollowing~\\citet{li2024datacomp}. In early-fusion models, image patches are\nlinearly projected to match the text token dimension, while late-fusion follows\nthe CLIP architecture~\\citep{radford2021learning}. We adopt causal attention for\ntext tokens and bidirectional attention for image tokens, we found this to work\nbetter. Training is conducted on a mixture of public and private multimodal\ndatasets, including DCLM \\citep{li2024datacomp}, Obelics\n\\citep{laurenccon2024obelics}, DFN \\citep{fang2023data}, COYO\n\\citep{kakaobrain2022coyo700m}, and a private collection of High-Quality\nImage-Text Pairs (HQITP) (see \\cref{tab:pretraining_datasets}). Images are\n\\edit{resized} to 224×224 resolution with a 14×14 patch size. We use a context\nlength of 1k for the multimodal sequences. For training efficiency, we train our\nmodels with \\texttt{bfloat16},  Fully Sharded Data Parallel (FSDP)\n\\citep{zhao2023pytorch}, \\edit{activation} checkpointing, and gradient accumulation. We\nalso use sequence packing for the image captioning dataset to reduce the amount\nof padded tokens. Similar to previous\nworks~\\citep{hoffmann2022training,aghajanyan2023scalingmm,abnar2025parameters},\nwe evaluate performance on a held-out subsets of interleaved (Obelics),\nImage-caption (HQITP), and text-only data (DCLM). Further implementation details\nare provided in~\\cref{app:implementation_details}.<PLACEHOLDER_sec/1.5_preliminaries_end>\n<PLACEHOLDER_sec/2_method_begin>",
        "trans_content": "\\subsection{实验设置}\n\\edit{我们的模型}基于自回归Transformer架构~\\citep{vaswani2017attention}，采用SwiGLU FFNs~\\citep{shazeer2020glu}和QK-Norm~\\citep{dehghani2023scaling}，参照~\\citet{li2024datacomp}的方法。在早期融合模型中，图像块被线性映射到与文本标记维度匹配，而晚期融合则遵循CLIP架构~\\citep{radford2021learning}。我们为文本标记采用因果注意力，为图像标记采用双向注意力，发现这种方式效果更佳。训练是在混合的公共和私有多模态数据集上进行的，包括DCLM \\citep{li2024datacomp}、Obelics \\citep{laurenccon2024obelics}、DFN \\citep{fang2023data}、COYO \\citep{kakaobrain2022coyo700m}，以及一个私有的高质量图像-文本对（HQITP）集合（见\\cref{tab:pretraining_datasets}）。图像被\\edit{调整大小}为224×224分辨率，图像块大小为14×14。我们为多模态序列采用1k的上下文长度。为了提高训练效率，我们使用\\texttt{bfloat16}、完全分片数据并行（FSDP）\\citep{zhao2023pytorch}、\\edit{激活}检查点和梯度累积进行训练。我们还对图像字幕数据集使用序列打包，以减少填充标记的数量。与之前的工作~\\citep{hoffmann2022training,aghajanyan2023scalingmm,abnar2025parameters}类似，我们在交替（Obelics）、图像-字幕（HQITP）和仅文本数据（DCLM）上评估性能。更多实现细节请参见~\\cref{app:implementation_details}。<PLACEHOLDER_sec/1.5_preliminaries_end>\n<PLACEHOLDER_sec/2_method_begin>"
    },
    {
        "section": "3",
        "content": "\\section{Scaling  native multimodal models}\n\nIn this section, we present a scaling laws study of native multimodal models,\nexamining various architectural choices~\\cref{sec:scaling_laws_early}, exploring\ndifferent data mixtures~\\cref{sec:scaling_data_mix}, analyzing the practical\ntrade-offs between late and early fusion\nNMMs, and comparing the performance of native\npre-training and continual pre-training of NMMs~\\cref{sec:native_vs_continual}.\n\n\\cpar{Setup.} We train models ranging from 0.3B to 4B active parameters,\nscaling the width while keeping the depth constant. For smaller training token\nbudgets, we reduce the warm-up phase to 1K steps while maintaining 5K steps for\nlarger budgets.\nFollowing~\\citet{hagele2024scaling}, models are trained with a constant learning\nrate, followed by a cool-down phase using an inverse square root scheduler. The\ncool-down phase spans 20\\% of the total steps spent at the constant learning\nrate.  To estimate the scaling coefficients in \\Cref{eq:scaling_laws}, we apply the\nL-BFGS algorithm~\\citep{lbfgs} and Huber loss~\\citep{Huber1992} (with $\\delta =\n10^{-3}$), performing a grid search over initialization ranges.\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_begin><PLACEHOLDER_ENV_7><PLACEHOLDER_tables/scaling_laws_coeffs_end>\n\n<PLACEHOLDER_figs/latr_vs_early_equal_flops_begin><PLACEHOLDER_ENV_8><PLACEHOLDER_figs/latr_vs_early_equal_flops_end>\n\n\\vspace{-0.5cm}",
        "trans_content": "\\section{缩放本地多模态模型}\n\n在本节中，我们呈现了本地多模态模型的缩放规律研究，考察了不同的架构选择~\\cref{sec:scaling_laws_early}，探索了不同的数据混合方式~\\cref{sec:scaling_data_mix}，分析了后期融合和早期融合NMMs之间的实际权衡，并比较了本地预训练和持续预训练NMMs的性能~\\cref{sec:native_vs_continual}。\n\n\\cpar{设置。} 我们训练的模型参数范围从0.3B到4B，缩放宽度，同时保持深度不变。对于较小的训练标记预算，我们将预热阶段减少到1K步，而对于较大的预算，则保持5K步。\n根据~\\citet{hagele2024scaling}，模型使用恒定学习率进行训练，然后使用逆平方根调度器进入冷却阶段。冷却阶段占总步骤的20\\%，这些步骤在恒定学习率下进行。为了估算\\Cref{eq:scaling_laws}中的缩放系数，我们应用了L-BFGS算法~\\citep{lbfgs}和Huber损失~\\citep{Huber1992}（$\\delta = 10^{-3}$），并在初始化范围上进行网格搜索。\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_begin><PLACEHOLDER_ENV_7><PLACEHOLDER_tables/scaling_laws_coeffs_end>\n\n<PLACEHOLDER_figs/latr_vs_early_equal_flops_begin><PLACEHOLDER_ENV_8><PLACEHOLDER_figs/latr_vs_early_equal_flops_end>\n\n\\vspace{-0.5cm}"
    },
    {
        "section": "3_1",
        "content": "\\subsection{Scaling laws of NMMs}\n\\label{sec:scaling_laws_early}\n\n\\cpar{Scaling laws for early-fusion and late-fusion models.}\n\\Cref{fig:early_vs_late_scaleflops_3d}~(left) presents the final loss averaged\nacross interleaved, image-caption, and text datasets for early-fusion NMMs. The\nlowest-loss frontier follows a power law as a function of FLOPs. Fitting the\npower law yields the expression $L \\propto C^{-0.049}$, indicating the rate of\nimprovement with increasing compute. When analyzing the scaling laws per data\ntype (\\eg, image-caption, interleaved, text), we observe that the exponent\nvaries (\\cref{tab:early_vs_late_coeffs}). For instance, the model achieves a\nhigher rate of improvement for image-caption data $(L \\propto C^{-0.061})$ when\ncompared to interleaved documents $(L \\propto C^{-0.046}$).\n\nTo model the loss as a function of the number of training tokens $D$ and model\nparameters $N$, we fit the parametric function in \\cref{eq:scaling_laws}, obtaining\nscaling exponents $\\alpha = 0.301$ and $\\beta = 0.335$. These describe the rates\nof improvement when scaling the number of model parameters and training tokens, respectively.\nAssuming a linear relationship between compute, $N$, and $D$ (\\ie, $C \\propto ND$),\nwe derive the law relating model parameters to the compute budget (see\n\\cref{app:scaling_laws} for details). Specifically, for a given compute budget\n$C$, we compute the corresponding model size $N$ at logarithmically spaced $D$\nvalues and determine $N_{opt}$, the parameter count that minimizes loss.\nRepeating this across different FLOPs values produces a dataset of $(C,\nN_{opt})$, to which we fit a power law predicting the compute-optimal model size\nas a function of compute: $N^* \\propto C^{0.526}.$\n\nSimilarly, we fit power laws to estimate the compute-optimal training dataset\nsize as a function of compute and model size:\n\\[\nD_{opt} \\propto C^{0.473}, \\quad D_{opt} \\propto N^{0.899}.\n\\]\nThese relationships allow practitioners to determine the optimal model and\ndataset size given a fixed compute budget. When analyzing by data type, we find\nthat interleaved data benefits more from larger models ($a=0.532$) compared to\nimage-caption data ($a=0.520$), whereas the opposite trend holds for training\ntokens.\n\n<PLACEHOLDER_figs/data_mixtures_scaling_begin>\n<PLACEHOLDER_ENV_9>\n<PLACEHOLDER_figs/data_mixtures_scaling_end>\n\nWe conduct a similar study on late-fusion models\nin~\\cref{fig:early_vs_late_scaleflops_3d}~(right) and observe comparable scaling\nbehaviors. In particular, the loss scaling exponent ($c = -0.0494$) is nearly\nidentical to that of early fusion ($c = -0.0492$).\nThis trend is evident in \\cref{fig:early_vs_late_scaleflops}, where early fusion\noutperforms late fusion at smaller model scales, while both architectures\nconverge to similar performance at larger model sizes. We also observe similar\ntrends when varying late-fusion configurations, such as using a smaller vision\nencoder with a larger text decoder~\\cref{app:late_vs_early}.\n\n<PLACEHOLDER_ENV_10>\n\n\\cpar{Scaling laws of NMMs \\textit{vs} LLMs.}\nUpon comparing the scaling law coefficients of our NMMs to those reported for\ntext-only LLMs (\\eg, GPT-3, Chinchilla), we find them to be within similar\nranges. In particular, for predicting the loss as a function of compute,\nGPT-3~\\citep{brown2020language} follows $L \\propto C^{-0.048}$, while our models\nfollow $L \\propto C^{-0.049}$, suggesting that the performance of NMMs adheres\nto similar scaling laws as LLMs.\nSimilarly, our estimates of the $\\alpha$ and $\\beta$ parameters in\n\\cref{eq:scaling_laws} ($\\alpha=0.301$, $\\beta=0.335$) closely match those\nreported by~\\citet{hoffmann2022training} ($\\alpha=0.339$, $\\beta=0.285$).\nLikewise, our computed values of $a=0.526$ and $b=0.473$ align closely with\n$a=0.46$ and $b=0.54$ from~\\citet{hoffmann2022training}, reinforcing the idea\nthat, for native multimodal models, the number of training tokens and model\nparameters should be scaled proportionally.\nHowever, since the gap between $a$ and $b$ is smaller than in LLMs, this\nprinciple holds even more strongly for NMMs. Additionally, as $a=0.526$ is\ngreater than $b=0.473$ in our case, the optimal model size for NMMs is larger\nthan that of LLMs, while the optimal number of training tokens is lower, given\na fixed compute budget.\n\n\\cpar{Compute-optimal trade-offs for early \\textit{vs.} late fusion NMMs.}\nWhile late- and early-fusion models reduce loss at similar rates with increasing\nFLOPs, we observe distinct trade-offs in their compute-optimal models.\nSpecifically, $N_{opt}$ is larger for late-fusion models, whereas $D_{opt}$ is\nlarger for early-fusion models. This indicates that, given a fixed compute\nbudget, late-fusion models require a higher number of parameters, while early-fusion\nmodels benefit more from a higher number of training tokens.\nThis trend is also reflected in the lower $\\frac{N_{opt}}{D_{opt}} \\propto\nC^{0.053}$ for early fusion compared to $\\frac{N_{opt}}{D_{opt}} \\propto\nC^{0.076}$ for late fusion. As shown in \\cref{fig:teaser}~(right), when scaling FLOPs,\nthe number of parameters of early fusion models becomes significantly lower, which is crucial\nfor reducing inference costs and, consequently, lowering serving costs after\ndeployment.\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_begin><PLACEHOLDER_ENV_11><PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_end>\n\n\\cpar{Early-fusion is more efficient to train.}\nWe compare the training efficiency of late- and early-fusion architectures. As shown in \\cref{fig:early_vs_late_efficiency}, early-fusion models consume less memory and train faster under the same compute budget. This advantage becomes even more pronounced as compute increases, highlighting the superior training efficiency of early fusion while maintaining comparable performance to late fusion at scale. Notably, for the same FLOPs, late-fusion models have a higher parameter count and higher effective depth (\\ie, additional vision encoder layers alongside decoder layers) compared to early-fusion models.\n\n<PLACEHOLDER_ENV_12>",
        "trans_content": "\\subsection{NMM 的缩放法则}\n\\label{sec:scaling_laws_early}\n\n\\cpar{早期融合与后期融合模型的缩放法则。}\n\\Cref{fig:early_vs_late_scaleflops_3d}~(左) 展示了早期融合 NMM 在交错数据、图文配对数据和纯文本数据上的最终损失均值。最低损失前沿遵循 FLOPs 的幂律函数。拟合该幂律得到表达式 $L \\propto C^{-0.049}$，表明随着计算量增加，模型性能的提升速率。当分别按数据类型（如图文配对、交错、文本）分析缩放法则时，可以观察到指数有所差异（见 \\cref{tab:early_vs_late_coeffs}）。例如，在图文配对数据上模型获得更高的提升速率 $(L \\propto C^{-0.061})$，而在交错文档上为 $(L \\propto C^{-0.046})$。\n\n为了将损失建模为训练 token 数 $D$ 和模型参数量 $N$ 的函数，我们对 \\cref{eq:scaling_laws} 中的参数化函数进行拟合，得到缩放指数 $\\alpha = 0.301$ 和 $\\beta = 0.335$。这两个指数分别描述了随着模型参数量和训练 token 数增加时的性能提升速率。假设计算量 $C$ 与 $N$ 和 $D$ 成线性关系（即 $C \\propto ND$），我们推导了模型参数与计算预算之间的关系法则（详见 \\cref{app:scaling_laws}）。具体而言，对于给定的计算预算 $C$，我们在对数间距的 $D$ 值下计算对应的模型大小 $N$，并确定能够最小化损失的参数数量 $N_{opt}$。在不同 FLOPs 值上重复此过程，生成 $(C, N_{opt})$ 数据集，并拟合一个幂律函数来预测最优模型大小与计算量之间的关系：$N^* \\propto C^{0.526}.$\n\n类似地，我们拟合幂律函数来估计最优训练数据集大小与计算量和模型大小之间的关系：\n\\[\nD_{opt} \\propto C^{0.473}, \\quad D_{opt} \\propto N^{0.899}.\n\\]\n这些关系使实践者能够在固定计算预算下确定最优模型规模和数据集大小。按数据类型分析时，我们发现交错数据相比图文配对数据更受益于更大的模型规模（$a=0.532$ 对比 $a=0.520$），而在训练 token 数方面则表现出相反的趋势。\n\n<PLACEHOLDER_figs/data_mixtures_scaling_begin>\n<PLACEHOLDER_ENV_9>\n<PLACEHOLDER_figs/data_mixtures_scaling_end>\n\n我们对后期融合模型也进行了类似研究，如~\\cref{fig:early_vs_late_scaleflops_3d}~(右) 所示，并观察到类似的缩放行为。尤其是损失缩放指数 ($c = -0.0494$) 几乎与早期融合模型 ($c = -0.0492$) 相同。\n该趋势在 \\cref{fig:early_vs_late_scaleflops} 中尤为明显，其中早期融合在小规模模型下优于后期融合，而在更大模型规模下两者的性能趋于一致。我们还在调整后期融合配置（例如使用更小的视觉编码器和更大的文本解码器）时观察到相似趋势~\\cref{app:late_vs_early}。\n\n<PLACEHOLDER_ENV_10>\n\n\\cpar{NMM 的缩放法则 \\textit{vs} LLM。}\n将我们 NMM 的缩放法则系数与文本专用 LLM（如 GPT-3、Chinchilla）中报告的系数进行比较，我们发现它们处于相近范围。特别是在预测损失与计算量的关系时，GPT-3~\\citep{brown2020language} 遵循 $L \\propto C^{-0.048}$，而我们的模型遵循 $L \\propto C^{-0.049}$，表明 NMM 的性能遵循与 LLM 相似的缩放法则。\n类似地，我们对 \\cref{eq:scaling_laws} 中参数 $\\alpha=0.301$, $\\beta=0.335$ 的估计值，与~\\citet{hoffmann2022training} 报告的值 ($\\alpha=0.339$, $\\beta=0.285$) 接近。同样地，我们计算的 $a=0.526$, $b=0.473$ 与~\\citet{hoffmann2022training} 中的 $a=0.46$, $b=0.54$ 非常一致，进一步强化了这样一个观点：对于原生多模态模型，训练 token 数和模型参数应当按比例进行扩展。\n然而，由于在 NMM 中 $a$ 与 $b$ 的差值小于 LLM，因此这一原则在 NMM 中体现得更为显著。此外，由于在我们模型中 $a=0.526$ 大于 $b=0.473$，所以在固定计算预算下，NMM 的最优模型规模大于 LLM，而最优训练 token 数则更少。\n\n\\cpar{早期融合 \\textit{vs.} 后期融合 NMM 的计算最优权衡。}\n虽然后期融合和早期融合模型在增加 FLOPs 时降低损失的速率相似，但它们在计算最优模型上存在显著差异。具体而言，后期融合模型的 $N_{opt}$ 更大，而早期融合模型的 $D_{opt}$ 更大。这表明在固定计算预算下，后期融合模型需要更多的参数，而早期融合模型则更依赖于更多的训练 token。\n这一趋势也体现在 $\\frac{N_{opt}}{D_{opt}} \\propto C^{0.053}$（早期融合）相比 $\\frac{N_{opt}}{D_{opt}} \\propto C^{0.076}$（后期融合）更低。如 \\cref{fig:teaser}~(右) 所示，在扩大 FLOPs 时，早期融合模型的参数数量显著减少，这对降低推理成本以及部署后的服务成本至关重要。\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_begin><PLACEHOLDER_ENV_11><PLACEHOLDER_tables/scaling_laws_coeffs_datamixture_end>\n\n\\cpar{早期融合训练更高效。}\n我们比较了后期融合与早期融合架构的训练效率。如 \\cref{fig:early_vs_late_efficiency} 所示，在相同计算预算下，早期融合模型消耗更少内存并训练更快。随着计算量的增加，这一优势更加显著，凸显了早期融合在保持与后期融合相当性能的同时具备更高的训练效率。值得注意的是，在相同 FLOPs 下，后期融合模型具有更多的参数数量以及更高的有效深度（即包含附加视觉编码器层和解码器层），相较于早期融合模型。\n\n<PLACEHOLDER_ENV_12>"
    },
    {
        "section": "3_2",
        "content": "\\subsection{\\edit{Scaling laws evaluation}}\n\\label{sec:scaling_laws_evaluation}\nFor each model size and number of training tokens, we compute the loss using the\nestimated functional form in \\cref{eq:scaling_laws} and compare it to the actual\nloss observed in our runs. \\Cref{fig:observed_vs_predicted_loss_extrapolation}\nand \\Cref{tab:scaling_laws_errors_main} visualizes these comparisons, showing\nthat our estimation is highly accurate, particularly for lower loss values and\nlarger FLOPs. We also assess our scaling laws in an extrapolation setting,\npredicting performance beyond the model sizes used for fitting. Notably, our\napproach estimates the performance of an 8B model with reasonable accuracy.\n\nAdditionally, we conduct a sensitivity analysis using bootstrapping.\nSpecifically, we sample \\( P \\) points with replacement (\\( P \\) being the total\nnumber of trained models) and re-estimate the scaling law coefficients. This\nprocess is repeated 100 times, and we report the mean and standard deviation of\neach coefficient. \\Cref{tab:scaling_laws_sensitivity_main} shows that our\nestimation is more precise for \\(\\beta\\) than for \\(\\alpha\\), primarily due to\nthe smaller number of model sizes relative to the number of different token\ncounts used to derive the scaling laws.",
        "trans_content": "\\subsection{\\edit{缩放定律评估}}\n\\label{sec:scaling_laws_evaluation}\n对于每个模型大小和训练令牌数量，我们使用\\cref{eq:scaling_laws}中估计的函数形式计算损失，并将其与我们实验中观察到的实际损失进行比较。\\Cref{fig:observed_vs_predicted_loss_extrapolation} 和 \\Cref{tab:scaling_laws_errors_main} 可视化了这些比较，结果表明我们的估计非常准确，尤其是在较低损失值和较大FLOP值下。我们还在外推设置中评估了我们的缩放定律，预测了超出用于拟合的模型大小的表现。值得注意的是，我们的方法能够合理准确地估计一个8B模型的表现。\n\n此外，我们通过自助法进行灵敏度分析。具体来说，我们有放回地抽取\\( P \\)个点（\\( P \\)为训练模型的总数），并重新估计缩放定律的系数。该过程重复进行100次，我们报告每个系数的均值和标准差。\\Cref{tab:scaling_laws_sensitivity_main} 显示，我们的估计在\\(\\beta\\)上的精度高于\\(\\alpha\\)，这主要是由于相对于用于推导缩放定律的不同令牌数量，模型大小的数量较少。"
    },
    {
        "section": "3_3",
        "content": "\\subsection{Scaling laws for different data mixtures}\n\\label{sec:scaling_data_mix}\nWe investigate how variations in the training mixture affect the scaling laws of\nnative multimodal models. To this end, we study four different mixtures that\nreflect common community\npractices~\\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila},\nwith Image Caption-Interleaved-Text ratios of \\colorbox{blue!10}{45-45-10} (our default setup),\n\\colorbox{red!10}{30-30-40}, \\colorbox{green!10}{40-20-40}, and \\colorbox{orange!10}{20-40-40}.\nFor each mixture, we conduct a separate scaling study by training 76 different\nmodels, following our setup in \\cref{sec:scaling_laws_early}. Overall,\n\\cref{fig:early_scaleflops_data_mixtures} shows that different mixtures follow\nsimilar scaling trends; however, the scaling coefficients vary depending on the\nmixture (\\cref{tab:scaling_laws_coeffs_data_mixtures}). Interestingly,\nincreasing the proportion of image-caption data (mixtures 1 and 2) leads to\nlower $a$ and higher $b$, whereas increasing the ratio of interleaved and text\ndata (mixtures 3 and 4) have the opposite effect.\nNotably, image-caption data contains more image tokens than text\ntokens; therefore, increasing its proportion results in more\nimage tokens, while increasing interleaved and text data increases text token\ncounts. This suggests that, when image tokens are prevalent, training for longer decreases the loss faster than increasing the model size.\nWe also found that for a fixed model size, increasing text-only and interleaved data ratio is in favor of\nearly-fusion \\cref{fig:early_vs_late_datatype_interleaved_text_main}.",
        "trans_content": "\\subsection{不同数据混合比例的缩放规律}  \n\\label{sec:scaling_data_mix}  \n我们研究了训练数据混合比例的变化如何影响原生多模态模型的缩放规律。为此，我们考察了四种不同的数据混合，反映了社区的常见做法~\\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}，其图像描述-交错文本-纯文本的比例分别为 \\colorbox{blue!10}{45-45-10}（我们的默认设置）、\\colorbox{red!10}{30-30-40}、\\colorbox{green!10}{40-20-40} 和 \\colorbox{orange!10}{20-40-40}。对于每种混合比例，我们分别开展了缩放研究，共训练了 76 个不同的模型，遵循我们在 \\cref{sec:scaling_laws_early} 中的实验设置。总体而言，\\cref{fig:early_scaleflops_data_mixtures} 显示，不同的混合比例遵循相似的缩放趋势；然而，缩放系数会因混合比例而异（\\cref{tab:scaling_laws_coeffs_data_mixtures}）。有趣的是，增加图像描述数据的比例（混合 1 和混合 2）会导致较低的 $a$ 和较高的 $b$，而增加交错文本和纯文本数据的比例（混合 3 和混合 4）则产生相反的效果。值得注意的是，图像描述数据中包含的图像标记多于文本标记；因此，提高其比例会带来更多的图像标记，而增加交错文本和纯文本数据则会增加文本标记数量。这表明，当图像标记占主导时，延长训练时长比扩大模型规模更能有效降低损失。我们还发现，在模型规模固定的情况下，提高纯文本和交错文本数据的比例有利于早期融合 \\cref{fig:early_vs_late_datatype_interleaved_text_main}。"
    },
    {
        "section": "3_4",
        "content": "\\subsection{Native multimodal pre-training \\textbf{\\vs} continual training of\nLLMs}\n\\label{sec:native_vs_continual}\nIn this section, we compare training natively from scratch to continual training\nafter initializing from a pre-trained LLM. We initialize the model from DCLM-1B~\\citep{fang2023data} that is trained on more than 2T tokens.\n\\Cref{fig:early_vs_early_init_scaledata} shows that native multimodal models can\nclose the gap with initialized models when trained for longer.\nSpecifically, on image captioning data, the model requires fewer than 100B\nmultimodal tokens to reach comparable performance. However, on interleaved and\ntext data, the model may need longer training—up to 1T tokens.\nConsidering the cost of pre-training, these results suggest that training\nnatively could be a more efficient approach for achieving the same performance on multimodal benchmarks.\n\n<PLACEHOLDER_figs/early_vs_early_init_scaledata_begin>\n<PLACEHOLDER_ENV_13><PLACEHOLDER_figs/early_vs_early_init_scaledata_end>",
        "trans_content": "\\subsection{原生多模态预训练 \\textbf{\\vs} 持续训练LLMs}\n\\label{sec:native_vs_continual}\n在本节中，我们比较了从头开始的原生训练与从预训练LLM初始化后的持续训练。我们从DCLM-1B~\\citep{fang2023data}初始化模型，该模型在超过2T的标记上进行了训练。\\Cref{fig:early_vs_early_init_scaledata} 显示了原生多模态模型在训练更长时间后可以缩小与初始化模型之间的差距。具体而言，在图像标注数据上，该模型需要不到100B的多模态标记即可达到可比的性能。然而，在交替和文本数据上，模型可能需要更长的训练时间——最多达到1T标记。考虑到预训练的成本，这些结果表明，原生训练可能是实现相同多模态基准性能的更高效方法。\n\n<PLACEHOLDER_figs/early_vs_early_init_scaledata_begin>\n<PLACEHOLDER_ENV_13><PLACEHOLDER_figs/early_vs_early_init_scaledata_end>"
    },
    {
        "section": "4",
        "content": "\\section{\\edit{Towards multimodal specialization}}\nPreviously, we demonstrated that early-fusion models achieve performance on par\nwith late-fusion models under a fixed compute budget. However, multimodal data\nis inherently heterogeneous, and training a unified model to fit such diverse\ndistributions may be suboptimal.\nHere, we argue for multimodal specialization within a unified architecture.\nIdeally, the model should implicitly adapt to each modality, for instance, by\nlearning modality-specific weights or specialized experts. MoEs is\na strong candidate for this approach, having demonstrated effectiveness in LLMs.\nIn this section, we highlight the advantages of sparse early-fusion models over\ntheir dense counterparts.\n\n\\cpar{Setup.} Our sparse models are based on the dropless-MoE implementation\nof~\\citet{gale2023megablocks}, which eliminates token dropping during training\ncaused by expert capacity constraints. We employ a top-$k$ expert-choice routing\nmechanism, where each token selects its top-$k$ experts among the $E$ available\nexperts. Specifically, we set $k=1$ and $E=8$, as we find this configuration to\nwork effectively.\nAdditionally, we incorporate an auxiliary load-balancing\nloss~\\citep{shazeer2017outrageously} with a weight of 0.01 to ensure a balanced\nexpert utilization. Following~\\citet{abnar2025parameters}, we compute training\nFLOPs as $6ND$, where $N$ represents the number of active parameters.",
        "trans_content": "\\section{\\edit{朝向多模态专业化}}\n\n此前，我们展示了在固定计算预算下，早期融合模型的表现与晚期融合模型相当。然而，多模态数据本质上是异质的，训练一个统一的模型来适应如此多样化的分布可能是次优的。在这里，我们主张在统一架构内实现多模态专业化。理想情况下，模型应自动适应每种模态，例如，通过学习特定模态的权重或专门的专家。MoEs 是这一方法的有力候选，已在大规模语言模型（LLMs）中证明了其有效性。在本节中，我们强调稀疏早期融合模型相较于其密集对手的优势。\n\n\\cpar{设置。} 我们的稀疏模型基于~\\citet{gale2023megablocks}的无丢弃 MoE 实现，该实现消除了由于专家容量限制而在训练过程中丢弃标记的问题。我们采用了一个 top-$k$ 专家选择路由机制，每个标记从 $E$ 个可用专家中选择其 top-$k$ 专家。具体而言，我们设置 $k=1$ 和 $E=8$，因为我们发现这一配置效果显著。此外，我们还结合了一个辅助负载平衡损失~\\citep{shazeer2017outrageously}，其权重为 0.01，以确保专家的平衡利用。遵循~\\citet{abnar2025parameters}，我们将训练过程的 FLOPs 计算为 $6ND$，其中 $N$ 代表活跃参数的数量。"
    },
    {
        "section": "4_1",
        "content": "\\subsection{Sparse vs dense NMMs when scaling FLOPs}\nWe compare sparse MoE models to their dense counterparts by training models with different numbers of active parameters and varying amounts of training tokens. \\cref{fig:dense_vs_moe_scaledata} shows that, under the same inference cost (or number of active parameters), MoEs significantly outperform dense models.\nInterestingly, this performance gap is more pronounced for smaller model sizes. This suggests that MoEs enable models to handle heterogeneous data more effectively and specialize in different modalities. However, as dense models become sufficiently large, the gap between the two architectures gradually closes.\n\n\\vspace{15pt}",
        "trans_content": "\\subsection{稀疏与密集NMM在扩展FLOPs时的比较}\n我们通过训练具有不同数量的激活参数和不同数量训练令牌的模型，将稀疏MoE模型与其密集对应模型进行比较。 \\cref{fig:dense_vs_moe_scaledata} 显示，在相同的推理成本（或激活参数数量）下，MoE显著优于密集模型。\n有趣的是，这种性能差距在较小的模型尺寸下更为显著。这表明，MoE使得模型能够更有效地处理异质数据，并在不同的模态中进行专业化。然而，随着密集模型变得足够大，两种架构之间的差距逐渐缩小。\n\n\\vspace{15pt}"
    },
    {
        "section": "4_2",
        "content": "\\subsection{Scaling laws for sparse early-fusion models}\nWe train different models (ranging from 300M to 3.4B active parameters) on\nvarying amounts of tokens (ranging from 250M to 600B) and report the final loss\nin \\cref{fig:early_scaleflops_moe_avg}. We fit a power law to the convex hull of\nthe lowest loss as a function of compute (FLOPs). Interestingly, the exponent\n($-0.047$) is close to that of dense NMMs ($-0.049$), indicating that both\narchitectures scale similarly. However, the multiplicative constant is smaller\nfor MoEs ($26.287$) compared to dense models ($29.574$), revealing lower loss.\nAdditionally, MoEs require longer training to reach saturation compared to dense\nmodels (\\cref{app:scaling_laws} for more details). \\edit{We also predict the\ncoefficients of \\cref{eq:scaling_laws} by \\edit{considering $N$ as the number of\nactive parameters. \\Cref{tab:early_vs_late_coeffs} shows significantly higher\n$\\alpha$ compared to dense models. Interestingly, $b$ is significantly higher\nthan $a$, revealing that the training tokens should be scaled at a higher rate\nthan the number of parameters when training sparse NMMs. We also experiment with a\nscaling law that takes into account the sparsity~\\citep{abnar2025parameters} and\nreached similar conclusions \\Cref{app:scaling_laws_moes}.}}",
        "trans_content": "\\subsection{稀疏早期融合模型的尺度定律}\n我们训练了不同的模型（参数量从3亿到34亿不等），使用不同数量的标记（从2.5亿到6000亿不等），并在\\cref{fig:early_scaleflops_moe_avg}中报告了最终的损失值。我们将幂律拟合到计算量（FLOPs）作为函数的最低损失的凸包上。有趣的是，指数（$-0.047$）接近密集型NMMs（$-0.049$）的指数，这表明这两种架构的扩展方式相似。然而，MoEs的乘法常数（$26.287$）比密集模型的（$29.574$）要小，这表明MoEs具有较低的损失。此外，MoEs需要比密集模型更长的训练时间才能达到饱和（有关更多细节，请参见\\cref{app:scaling_laws}）。 \\edit{我们还通过\\edit{考虑$N$作为活跃参数的数量}来预测\\cref{eq:scaling_laws}的系数。\\Cref{tab:early_vs_late_coeffs}显示，与密集模型相比，$\\alpha$显著较高。有趣的是，$b$显著高于$a$，这表明在训练稀疏NMMs时，训练标记应该以比参数数量更高的速率进行缩放。我们还实验了一种考虑稀疏性的尺度定律~\\citep{abnar2025parameters}，并得出了类似的结论，详见\\Cref{app:scaling_laws_moes}。}}"
    },
    {
        "section": "4_3",
        "content": "\\subsection{Modality-aware \\vs Modality-agnostic routing}\n\nAnother alternative to MoEs is modality-aware routing, where multimodal tokens are assigned to experts based on their modalities, similar\nto previous works~\\citep{bao2021vlmo,wang2022image}. We train models with\ndistinct image and text experts in the form of FFNs, where image tokens are\nprocessed only by the image FFN and text tokens only by the text FFN. Compared to modality-aware routing, MoEs exhibit significantly better performance on both image-caption and interleaved data as presented in~\\cref{fig:hard_vs_moe_scaledata}.\n\n<PLACEHOLDER_ENV_14>",
        "trans_content": "\\subsection{基于模态的路由 \\vs 无关模态的路由}\n\nMoEs的另一种替代方案是基于模态的路由，其中多模态的token根据其模态分配给专家，类似于先前的工作~\\citep{bao2021vlmo,wang2022image}。我们训练具有不同图像和文本专家的模型，专家的形式为FFN，其中图像token仅由图像FFN处理，文本token仅由文本FFN处理。与基于模态的路由相比，MoEs在图像-文本说明和交错数据上的表现显著更好，如~\\cref{fig:hard_vs_moe_scaledata}所示。\n\n<PLACEHOLDER_ENV_14>"
    },
    {
        "section": "4_4",
        "content": "\\subsection{Emergence of expert specialization and sharing}\n\\label{sec:specialization}\nWe investigate multimodal specialization in MoE architectures. In~\\cref{fig:tokens_assignment}, we visualize the normalized number of text and image tokens assigned to each expert across layers.  To quantify this specialization, we compute a specialization score, defined as the average, across all experts within a layer, of $1-H(p)$, where $H$ is the binary entropy of each expert's text/image token distribution. We plot this specialization score in~\\cref{fig:tokens_specialization}.  Higher specialization scores indicate a tendency for experts to focus on either text or image tokens, while lower scores indicate a shared behavior.  These visualizations provide clear evidence of modality-specific experts, particularly in the early layers. Furthermore, the specialization score decreases as the number of layers increases, before rising again in the last layers. This suggests that early and final layers exhibit higher modality specialization compared to mid-layers. This behavior is intuitive, as middle layers are expected to hold higher-level features that may generalize across modalities, \\edit{and consistent with findings in \\citep{shukor2024implicit} that shows increasing alignment between modalities across layers}. The emergence of both expert specialization and cross-modality sharing in our modality-agnostic MoE, suggests it may be a preferable approach compared to modality-aware sparsity. All data displayed here is from an early-fusion MoE model with 1B active parameters trained for 300B tokens.\n\n<PLACEHOLDER_tables/sft_results_begin>\n<PLACEHOLDER_ENV_15><PLACEHOLDER_tables/sft_results_end>\n\n\\vspace{-1cm}",
        "trans_content": "\\subsection{专家专精化与共享性的出现}  \n\\label{sec:specialization}  \n我们研究了 MoE 架构中的多模态专精化现象。在~\\cref{fig:tokens_assignment} 中，我们可视化了在各层中分配给每个专家的文本与图像 token 的归一化数量。为了量化这种专精化，我们计算了一个专精化得分，该得分定义为某一层中所有专家的 $1-H(p)$ 的平均值，其中 $H$ 是每个专家文本/图像 token 分布的二进制熵函数。我们在~\\cref{fig:tokens_specialization} 中绘制了这一专精化得分。较高的专精化得分表示专家倾向于专注于文本或图像 token，而较低的得分则表示存在共享行为。这些可视化结果清晰地表明存在模态特定的专家，尤其是在前几层中。此外，随着层数的增加，专精化得分先是下降，然后在最后几层再次上升。这表明早期与末尾的层在模态专精化方面表现更强，而中间层则较弱。这一行为是合理的，因为中间层被认为包含可以跨模态泛化的高层特征， \\edit{并且这也与 \\citep{shukor2024implicit} 中的发现一致，该研究表明不同模态在层级中表现出越来越强的一致性}。我们在模态无关的 MoE 中观察到专家专精化与跨模态共享的共同出现，表明相比于模态感知稀疏性方法，该方法可能更具优势。此处展示的所有数据均来自一个早期融合的 MoE 模型，该模型包含 10 亿个激活参数，并在 3000 亿个 token 上进行了训练。\n\n<PLACEHOLDER_tables/sft_results_begin>  \n<PLACEHOLDER_ENV_15><PLACEHOLDER_tables/sft_results_end>\n\n\\vspace{-1cm}"
    },
    {
        "section": "5",
        "content": "\\section{Evaluation on downstream tasks with SFT}\nFollowing previous work on scaling laws, we primarily rely on validation losses. However, we generally find that this evaluation correlates well\nwith performance on downstream tasks. To validate this, we conduct a multimodal\ninstruction tuning stage (SFT) on the LLaVA mixture \\citep{liu2024improvedllava} and report\naccuracy and CIDEr scores across several VQA and captioning tasks.\n\\cref{tab:sft} confirms the ranking of different model configurations.\nSpecifically, early fusion outperforms late fusion, and MoEs outperform dense\nmodels. However, since the models are relatively small (1.5B scale), trained\nfrom scratch, and fine-tuned on a small dataset, the overall scores\nare lower than the current state of the art.  Further implementation\ndetails can be found in~\\Cref{app:implementation_details}.\n\n<PLACEHOLDER_ENV_16>\n<PLACEHOLDER_sec/2_method_end>\n<PLACEHOLDER_sec/2_related_work_begin>",
        "trans_content": "\\section{基于SFT的下游任务评估}\n根据先前关于扩展法则的研究，我们主要依赖验证损失。然而，我们通常发现，这种评估与下游任务的表现高度相关。为了验证这一点，我们在LLaVA混合模型上进行了一阶段的多模态指令调优（SFT）\\citep{liu2024improvedllava}，并报告了多个VQA和图像字幕生成任务中的准确率和CIDEr分数。 \\cref{tab:sft} 确认了不同模型配置的排名。具体而言，早期融合优于晚期融合，MoEs优于密集型模型。然而，由于这些模型相对较小（1.5B规模），从头开始训练，并且在小型数据集上进行了微调，因此总体分数低于当前的最先进水平。更多的实现细节可以在~\\Cref{app:implementation_details} 中找到。\n\n<PLACEHOLDER_ENV_16>\n<PLACEHOLDER_sec/2_method_end>\n<PLACEHOLDER_sec/2_related_work_begin>"
    },
    {
        "section": "6",
        "content": "\\section{Related work}\n\n\\cpar{Large multimodal models.} A long-standing research goal has been to develop models capable of perceiving the world through multiple modalities, akin to human sensory experience.  Recent progress in vision and language processing has shifted the research focus from smaller, task-specific models toward large, generalist models that can handle diverse inputs \\citep{team2023gemini,hurst2024gpt4o}.  Crucially, pre-trained vision and language backbones often require surprisingly little adaptation to enable effective cross-modal communication \\citep{tsimpoukelli2021multimodalfrozen,shukor2023epalm,vallaeys2024improveddepalm,merullo2023linearly,koh2023grounding}.  Simply integrating a vision encoder with either an encoder-decoder architecture \\citep{shukor2023unival,wang2022ofa,lu2022unified,mizrahi20234m} or a decoder-only LLM has yielded highly capable multimodal systems \\citep{laurenccon2024mattersidefics2,alayrac2022flamingo,liu2024improvedllava,wang2024qwen2,xue2024xgenblip3,chen2024internvl,zhu2024minigpt,abdin2024phi3,dai2024nvlm,beyer2024paligemma,moon2024anymal}. This late-fusion approach, where modalities are processed separately before being combined, is now well-understood, with established best practices for training effective models \\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}.  In contrast, early-fusion models \\citep{fuyu8b,team2024chameleon,diao2024unveiling}, which combine modalities at an earlier stage, remain relatively unexplored, with only a limited number of publicly released models \\citep{fuyu8b,diao2024unveiling}.  Unlike \\citep{diao2024unveiling,team2024chameleon}, our models utilize only a single linear layer and rely exclusively on a next-token prediction loss. Furthermore, we train our models from scratch on all modalities without image tokenization.\n\n\\cpar{Native Multimodal Models.} We define native multimodal models as those trained from scratch on all modalities simultaneously \\citep{team2023gemini} rather than adapting LLMs to accommodate additional modalities. Due to the high cost of training such models, they remain relatively underexplored, with most relying on late-fusion architectures \\citep{kosmoshuang2023language,yu2022coca}. Some multimodal models trained from scratch \\citep{aghajanyan2022cm3,team2024chameleon,wang2024emu3} relax this constraint by utilizing pre-trained image tokenizers such as \\citep{vqgan,vqvae} to convert images into discrete tokens, integrating them into the text vocabulary. This approach enables models to understand and generate text and images, facilitating a more seamless multimodal learning process.\n\n\\cpar{Scaling laws.} Scaling law studies aim to predict how model\nperformance scales with training compute. Early works\n\\citep{kaplan2020scaling,hoffmann2022training} found that LLM performance follows\na power-law relationship with compute, enabling the compute-optimal estimation of the number of model parameters and training tokens at scale for a given budget. Similar research has\nextended these findings to sparse Mixture of Experts (MoE) models, considering\nfactors such as sparsity, number of experts, and routing granularity\n\\citep{krajewski2024scalingmoe,clark2022unifiedscalingmoe,wangscalingmoe}.\nScaling laws have also been observed across various domains, including image\nmodels \\citep{fini2024multimodalaimv2}, video models\n\\citep{rajasegaran2025empirical}, protein LLMs \\citep{scalingprotein}, and\nimitation learning \\citep{pearce2024scaling}. However, few studies have\ninvestigated scaling laws for multimodal models.\nNotably,~\\citet{aghajanyan2023scalingmm} examined multimodal models that tokenize\nmodalities into discrete tokens and include multimodal generation. In contrast,\nwe focus on studying early-fusion models that take raw multimodal inputs and\nare trained on interleaved multimodal data.\n\n\\cpar{Mixture of experts (MoEs).} Mixture of Experts~\\citep{shazeer2017outrageously} enables scaling model capacity by decoupling\nmodel size from per-sample compute. This is done through sparsely\nactivating a small number of parameters. This approach has led to large\nsparse models that rival dense counterparts while being more efficient during\ntraining and inference\n\\citep{fedus2022switch,sun2024hunyuan,jiang2024mixtral,liu2024deepseekv3,wei2024skywork}.\nMany studies have explored improving MoE LLMs across various aspects, such as\nload balancing, routing, stability, scaling, and granularity\n\\citep{lewis2021base,zoph2022st,lepikhin2020gshard}. However, there is limited\nresearch on adopting MoEs for multimodal models, with some work focusing on contrastive\nimage-text models~\\citep{mustafa2022multimodal} and late-fusion multimodal LLMs~\\citep{lin2024moe,li2024aria}. Additionally, some studies investigate\npredefined expert routing, where certain parameters are reserved to process specific modalities~\\cite {bao2021vlmo,chen2024eve,shen2023scaling}. We focus on studying MoEs for native early-fusion models\nrather than proposing new architectures.\n\n<PLACEHOLDER_figs/moe_specialization_begin>\n\n<PLACEHOLDER_ENV_17><PLACEHOLDER_figs/moe_specialization_end><PLACEHOLDER_sec/2_related_work_end>\n<PLACEHOLDER_sec/3_conclusion_begin>",
        "trans_content": "\\section{相关工作}\n\n\\cpar{大型多模态模型。} 长期以来的研究目标之一是开发能够通过多种模态感知世界的模型，类似于人类的感官体验。近年来，视觉和语言处理的进展使得研究重点从小型、特定任务的模型转向能够处理多样输入的大型通用模型 \\citep{team2023gemini,hurst2024gpt4o}。至关重要的是，预训练的视觉和语言主干通常只需要极少的适应，就能有效地实现跨模态通信 \\citep{tsimpoukelli2021multimodalfrozen,shukor2023epalm,vallaeys2024improveddepalm,merullo2023linearly,koh2023grounding}。仅仅将一个视觉编码器与编码器-解码器架构 \\citep{shukor2023unival,wang2022ofa,lu2022unified,mizrahi20234m} 或仅解码器LLM结合，就能够生成高度有效的多模态系统 \\citep{laurenccon2024mattersidefics2,alayrac2022flamingo,liu2024improvedllava,wang2024qwen2,xue2024xgenblip3,chen2024internvl,zhu2024minigpt,abdin2024phi3,dai2024nvlm,beyer2024paligemma,moon2024anymal}。这种后融合方法，即在将模态分开处理后再进行组合，现在已被充分理解，并且已经有了训练有效模型的最佳实践 \\citep{laurenccon2024obelics,mckinzie2025mm1,zhang2024mm1_5,lin2024vila}。相比之下，早期融合模型 \\citep{fuyu8b,team2024chameleon,diao2024unveiling} 在早期阶段就将模态结合起来，仍然相对未被深入探索，公开发布的模型也较为有限 \\citep{fuyu8b,diao2024unveiling}。与 \\citep{diao2024unveiling,team2024chameleon} 不同，我们的模型仅使用单个线性层，并且仅依赖于下一个标记预测损失。此外，我们从零开始训练模型，使用所有模态，而不进行图像标记化。\n\n\\cpar{原生多模态模型。} 我们将原生多模态模型定义为那些从头开始同时在所有模态上进行训练的模型 \\citep{team2023gemini}，而不是将LLM调整以适应额外的模态。由于训练这类模型的成本较高，它们仍然相对较少被探索，大多数依赖于后融合架构 \\citep{kosmoshuang2023language,yu2022coca}。一些从头开始训练的多模态模型 \\citep{aghajanyan2022cm3,team2024chameleon,wang2024emu3} 通过利用预训练的图像标记器，如 \\citep{vqgan,vqvae}，将图像转换为离散的标记，并将其集成到文本词汇中，从而放松了这一限制。这种方法使得模型能够理解和生成文本与图像，促进了更加无缝的多模态学习过程。\n\n\\cpar{规模法则。} 规模法则研究旨在预测模型性能如何随着训练计算量的增加而变化。早期的研究 \\citep{kaplan2020scaling,hoffmann2022training} 发现LLM性能与计算量呈幂律关系，从而使得能够根据预算进行计算的计算最优估算，估算模型参数的数量和训练标记的数量。类似的研究将这些发现扩展到稀疏的专家混合（MoE）模型，考虑了稀疏性、专家数量和路由粒度等因素 \\citep{krajewski2024scalingmoe,clark2022unifiedscalingmoe,wangscalingmoe}。规模法则还在多个领域中得到了验证，包括图像模型 \\citep{fini2024multimodalaimv2}、视频模型 \\citep{rajasegaran2025empirical}、蛋白质LLM \\citep{scalingprotein} 和模仿学习 \\citep{pearce2024scaling}。然而，关于多模态模型的规模法则研究较少。值得注意的是，\\citet{aghajanyan2023scalingmm} 研究了将模态标记化为离散标记并包括多模态生成的多模态模型。相比之下，我们的研究重点是早期融合模型，它们接受原始的多模态输入，并在交错的多模态数据上进行训练。\n\n\\cpar{专家混合（MoEs）。} 专家混合 \\citep{shazeer2017outrageously} 通过将模型规模与每样本计算解耦，实现了模型容量的扩展。通过稀疏激活少量参数来完成这一目标。这种方法促成了大型稀疏模型，它们在训练和推理时比密集模型更加高效，同时又能与密集模型相媲美 \\citep{fedus2022switch,sun2024hunyuan,jiang2024mixtral,liu2024deepseekv3,wei2024skywork}。许多研究探讨了如何改进MoE LLM，在负载平衡、路由、稳定性、扩展性和粒度等各方面 \\citep{lewis2021base,zoph2022st,lepikhin2020gshard}。然而，关于将MoE应用于多模态模型的研究较为有限，一些研究聚焦于对比图像-文本模型 \\citep{mustafa2022multimodal} 和后融合多模态LLM \\citep{lin2024moe,li2024aria}。另外，一些研究还探讨了预定义专家路由，其中某些参数专门用于处理特定模态 \\citep{bao2021vlmo,chen2024eve,shen2023scaling}。我们专注于研究用于原生早期融合模型的MoE，而非提出新架构。\n\n<PLACEHOLDER_figs/moe_specialization_begin>\n\n<PLACEHOLDER_ENV_17><PLACEHOLDER_figs/moe_specialization_end><PLACEHOLDER_sec/2_related_work_end>\n<PLACEHOLDER_sec/3_conclusion_begin>"
    },
    {
        "section": "7",
        "content": "\\section{\\edit{Discussion and }Limitations}\n\n\\cpar{\\edit{Scaling laws for multimodal data mixtures.}} Our scaling laws study\nspans different model configurations and training mixtures. While results\nsuggest that the scaling law coefficients remain largely consistent across\nmixtures, a broader exploration of mixture variations is needed to validate this\nobservation and establish a unified scaling law that accounts for this factor.\n\n\\cpar{\\edit{Scaling laws and performance on downstream tasks.}} Similar to\nprevious scaling law studies, our analysis focuses on pretraining performance as\nmeasured by the validation loss. However, the extent to which these findings\ntranslate to downstream performance remains an open question and requires\nfurther investigation.\n\n\\cpar{\\edit{Extrapolation to larger scales.}} The accuracy of scaling law\npredictions improves with increasing FLOPs~\\cref{app:scaling_laws}.\n\\edit{Furthermore, we validate our laws when extrapolating to larger model sizes\n(\\cref{sec:scaling_laws_evaluation}).} However, whether these laws can be reliably\nextrapolated to extremely large model sizes remains an open question.\n\n\\cpar{\\edit{High resolution and early-fusion models.}} Training early-fusion\nmodels with high-resolution inputs leads to a significant increase in vision\ntokens. While pooling techniques have been widely adopted for late-fusion\nmodels, alternative approaches may be necessary for early fusion. \\edit{Given\nthe similarity of early-fusion models to LLMs, it appears that techniques for\nextending context length could be beneficial.}\n\n\\cpar{\\edit{Scaling laws for multimodal MoEs models.}} For MoEs, we consider\nonly a single configuration (top-1 routing with 8 experts). \\edit{We found this\nconfiguration to work reasonably well in our setup, and follow a standard MoEs\nimplementation}. However, the findings may vary when optimizing \\edit{more} the\nMoE architecture or exploring different load-balancing, routing strategies\n\\edit{or different experts implementations}.",
        "trans_content": "\\section{\\edit{讨论与}局限性}\n\n\\cpar{\\edit{多模态数据混合的缩放规律。}} 我们的缩放规律研究涵盖了不同的模型配置和训练混合。尽管结果表明缩放规律系数在不同混合之间大致保持一致，但仍需要更广泛地探索混合变体，以验证这一观察结果并建立一个统一的缩放规律来考虑这一因素。\n\n\\cpar{\\edit{缩放规律与下游任务的表现。}} 类似于先前的缩放规律研究，我们的分析侧重于通过验证损失来衡量的预训练表现。然而，这些发现是否能够转化为下游任务的表现仍然是一个未解的问题，需要进一步的研究。\n\n\\cpar{\\edit{外推到更大规模。}} 随着FLOPs的增加，缩放规律预测的准确性得到了提升~\\cref{app:scaling_laws}。\\edit{此外，我们在外推到更大的模型尺寸时验证了我们的规律（\\cref{sec:scaling_laws_evaluation}）。} 然而，这些规律是否可以可靠地外推到极大的模型尺寸仍然是一个未解的问题。\n\n\\cpar{\\edit{高分辨率和早期融合模型。}} 使用高分辨率输入训练早期融合模型会显著增加视觉token的数量。虽然池化技术已经广泛应用于晚期融合模型，但对于早期融合，可能需要采用不同的方法。\\edit{鉴于早期融合模型与LLM的相似性，延长上下文长度的技术似乎是有益的。}\n\n\\cpar{\\edit{多模态MoEs模型的缩放规律。}} 对于MoEs，我们只考虑单一配置（使用8个专家的top-1路由）。\\edit{我们发现这种配置在我们的设置中效果较好，并遵循标准的MoEs实现。} 然而，研究结果可能会因优化\\edit{更多} MoE架构或探索不同的负载平衡、路由策略\\edit{或不同专家的实现}而有所不同。"
    },
    {
        "section": "8",
        "content": "\\section{Conclusion}\nWe explore various strategies for compute-optimal pretraining of native\nmultimodal models. We found the NMMs follow similar scaling laws to those of\nLLMs. Contrary to common belief, we find no inherent advantage in adopting\nlate-fusion architectures over early-fusion ones. While both architectures\nexhibit similar scaling properties, early-fusion models are more efficient to\ntrain and outperform late-fusion models at lower compute budgets. Furthermore,\nwe show that sparse architectures encourage modality-specific specialization,\nleading to performance improvements while maintaining the same inference cost.",
        "trans_content": "\\section{结论}\n我们探讨了原生多模态模型计算最优预训练的各种策略。我们发现，原生多模态模型遵循与大规模语言模型相似的扩展规律。与普遍观点相反，我们发现采用后融合架构并没有比早期融合架构具有固有的优势。尽管这两种架构表现出相似的扩展特性，但早期融合模型在训练上更加高效，并且在较低的计算预算下超越后融合模型。此外，我们展示了稀疏架构促进了模态特定的专业化，在保持相同推理成本的情况下，带来了性能的提升。"
    },
    {
        "section": "9",
        "content": "\\section*{\\edit{Acknowledgment}} We thank Philipp Dufter, Samira Abnar, Xiujun\nLi, Zhe Gan, Alexander Toshev, Yinfei Yang, Dan Busbridge, and Jason Ramapuram\nfor many fruitful discussions. We thank Denise Hui, and Samy Bengio for infra\nand compute support. Finally, we thank, Louis Béthune, Pierre Ablin, Marco\nCuturi, and the MLR team at Apple for their support throughout the project.<PLACEHOLDER_sec/3_conclusion_end>\n{\n    \\small\n    \\bibliographystyle{ieeenat_fullname}\n    \\bibliography{main}\n}\n\\clearpage\n\\appendix\n\\addcontentsline{toc}{section}{Appendix}\n<PLACEHOLDER_NEWCOMMAND_45>\n\\part{}\n\\parttoc\n\\clearpage\n<PLACEHOLDER_sec/X_suppl_begin>",
        "trans_content": "\\section*{\\edit{致谢}} 我们感谢 Philipp Dufter、Samira Abnar、Xiujun Li、Zhe Gan、Alexander Toshev、Yinfei Yang、Dan Busbridge 和 Jason Ramapuram 的许多富有成效的讨论。我们感谢 Denise Hui 和 Samy Bengio 提供的基础设施和计算支持。最后，我们感谢 Louis Béthune、Pierre Ablin、Marco Cuturi 以及 Apple 的 MLR 团队在整个项目中的支持。<PLACEHOLDER_sec/3_conclusion_end>\n{\n    \\small\n    \\bibliographystyle{ieeenat_fullname}\n    \\bibliography{main}\n}\n\\clearpage\n\\appendix\n\\addcontentsline{toc}{section}{附录}\n<PLACEHOLDER_NEWCOMMAND_45>\n\\part{}\n\\parttoc\n\\clearpage\n<PLACEHOLDER_sec/X_suppl_begin>"
    },
    {
        "section": "10",
        "content": "\\section{Experimental setup}\n\\label{app:implementation_details}\n\nIn \\Cref{tab:scaling_laws_hparams}, we show the pre-training hyperparameters for different model configurations used to derive the scaling laws. The number of parameters ranges from 275M to 3.7B, with model width increasing accordingly, while the depth remains fixed at 24 layers. Learning rates vary by model size, decreasing as the model scales up. Based on empirical experiments and estimates similar to \\citep{mckinzie2025mm1}, we found these values to be effective in our setup. Training is optimized using a fully decoupled AdamW optimizer with momentum values $\\beta_1=0.9$, $\\beta_2=0.95$, and a weight decay of $1\\text{e}{-4}$. The batch size is set to 2k samples, which account for 2M tokens, given a 1k context length.  Gradient clipping is set to 1.0, with a maximum warmup duration of 5k iterations, adjusted for shorter training runs: 1k and 2.5k warmup steps for models trained between 1k–4k and 5k–15k steps, respectively. For MoEs, we found that a longer warmup is significantly better, so we adopt a 2.5k warmup for all runs under 20k steps. We use a constant learning rate schedule with cooldown during the final 20\\% of training, gradually reducing to zero following an inverse square root schedule. For vision processing, image inputs are divided into $(14,14)$ patches, with augmentations including Random Resized Crop (resizing images to 224px with a scale range of [0.4, 1.0]) and Random Horizontal Flip with a probability of 0.5.  We train our models on mixture of interleaved, image captions and text only data \\Cref{tab:pretraining_datasets}.\nFor late fusion models, we found that using smaller learning rate for the vision encoder significantly boost the performance \\Cref{tab:late_scaler_scratch}, and when both the encoder and decoder are initialized (\\Cref{sec:app_init_early_late}) we found that freezing the vision encoder works best \\Cref{tab:late_scaler_init}.\n\n<PLACEHOLDER_ENV_18>\n\n<PLACEHOLDER_ENV_19>\n\n<PLACEHOLDER_ENV_20>\n\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_end>\n\n<PLACEHOLDER_ENV_22>",
        "trans_content": "\\section{实验设置}\n\\label{app:implementation_details}\n\n在\\Cref{tab:scaling_laws_hparams}中，我们展示了用于推导缩放规律的不同模型配置的预训练超参数。参数数量从275M到3.7B不等，随着模型宽度的增加，深度保持在24层不变。学习率根据模型的大小有所变化，随着模型的增大而减少。基于类似于\\citep{mckinzie2025mm1}的经验实验和估算，我们发现这些值在我们的设置中是有效的。训练使用完全解耦的AdamW优化器，动量值为$\\beta_1=0.9$，$\\beta_2=0.95$，权重衰减为$1\\text{e}{-4}$。批量大小设置为2k样本，考虑到1k的上下文长度，这相当于2M个token。梯度裁剪设置为1.0，最大预热持续时间为5k次迭代，针对较短的训练运行做了调整：对于在1k–4k和5k–15k步之间训练的模型，分别采用1k和2.5k的预热步数。对于MoEs，我们发现较长的预热显著提高了效果，因此我们为所有低于20k步的训练都采用了2.5k的预热。我们使用常数学习率调度，并在训练的最后20\\%进行降温，按反平方根调度逐步减小至零。对于视觉处理，图像输入被划分为$(14,14)$的patch，增强方式包括随机调整大小裁剪（将图像调整为224px，缩放范围为[0.4, 1.0]）和随机水平翻转，概率为0.5。我们在交替混合的图像字幕和文本数据集上训练我们的模型\\Cref{tab:pretraining_datasets}。\n\n对于晚期融合模型，我们发现对于视觉编码器使用较小的学习率显著提高了性能\\Cref{tab:late_scaler_scratch}，而当编码器和解码器都被初始化时（\\Cref{sec:app_init_early_late}），我们发现冻结视觉编码器效果最佳\\Cref{tab:late_scaler_init}。\n\n<PLACEHOLDER_ENV_18>\n\n<PLACEHOLDER_ENV_19>\n\n<PLACEHOLDER_ENV_20>\n\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_begin><PLACEHOLDER_ENV_21>\n<PLACEHOLDER_figs/late_vs_early_equal_tokens_end>\n\n<PLACEHOLDER_ENV_22>"
    },
    {
        "section": "11+11_1",
        "content": "\\section{Late vs early fusion}\n\\label{app:late_vs_early}\nThis section provides additional comparison between early and late fusion models.\n\n\n\\subsection{Scaling FLOPs} \\Cref{fig:early_vs_late_scaledata_main} compares early-fusion and late-fusion models when scaling FLOPs. Specifically, for each model size, we train multiple models using different amounts of training tokens. The performance gap between the two approaches mainly decreases due to increasing model sizes rather than increasing the number of training tokens. Despite the decreasing gap, across all the models that we train, early-fusion consistently outperform late-fusion.",
        "trans_content": "\\section{晚期融合与早期融合}\n\\label{app:late_vs_early}\n本节提供了早期融合和晚期融合模型的额外比较。\n\n\\subsection{FLOPs的规模} \\Cref{fig:early_vs_late_scaledata_main} 比较了在扩大FLOPs时，早期融合与晚期融合模型的表现。具体来说，对于每个模型大小，我们使用不同数量的训练标记训练多个模型。两种方法之间的性能差距主要是由于模型大小的增加，而不是训练标记数量的增加而减小的。尽管差距在减小，但在我们训练的所有模型中，早期融合始终优于晚期融合。"
    },
    {
        "section": "11_2",
        "content": "\\subsection{Changing the training data mixture} We analyze how the performance gap between early and late fusion models changes with variations in the training data mixture. As shown in \\Cref{fig:early_vs_late_textratio} and \\Cref{fig:early_vs_late_datatype_sameflops}, when fixing the model size, increasing the ratio of text and interleaved data favors early fusion. Interestingly, the gap remains largely unchanged for other data types. We also observe interference effects between different data types. Specifically, increasing the amount of interleaved data negatively impacts performance on image captions and vice versa. Additionally, increasing the proportion of text-only data slightly improves interleaved performance but increases loss on image captions. Overall, we find that text-only and interleaved data are correlated across different setups.\n\n<PLACEHOLDER_ENV_23>\n\n<PLACEHOLDER_figs/early_vs_late_imageres_begin><PLACEHOLDER_ENV_24><PLACEHOLDER_figs/early_vs_late_imageres_end>",
        "trans_content": "\\subsection{改变训练数据混合方式}  \n我们分析了随着训练数据混合方式的变化，早期融合模型与晚期融合模型之间的性能差距如何变化。如 \\Cref{fig:early_vs_late_textratio} 和 \\Cref{fig:early_vs_late_datatype_sameflops} 所示，在固定模型规模的情况下，增加文本与交错数据的比例有利于早期融合。有趣的是，对于其他类型的数据，这种差距基本保持不变。我们还观察到不同数据类型之间存在干扰效应。具体而言，增加交错数据的数量会对图像标题的性能产生负面影响，反之亦然。此外，增加纯文本数据的比例会略微提升交错数据的性能，但会加剧图像标题的损失。总体而言，我们发现纯文本数据与交错数据在不同配置下存在相关性。\n\n<PLACEHOLDER_ENV_23>\n\n<PLACEHOLDER_figs/early_vs_late_imageres_begin><PLACEHOLDER_ENV_24><PLACEHOLDER_figs/early_vs_late_imageres_end>"
    },
    {
        "section": "11_3",
        "content": "\\subsection{Scaling image resolution is in favor of early-fusion}\n\nWe examine how both\narchitectures perform with varying image resolution. We fix the number of model parameters to 1.63B and 1.75B for early and late fusion respecively. All models are trained for 100K steps or 200B tokens. Since the patch size remains\nconstant, increasing the resolution results in a higher number of visual tokens. For all resolutions, we maintain the same number of text tokens.\nAs shown in \\Cref{fig:early_vs_late_imageres}, the early-fusion model\nconsistently outperforms the late-fusion model across resolutions, particularly\nfor multimodal data, with the performance gap widening at higher resolutions.\nAdditionally, we observe that the loss on text and interleaved data increases as\nresolution increases.\n\n\\vspace{1cm}",
        "trans_content": "\\subsection{图像分辨率的缩放有利于早期融合}\n\n我们研究了在不同图像分辨率下两种架构的表现。我们将早期和晚期融合的模型参数分别固定为1.63B和1.75B。所有模型训练了100K步或200B个token。由于补丁大小保持不变，分辨率的增加会导致更多的视觉token。对于所有分辨率，我们保持相同数量的文本token。正如\\Cref{fig:early_vs_late_imageres}所示，早期融合模型在所有分辨率下始终优于晚期融合模型，特别是在多模态数据方面，且随着分辨率的提高，性能差距进一步扩大。此外，我们观察到，随着分辨率的增加，文本和交错数据的损失也会增加。\n\n\\vspace{1cm}"
    },
    {
        "section": "11_4",
        "content": "\\subsection{Early-fusion is consistently better when matching the late-fusion model size}\n<PLACEHOLDER_figs/early_vs_late_datatype_isoparams_begin>\n<PLACEHOLDER_ENV_25><PLACEHOLDER_figs/early_vs_late_datatype_isoparams_end>\n\nIn this section, we compare the late-fusion model with different configurations\nof early-fusion one. Specifically, we train early-fusion models that match the\nlate-fusion model in total parameters (Params), text model size (Text), and\nFLOPs (FLOPs), assuming 45-45-10 training mixture. As shown in\n\\Cref{fig:early_vs_late_datatype_isoparams}, early fusion consistently\noutperforms late fusion when normalized by total parameters, followed by\nnormalization by FLOPs. When matching the text model size, early fusion performs\nbetter at higher ratios of interleaved data.",
        "trans_content": "\\subsection{在匹配后融合模型大小时，早融合始终表现更优}\n<PLACEHOLDER_figs/early_vs_late_datatype_isoparams_begin>\n<PLACEHOLDER_ENV_25><PLACEHOLDER_figs/early_vs_late_datatype_isoparams_end>\n\n本节中，我们将具有不同配置的早融合模型与后融合模型进行比较。具体而言，我们训练了在总参数量（Params）、文本模型大小（Text）和计算量（FLOPs）上与后融合模型相匹配的早融合模型，假设采用 45-45-10 的训练混合比例。如 \\Cref{fig:early_vs_late_datatype_isoparams} 所示，当以总参数量归一化时，早融合始终优于后融合，其次是在计算量归一化的情况下。当匹配文本模型大小时，在较高比例的交错数据下，早融合的性能更佳。"
    },
    {
        "section": "11_5",
        "content": "\\subsection{Different late-fusion configuration}\nWe examine how this scaling changes with different late-fusion configurations. Instead of scaling both the vision and text models equally, as done in the main paper, we fix the vision encoder size to 300M and scale only the text model. \\Cref{fig:early_vs_late_scalellmdata_dclm} shows that late-fusion models lag behind at smaller model sizes, with the gap closing significantly as the text model scales. This suggests that allocating more parameters to shared components is more beneficial, further supporting the choice of early-fusion models.\n\n<PLACEHOLDER_ENV_26>",
        "trans_content": "\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>\\subsection{不同的后期融合配置}\n我们研究在不同的后期融合配置下，这种扩展方式如何变化。与主文中同时等比例扩展视觉模型和文本模型不同，我们固定视觉编码器的规模为 300M，仅扩展文本模型。 \\Cref{fig:early_vs_late_scalellmdata_dclm} 显示，当模型规模较小时，后期融合模型的表现落后，但随着文本模型的扩展，这一差距显著缩小。这表明，将更多参数分配给共享组件更具优势，进一步支持了选择早期融合模型的理由。\n\n<PLACEHOLDER_ENV_26>"
    },
    {
        "section": "11_6",
        "content": "\\subsection{Initializing from LLM and CLIP}\n\\label{sec:app_init_early_late}\n\nWe study the case where both late and early fusion models are initialized from pre-trained models, specifically DCLM-1B \\citep{li2024datacomp} and CLIP-ViT-L \\citep{radford2021learning} for late fusion. Interestingly, \\Cref{fig:early_vs_late_init_scaledata} shows that for text and interleaved multimodal documents, early fusion can match the performance of late fusion when trained for longer. However, closing the gap on image caption data remains more challenging. Notably, when considering the overall training cost, including that of pre-trained models, early fusion requires significantly longer training to compensate for the vision encoder’s pretraining cost.\n\n<PLACEHOLDER_figs/early_vs_late_init_scaledata_begin>\n<PLACEHOLDER_ENV_27><PLACEHOLDER_figs/early_vs_late_init_scaledata_end>",
        "trans_content": "\\subsection{从 LLM 和 CLIP 初始化}\n\\label{sec:app_init_early_late}\n\n我们研究了晚期融合和早期融合模型均从预训练模型初始化的情形，具体来说，晚期融合采用了 DCLM-1B \\citep{li2024datacomp} 和 CLIP-ViT-L \\citep{radford2021learning} 进行初始化。有趣的是，\\Cref{fig:early_vs_late_init_scaledata} 表明，对于文本和交错的多模态文档，经过较长时间训练后，早期融合可以达到与晚期融合相当的性能。然而，在图像字幕数据上缩小差距仍然更具挑战性。值得注意的是，在考虑包括预训练模型在内的整体训练成本时，早期融合需要显著更长的训练时间，以补偿视觉编码器的预训练开销。\n\n<PLACEHOLDER_figs/early_vs_late_init_scaledata_begin>\n<PLACEHOLDER_ENV_27><PLACEHOLDER_figs/early_vs_late_init_scaledata_end>"
    },
    {
        "section": "12+12_1",
        "content": "\\section{Scaling laws}\n\\label{app:scaling_laws}\n\n\n\\subsection{Fitting \\(L = F(N, D)\\)}\nFollowing \\citep{hoffmann2022training}, we determine the parameters that minimize the following objective across all our runs \\(i\\):\n<PLACEHOLDER_ENV_28>\nWe perform this optimization across various initialization ranges and select the parameters that achieve the lowest loss across all initializations. Specifically, our grid search spans \\(\\{0, 0.5, 2.5\\}\\) for \\(\\alpha\\) and \\(\\beta\\), \\(\\{0, 5, 10, ..., 30\\}\\) for \\(a\\) and \\(b\\), and \\(\\{-1, -0.5, 1, 0.5\\}\\) for \\(e\\). We use the L-BFGS algorithm with \\(\\delta=1e-3\\).",
        "trans_content": "\\section{尺度定律}\n\\label{app:scaling_laws}\n\n\\subsection{拟合 \\(L = F(N, D)\\)}\n根据 \\citep{hoffmann2022training}，我们确定最小化以下目标函数的参数，该目标函数在所有运行 \\(i\\) 上进行最小化：\n<PLACEHOLDER_ENV_28>\n我们在不同的初始化范围内执行此优化，并选择在所有初始化中实现最低损失的参数。具体来说，我们的网格搜索涵盖了 \\(\\{0, 0.5, 2.5\\}\\) 的 \\(\\alpha\\) 和 \\(\\beta\\)，\\(\\{0, 5, 10, ..., 30\\}\\) 的 \\(a\\) 和 \\(b\\)，以及 \\(\\{-1, -0.5, 1, 0.5\\}\\) 的 \\(e\\)。我们使用 L-BFGS 算法，设置 \\(\\delta=1e-3\\)。"
    },
    {
        "section": "12_2",
        "content": "\\subsection{Fitting \\(N \\propto C^a\\), \\(D \\propto C^b\\) and \\(D \\propto N^d\\)}\nWhile these equations have a closed-form solution \\citep{hoffmann2022training} for early-fusion models that can be derived from \\Cref{eq:scaling_laws}, this is not the case for late-fusion models without specifying either the vision encoder or text model size. To ensure a fair comparison, we derive these equations for both models, by performing linear regression in log space. We found that the regression is very close to the coefficient found with closed-form derivation \\Cref{tab:scaling_laws_closed_form}. For instance, to derive \\(N = K_aC^a\\), given a FLOP budget \\(C\\) and a set of linearly spaced tokens \\(D_i\\) ranging from 10B to 600B, we compute the model size for each \\(D_i\\) as \\(N_i = \\frac{C}{6D}\\) for early fusion and \\(N_i = \\frac{C}{6D}+0.483*N_v\\) for late fusion (for the 45-45-10 mixture, \\(D_v=0.544D\\), thus $C=6D(0.544N_v+N_t)$). We then apply \\Cref{eq:scaling_laws} to obtain the loss for each model size and select \\(N\\) that has the minimum loss. We repeat this for all FLOP values corresponding to our runs, resulting in a set of points \\((C, N_{opt})\\) that we use to regress \\(a\\) and \\(K_a\\).  We follow a similar procedure to find \\(b\\) and \\(d\\). For late-fusion models, we regress a linear model to determine \\(N_v\\) given \\(N\\). Notably, even though we maintain a fixed width ratio for late-fusion models, this approach is more accurate, as embedding layers prevent a strictly fixed ratio between text and vision model sizes. We present the regression results in \\Cref{fig:scaling_laws_closed_form_early_late}.\n\n<PLACEHOLDER_ENV_29>",
        "trans_content": "\\subsection{拟合 \\(N \\propto C^a\\), \\(D \\propto C^b\\) 和 \\(D \\propto N^d\\)}\n尽管这些方程对早期融合模型有封闭解 \\citep{hoffmann2022training}，该解可以从 \\Cref{eq:scaling_laws} 推导出，但对于晚期融合模型，若未指定视觉编码器或文本模型的大小，则无法得到封闭解。为了确保公平比较，我们通过在对数空间中执行线性回归，推导出这两个模型的方程。我们发现回归结果与通过封闭形式推导得到的系数非常接近 \\Cref{tab:scaling_laws_closed_form}。例如，为了推导 \\(N = K_aC^a\\)，给定一个 FLOP 预算 \\(C\\) 和一组线性间隔的令牌 \\(D_i\\)，范围从 10B 到 600B，我们为每个 \\(D_i\\) 计算模型大小 \\(N_i = \\frac{C}{6D}\\) 用于早期融合，而对于晚期融合，计算式为 \\(N_i = \\frac{C}{6D}+0.483*N_v\\)（对于 45-45-10 混合，\\(D_v=0.544D\\)，因此 \\(C=6D(0.544N_v+N_t)\\)）。然后我们应用 \\Cref{eq:scaling_laws} 来获得每个模型大小的损失，并选择具有最小损失的 \\(N\\)。我们对所有与我们的实验相关的 FLOP 值重复此过程，得到一组点 \\((C, N_{opt})\\)，用于回归 \\(a\\) 和 \\(K_a\\)。我们采用类似的程序来找到 \\(b\\) 和 \\(d\\)。对于晚期融合模型，我们回归一个线性模型来确定给定 \\(N\\) 的 \\(N_v\\)。值得注意的是，尽管我们对晚期融合模型保持固定的宽度比，但这种方法更为准确，因为嵌入层防止了文本和视觉模型大小之间严格固定的比率。我们在 \\Cref{fig:scaling_laws_closed_form_early_late} 中展示了回归结果。\n\n<PLACEHOLDER_ENV_29>"
    },
    {
        "section": "12_3",
        "content": "\\subsection{Fitting \\(L \\propto C^c\\)}\nTo determine the relationship between the final model loss and the compute budget \\(C\\), we begin by interpolating the points corresponding to the same model size and compute the convex hull that covers the minimum loss achieved by all runs for each FLOP. This results in a continuous mapping from the FLOPs to the lowest loss. We consider a range of FLOPs, excluding very small values ($\\leq 3e^{19}$), and construct a dataset of \\((C, L)\\) for linearly spaced compute \\(C\\). Using this data, we find the linear relationship between \\(L\\) and \\(C\\) in the log space and deduce the exponent \\(c\\). We visualize the results in \\Cref{fig:scaling_laws_early_late_moe}.\n\n<PLACEHOLDER_ENV_30>\n\n<PLACEHOLDER_ENV_31>",
        "trans_content": "\\subsection{拟合 \\(L \\propto C^c\\)}\n\n为了确定最终模型损失与计算预算 \\(C\\) 之间的关系，我们首先对对应于相同模型规模的点进行插值，并计算包含所有运行在每个 FLOP 下所达到的最小损失的凸包。这产生了一个从 FLOPs 到最低损失的连续映射。我们考虑一系列 FLOPs，排除非常小的值（$\\leq 3e^{19}$），并构建一个关于线性间隔的计算量 \\(C\\) 的 \\((C, L)\\) 数据集。利用这些数据，我们在对数空间中找到 \\(L\\) 与 \\(C\\) 之间的线性关系，并推导出指数 \\(c\\)。我们在 \\Cref{fig:scaling_laws_early_late_moe} 中对结果进行了可视化。\n\n<PLACEHOLDER_ENV_30>\n\n<PLACEHOLDER_ENV_31>"
    },
    {
        "section": "12_4",
        "content": "\\subsection{Scaling laws for different target data type}\nIn \\Cref{fig:scaling_laws_early_late_moe_getty_obelics_dclm}, we derive the scaling laws for different target data types. In general, we observe that the model learns image captioning faster than interleaved data, as indicated by the higher absolute value of the scaling exponent (e.g., 0.062 vs 0.046), despite using the same data ratio for captioning and interleaved data (45\\% each). Additionally, we find that the model learns more slowly on text-only data, likely due to the smaller amount of text-only data (10\\%). Across model configurations, we find that early fusion scales similarly to late fusion on image captioning but has a lower multiplicative constant (49.99 vs 47.97). For MoEs, the model learns faster but exhibits a higher multiplicative constant. On text and interleaved data, early and late fusion models scale similarly and achieve comparable performance. However, MoEs demonstrate better overall performance while learning slightly more slowly.",
        "trans_content": "\\subsection{不同目标数据类型的缩放规律}\n在\\Cref{fig:scaling_laws_early_late_moe_getty_obelics_dclm}中，我们推导了不同目标数据类型的缩放规律。通常，我们观察到模型在图像标注任务上的学习速度快于交错数据任务，如通过较高的缩放指数绝对值所示（例如，0.062与0.046），尽管在标注和交错数据上使用了相同的数据比例（各占45\\%）。此外，我们发现模型在仅文本数据上的学习速度较慢，这可能是由于仅文本数据量较少（10\\%）。在不同的模型配置下，我们发现早期融合在图像标注任务上与后期融合的缩放规律相似，但其乘法常数较低（49.99与47.97）。对于MoE模型，尽管学习速度较快，但呈现出较高的乘法常数。在文本和交错数据上，早期融合和后期融合模型的缩放规律相似，且取得了相似的性能。然而，MoE模型展现了更好的整体性能，尽管学习速度略慢。"
    },
    {
        "section": "12_5",
        "content": "\\subsection{Scaling laws for different training mixtures}\n\nWe investigate how the scaling laws change when modifying the training mixtures. Specifically, we vary the ratio of image caption, interleaved, and text-only data and report the results in \\Cref{fig:app_early_scaleflops_data_mixtures}. Overall, we observe similar scaling trends, with only minor changes in the scaling coefficients. Upon closer analysis, we find that increasing the ratio of a particular data type in the training mixture, leads to a corresponding increase in its scaling exponent. For instance, increasing the ratio of image captions from 30\\% to 40\\% raises the absolute value of the exponent from 0.056 to 0.061. However, for text-only data, we do not observe significant changes in the scaling coefficients when varying its proportion in the training mixture.\n\n<PLACEHOLDER_ENV_32>",
        "trans_content": "\\subsection{不同训练混合的缩放规律}\n\n我们研究了在修改训练混合时，缩放规律如何变化。具体来说，我们改变了图像描述、交替混合和仅文本数据的比例，并在\\Cref{fig:app_early_scaleflops_data_mixtures}中报告了结果。总体来看，我们观察到相似的缩放趋势，只有缩放系数发生了轻微变化。通过进一步分析，我们发现，当训练混合中某种数据类型的比例增加时，其缩放指数也相应增加。例如，将图像描述的比例从30\\%提高到40\\%，其指数的绝对值从0.056增加到0.061。然而，对于仅文本数据，当其在训练混合中的比例变化时，我们并未观察到缩放系数的显著变化。\n\n<PLACEHOLDER_ENV_32>"
    },
    {
        "section": "12_6",
        "content": "\\subsection{Scaling laws evaluation and sensitivity}\n\nFor each model size and number of training tokens, we compute the loss based on the estimated functional form in \\Cref{eq:scaling_laws} and compare it with the actual loss achieved by our runs. We visualize these points in \\Cref{fig:observed_vs_predicted_loss}, demonstrating that our estimation is highly accurate, particularly for lower loss values, and hence for larger FLOPs. Additionally, we perform a sensitivity analysis using bootstrapping. Specifically, we sample with replacement \\( P \\) points (\\( P \\) being equal to the total number of trained models) and re-estimate the scaling law coefficients. This process is repeated 100 times, and we report the average and standard deviation of each coefficient. \\Cref{tab:scaling_laws_sensitivity} shows that our estimation is more precise for \\(\\beta\\) compared to \\(\\alpha\\), primarily due to the smaller number of model sizes relative to the number of different token counts used to derive the scaling laws.\n\n<PLACEHOLDER_ENV_33>",
        "trans_content": "\\subsection{缩放规律评估与敏感性分析}\n\n对于每个模型大小和训练令牌数，我们根据\\Cref{eq:scaling_laws}中估计的函数形式计算损失，并将其与我们实验中实际获得的损失进行比较。我们在\\Cref{fig:observed_vs_predicted_loss}中可视化这些点，展示了我们的估计非常准确，特别是在较低损失值下，因此在更大的FLOPs下尤为如此。此外，我们还通过自助法进行了敏感性分析。具体而言，我们进行有放回抽样，抽取\\( P \\)个点（\\( P \\)等于训练的模型总数），并重新估计缩放规律系数。这个过程重复进行100次，我们报告每个系数的平均值和标准差。\\Cref{tab:scaling_laws_sensitivity}显示，相较于\\(\\alpha\\)，我们对\\(\\beta\\)的估计更为精确，这主要是因为相对于用以推导缩放规律的不同令牌数，模型大小的数量较少。\n\n<PLACEHOLDER_ENV_33>"
    },
    {
        "section": "12_7",
        "content": "\\subsection{\\edit{Scaling laws for sparse NMMs.}}\n\\label{app:scaling_laws_moes}\n\nSimilar to dense models, we fit a parametric loss function (\\Cref{eq:scaling_laws}) to predict the loss of sparse NMMs based on the number of parameters and training tokens, replacing the total parameter count with the number of active parameters. While incorporating sparsity is standard when deriving scaling laws for MoEs \\citep{wangscalingmoe,krajewski2024scalingmoe,abnar2025parameters}, we focus on deriving scaling laws specific to the sparsity level used in our MoE setup. This yields coefficients that are implicitly conditioned on the sparsity configuration.\n\nWe also experiment with a sparsity-aware formulation of the scaling law as proposed in \\citep{abnar2025parameters}, and observe consistent trends (\\Cref{tab:moes_coeffs}). In particular, the exponents associated with model size ($N$) are substantially larger than those for training tokens ($\\beta$), reinforcing the importance of scaling model size in sparse architectures. Additionally, we observe that the terms governing the scaling of active parameters decompose into two components.\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_moes_begin><PLACEHOLDER_ENV_34><PLACEHOLDER_tables/scaling_laws_coeffs_moes_end>",
        "trans_content": "\\subsection{\\edit{稀疏NMMs的缩放规律。}}\n\\label{app:scaling_laws_moes}\n\n与密集模型类似，我们拟合了一个参数化的损失函数（\\Cref{eq:scaling_laws}），以根据参数数量和训练标记预测稀疏NMMs的损失，将总参数数量替换为活跃参数的数量。在推导MoEs的缩放规律时，通常会纳入稀疏性\\citep{wangscalingmoe,krajewski2024scalingmoe,abnar2025parameters}，但我们专注于推导特定于我们MoE设置中使用的稀疏性水平的缩放规律。这产生的系数隐式地依赖于稀疏配置。\n\n我们还实验了\\citep{abnar2025parameters}提出的基于稀疏性的缩放规律公式，并观察到一致的趋势（\\Cref{tab:moes_coeffs}）。特别是，与训练标记（$\\beta$）相关的模型大小（$N$）的指数显著大于训练标记，强调了在稀疏架构中扩大模型规模的重要性。此外，我们还观察到，控制活跃参数缩放的项分解为两个组成部分。\n\n<PLACEHOLDER_tables/scaling_laws_coeffs_moes_begin><PLACEHOLDER_ENV_34><PLACEHOLDER_tables/scaling_laws_coeffs_moes_end>"
    },
    {
        "section": "13",
        "content": "\\section{Mixture of experts and modality-specific specialization}\n\\label{app:moes}\n\nWe investigate multimodal specialization in MoE architectures. We compute a\nspecialization score as the average difference between the number of text/images\ntokens assigned to each expert and a uniform assignment ($1/E$). Additionally,\nwe visualize the normalized number of text and image tokens assigned to each\nexpert across layers. \\Cref{fig:app_moes_specialization} shows clear modality-specific\nexperts, particularly in the early layers. Furthermore, the specialization score\ndecreases as the number of layers increases but rises again in the very last\nlayers. This suggests that early and final layers require more modality\nspecialization compared to mid-layers. Additionally, we observe several experts\nshared between text and image modalities, a phenomenon not present in\nhard-routed or predefined modality-specific experts.\n\n<PLACEHOLDER_ENV_35>\n\n<PLACEHOLDER_ENV_36>\n\n<PLACEHOLDER_ENV_37><PLACEHOLDER_sec/X_suppl_end>\n\n\\end{document}",
        "trans_content": "\\section{专家混合模型与模态特定的专门化}\n\\label{app:moes}\n\n我们研究了MoE架构中的多模态专门化。我们计算了一个专门化得分，该得分为分配给每个专家的文本/图像标记数与均匀分配（$1/E$）之间的平均差异。此外，我们可视化了每个专家在各层之间分配的标准化文本和图像标记数。\\Cref{fig:app_moes_specialization} 显示了明显的模态特定专家，尤其是在早期层。进一步地，专门化得分随着层数的增加而减小，但在最后几层再次上升。这表明，与中间层相比，早期和最后几层需要更多的模态专门化。此外，我们观察到几个文本和图像模态共享的专家，这在硬路由或预定义的模态特定专家中并不存在。\n\n<PLACEHOLDER_ENV_35>\n\n<PLACEHOLDER_ENV_36>\n\n<PLACEHOLDER_ENV_37><PLACEHOLDER_sec/X_suppl_end>\n\n\\end{document}"
    }
]