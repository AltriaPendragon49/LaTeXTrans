[
    {
        "placeholder": "<PLACEHOLDER_CAP_1>",
        "cap_type": "title",
        "content": "\\title{Scaling Laws for Native Multimodal Models}",
        "trans_content": "\\title{本地多模态模型的扩展规律}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_2>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling properties of Native Multimodal Models.} Based on\n    the scaling laws study in \\cref{sec:scaling_laws_early}, we observe: (1)\n    early and late fusion models provide on par validation loss $L$ when trained\n    using the same compute budget $C$ (in FLOPs); (2) This performance is\n    achieved via a different trade-off between parameters $N$ and number of\n    training tokens $D$, where early-fusion models requires fewer parameters.\n    \\edit{; (3) Sparse early-fusion models achieve lower loss and require more\n    training tokens for a given FLOP budget.}\n    }",
        "trans_content": "\\caption{\\textbf{原生多模态模型的可扩展性特性。} 基于 \\cref{sec:scaling_laws_early} 中的可扩展性规律研究，我们观察到： (1) 在使用相同计算预算 $C$（以 FLOPs 计）进行训练时，早期融合模型和后期融合模型在验证损失 $L$ 上表现相当； (2) 该性能是通过参数量 $N$ 与训练标记数 $D$ 之间的不同权衡实现的，其中早期融合模型所需参数更少； \\edit{；(3) 稀疏的早期融合模型在给定的 FLOP 预算下可获得更低的损失，但需要更多的训练标记。} }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_3>",
        "cap_type": "caption",
        "content": "\\caption{Definitions of the expressions used throughout the paper.}",
        "trans_content": "\\caption{本文中使用的表达式的定义。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_4>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for \\fbox{\\colorbox{CustomC_Light1!20}{\\strut\n    early-fusion}} and \\fbox{\\colorbox{CustomD_Light1!20}{late-fusion\\strut}}\n    native multimodal models.} Each point represents a model (300M to 3B\n    parameters) trained on varying \\edit{number of} tokens (250M to 400B). We\n    report the average cross-entropy loss on the validation sets of\n    \\edit{interleaved (Obelics), Image-caption (HQITP), and text-only data\n    (DCLM).}}",
        "trans_content": "\\caption{\\textbf{早期融合和 \\fbox{\\colorbox{CustomC_Light1!20}{\\strut\n    早期融合}} 和 \\fbox{\\colorbox{CustomD_Light1!20}{晚期融合\\strut}}\n    本地多模态模型的规模规律。} 每个点代表一个模型（300M 到 3B\n    参数），在不同的 \\edit{标记数量}（250M 到 400B）上训练。我们\n    报告在 \\edit{交替（Obelics）、图像-标题（HQITP）和仅文本数据\n    （DCLM）} 上的平均交叉熵损失。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_5>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Pre-training data mixture.} Unless otherwise\n    specified, the training mixture contains 45\\%, 45\\% and 10\\% of image\n    captions, interleaved documents and text-only data.}",
        "trans_content": "\\caption{\\textbf{预训练数据混合。} 除非另有说明，训练混合数据包含45\\%、45\\%和10\\%的图像说明、交错文档和仅文本数据。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_6>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for native multimodal models}. We report the\n        scaling laws results for early and late fusion models. We fit the scaling laws for different target data types as well as their average loss (AVG).\n        }",
        "trans_content": "\\caption{\\textbf{原生多模态模型的扩展规律}. 我们报告了早期和晚期融合模型的扩展规律结果。我们为不同的目标数据类型及其平均损失（AVG）拟合了扩展规律。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_7>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: scaling training FLOPs.} We compare\n    early and late fusion models when scaling both the number of model parameters and the number\n    of training tokens. Overall, early fusion shows a slight advantage, especially at smaller model sizes, and the gap decreases when scaling the number of parameters $N$.}",
        "trans_content": "\\caption{\\textbf{早期融合与晚期融合：扩展训练FLOPs。} 我们比较了在扩展模型参数数量和训练标记数量时的早期融合和晚期融合模型。总体而言，早期融合显示出轻微的优势，尤其是在较小的模型尺寸下，当参数数量 $N$ 扩展时，差距逐渐缩小。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_8>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws with different training mixtures.}\n    Early-fusion models follow similar scaling trends when changing the pretraining mixtures. However, increasing the image captions leads to a higher scaling exponent norm (see~\\cref{tab:scaling_laws_coeffs_data_mixtures}).\n    }",
        "trans_content": "\\caption{\\textbf{不同训练混合比例下的缩放规律。}\n    当改变预训练混合比例时，early-fusion 模型遵循类似的缩放趋势。然而，增加图像字幕比例会导致更高的缩放指数范数（参见~\\cref{tab:scaling_laws_coeffs_data_mixtures}）。\n    }"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_9>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late: pretraining efficiency.} Early-fusion is faster to train and consumes less memory. Models are trained on 16 H100 GPUs for 160k steps (300B tokens).}",
        "trans_content": "\\caption{\\textbf{早期融合 vs 晚期融合：预训练效率。}早期融合训练速度更快，占用内存更少。模型在 16 块 H100 GPU 上训练了 160k 步（300B 个标记）。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_10>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: changing the training mixture.} We vary the training mixtures and plot the final training loss. Early fusion models attain a favorable performance when increasing the proportion of interleaved documents and text-only data.}",
        "trans_content": "\\caption{\\textbf{早期融合与晚期融合：改变训练混合。} 我们改变训练混合并绘制最终训练损失。当增加交错文档和仅文本数据的比例时，早期融合模型表现出较好的性能。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_11>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for different training mixtures}. Early-fusion models. C-I-T refer to image-caption, interleaved and text}",
        "trans_content": "\\caption{\\textbf{不同训练混合物的规模法则}. 早期融合模型. C-I-T 指图像-标题、交替和文本}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_12>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Observed vs predicted loss.} We visualize the loss predicted by our scaling laws \\cref{eq:scaling_laws} and the actual loss achieved by each run. We can reliably predict the performance of models larger (8B params) than those used to fit the scaling laws.}",
        "trans_content": "\\caption{\\textbf{观测损失与预测损失对比。} 我们可视化了由我们的缩放定律 \\cref{eq:scaling_laws} 所预测的损失，以及每次运行实际达到的损失。我们能够可靠地预测比用于拟合缩放定律更大的模型（8B 参数）的性能。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_13>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early native vs initializing from LLMs: initializing from\n    pre-trained models and scaling training tokens.} We compare training with and\n    without initializing from DCLM-1B.}",
        "trans_content": "\\caption{\\textbf{早期原生与从LLMs初始化：从预训练模型初始化和扩展训练标记。} 我们比较了使用和不使用DCLM-1B初始化的训练。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_14>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{MoE vs Dense: scaling training FLOPs.} We compare MoE and\n    dense early-fusion models when scaling both the amount of training tokens\n    and model sizes. MoEs beat dense models when matching the\n    number of active parameters.}",
        "trans_content": "\\caption{\\textbf{MoE 与 Dense：扩展训练 FLOP。} 我们比较了 MoE 和密集型早期融合模型，在扩展训练令牌数量和模型规模时的表现。当激活参数数量相同时，MoE 优于密集型模型。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_15>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for sparse early-fusion NMMs.} We report the final validation loss averaged across interleaved,\n    image-captions and text data.}",
        "trans_content": "\\caption{\\textbf{稀疏早期融合 NMM 的缩放法则。}我们报告了在图像标题与文本数据交错情况下，最终验证损失的平均值。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_16>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Supervised finetuning on the LLaVA mixture.} All models are native at 1.5B scale and pre-trained on 300B tokens.}",
        "trans_content": "\\caption{\\textbf{在 LLaVA 混合体上的监督微调。} 所有模型都是原生 1.5B 规模，并且在 300B 令牌上进行了预训练。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_17>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Modality-aware vs modality agnostic routing for sparse NMMs.} We compare\n    modality-agnostic routing with modality-aware routing\n    when scaling both the amount of training tokens and model sizes.}",
        "trans_content": "\\caption{\\textbf{模态感知与模态无关路由在稀疏NMM中的比较。} 我们比较了模态无关路由和模态感知路由在训练标记数量和模型规模增加时的表现。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_18>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{MoE specialization.} Entropy-based image/text specialization (see~\\cref{sec:specialization}) across layers for two data sources: HQITP and Obelics.\n\\edit{Both sources exhibit a similar trend: the score decreases in the early layers\nbefore increasing again in the final layers.}}",
        "trans_content": "\\caption{\\textbf{MoE 专门化.} 基于熵的图像/文本专门化（见~\\cref{sec:specialization}）在两个数据源：HQITP 和 Obelics 的各层之间的变化。\n\\edit{两个数据源都表现出类似的趋势：在早期层中得分下降，随后在最后几层中再次上升.}}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_19>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{MoE specialization frequency.}\n    Percentage of text and image tokens routed to each expert on interleaved\n    data from Obelics. Experts are ordered for better visualization. The first\n    layer shows the highest amount of unimodal experts.}",
        "trans_content": "\\caption{\\textbf{MoE 专门化频率.}  \n    从 Obelics 的交错数据中，文本和图像 token 被路由到每个专家的百分比。为了更好的可视化，专家按顺序排列。第一层显示的是最多的单模态专家.}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_20>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Pre-training hyperparameters} We detail the hyperaparmeters used for pre-training different model configurations to derive scaling laws.}",
        "trans_content": "\\caption{\\textbf{预训练超参数} 我们详细说明了用于预训练不同模型配置以推导规模规律的超参数。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_21>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Vision encoder scaler.} Freezing the vision encoder works best when initializing late-fusion models with pre-trained models.}",
        "trans_content": "\\caption{\\textbf{视觉编码器缩放器。} 在使用预训练模型初始化后融合模型时，冻结视觉编码器效果最佳。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_22>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Vision encoder scaler.} Reducing the learning rate for the vision encoder is better when training late-fusion models from scratch.}",
        "trans_content": "\\caption{\\textbf{视觉编码器缩放器。} 当从头开始训练后融合模型时，降低视觉编码器的学习率效果更佳。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_23>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: scaling training FLOPs.} We compare\n    early and late fusion models when scaling both the model size and the number\n    of training tokens. The gap decreases mainly due to scaling models size.}",
        "trans_content": "\\caption{\\textbf{早期融合与后期融合：训练 FLOPs 的扩展。} 我们在同时扩展模型规模和训练 token 数量时，比较了早期融合模型与后期融合模型的表现。该差距主要由于模型规模的扩展而减小。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_24>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: changing the training mixture.} We vary the training mixtures and plot the final training loss. Early fusion models become better when increasing the proportion of interleaved documents. Early and late fusion has 1.63B and 1.75B parameters respectively.}",
        "trans_content": "\\caption{\\textbf{早期与晚期融合：改变训练混合。} 我们变化训练混合并绘制最终训练损失图。随着交织文档比例的增加，早期融合模型表现更好。早期和晚期融合分别具有1.63B和1.75B个参数。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_25>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: changing the amount of text-only data in the training mixture (isoFLOPs).} We vary the ratio of text-only data and plot the final training loss. The gap increases with the text data ratio in favor of early fusion model. Early fusion has 1.63B parameters and late fusion 1.75B parameters.}",
        "trans_content": "\\caption{\\textbf{早期融合 vs 晚期融合：改变训练混合数据中仅文本数据的比例（isoFLOPs）。} 我们改变仅文本数据的比例，并绘制最终训练损失。随着文本数据比例的增加，早期融合模型的优势差距也随之扩大。早期融合模型具有 1.63B 参数，而晚期融合模型具有 1.75B 参数。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_26>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: training with different image\n    resolutions (isoFLOPs).} For the same training FLOPs we vary the image\n    resolution (and thus the number of image tokens) during training and report\n    the final training loss. Increasing resolution, hurts the performance on\n    text and interleaved documents, while helping image captioning. The gap\n    stays almost the same on text and interleaved data while slightly increase\n    on image captioning in favor of early fusion.}",
        "trans_content": "\\caption{\\textbf{早期融合与晚期融合：使用不同图像分辨率进行训练（isoFLOPs）。} 对于相同的训练FLOPs，我们在训练过程中改变图像分辨率（因此改变图像标记的数量），并报告最终的训练损失。提高分辨率会对文本和交错文档的性能产生负面影响，同时有助于图像标注。在文本和交错数据上，性能差距几乎保持不变，而在图像标注上，早期融合略微占优，差距略有增加。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_27>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: changing the training mixture and early-fusion configuration.} We\n    vary the training mixtures and plot the final training loss for different\n    configuration of early fusion models. For the\n    same number of total parameters early fusion consistently outperform late\n    fusion.}",
        "trans_content": "\\caption{\\textbf{早期融合 vs 晚期融合：更改训练混合比例与早期融合配置。}我们变更训练混合比例，并绘制不同早期融合模型配置下的最终训练损失。对于相同的总参数数量，早期融合始终优于晚期融合。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_28>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion: scaling training FLOPs while fixing the vision encoder size.} We compare early and late fusion models when scaling both the amount of training tokens and model sizes. For late fusion mdoels, we fix the vision encoder size (300M) and scale the text model (250M, 834M, 2B, 3B). The gap between early and late get tighter when scaling the text model.}",
        "trans_content": "\\caption{\\textbf{早期融合 vs 晚期融合：在固定视觉编码器大小的同时扩展训练 FLOPs。}我们在扩展训练标记数量和模型规模的同时，比较了早期融合模型和晚期融合模型。对于晚期融合模型，我们固定视觉编码器的大小（300M），并扩展文本模型（250M、834M、2B、3B）。随着文本模型规模的扩展，早期融合与晚期融合之间的差距逐渐缩小。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_29>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Early vs late fusion when initializing the encoder and decoder.} Early-fusion can match the performance of late-fusion models when trained for longer. However, the gap is bigger on image-caption data.}",
        "trans_content": "\\caption{\\textbf{编码器和解码器初始化时的早期融合与后期融合比较。} 当训练时间更长时，早期融合可以达到后期融合模型的性能。然而，在图像字幕数据上差距更大。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_30>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws parameters for early-fusion.} Doing regression to derive the scaling laws coefficients leads to very close results to using the closed-form solution.}",
        "trans_content": "\\caption{\\textbf{早期融合的缩放律参数。} 通过回归推导缩放律系数所得结果与使用闭式解所得到的结果非常接近。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_31>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Regression results of the scaling laws coefficients.} our estimation of the scaling coefficients is close to the closed form solution.}",
        "trans_content": "\\caption{\\textbf{尺度定律系数的回归结果。} 我们对尺度系数的估计接近闭式解。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_32>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Observed vs predicted loss.} We visualize the loss predicted by our scaling laws (\\Cref{eq:scaling_laws}) and the actual loss achieved by each run.}",
        "trans_content": "\\caption{\\textbf{观察到的损失与预测的损失.} 我们展示了通过我们的尺度定律 (\\Cref{eq:scaling_laws}) 预测的损失与每次运行实际达到的损失。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_33>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Modality-specific specialization.} We visualize the experts specialization to text and image modalities. Models are evaluated on Obelics.}",
        "trans_content": "\\caption{\\textbf{模态特定的专门化。} 我们可视化了专家在文本和图像模态上的专门化情况。模型在 Obelics 上进行评估。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_34>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws sensitivity.} We report the mean and standard deviation after bootstrapping with 100 iterations.}",
        "trans_content": "\\caption{\\textbf{缩放规律的敏感性。} 我们报告了经过 100 次迭代的自助法后的均值和标准差。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_35>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for sparse native multimodal models}. Higher exponent for active parameters.}",
        "trans_content": "\\caption{\\textbf{稀疏原生多模态模型的缩放规律}。活动参数的指数更高。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_36>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for native multimodal models.} From left to right: late-fusion (dense), early-fusion (dense) and early-fusion MoEs. The scaling exponents are very close for all models. However, MoEs leads to overall lower loss (smaller multiplicative constant) and takes longer to saturate.}",
        "trans_content": "\\caption{\\textbf{原生多模态模型的尺度法则。} 从左到右：后融合（密集型）、早融合（密集型）和早融合 MoEs。所有模型的尺度指数非常接近。然而，MoEs 导致总体较低的损失（较小的乘法常数），并且需要更长的时间才能饱和。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_37>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for native multimodal models.} From top to bottom: late-fusion (dense), early-fusion (dense) and early-fusion MoEs. From left to right: cross-entropy on the validation set of image-caption, interleaved and text-only data.}",
        "trans_content": "\\caption{\\textbf{原生多模态模型的规模法则。} 从上到下：后融合（密集型）、前融合（密集型）和前融合MoEs。 从左到右：图像-标题验证集上的交叉熵、交错数据和仅文本数据。}"
    },
    {
        "placeholder": "<PLACEHOLDER_CAP_38>",
        "cap_type": "caption",
        "content": "\\caption{\\textbf{Scaling laws for early-fusion native multimodal models.} Our runs across different training mixtures (Image-caption-Interleaved-Text) and FLOPs. We visulize the final validation loss on 3 data types: HQITP (left), Obelics (middle) and DCLM (right).}",
        "trans_content": "\\caption{\\textbf{早期融合原生多模态模型的尺度定律。} 我们在不同的训练混合（图像-字幕交错文本）和FLOPs上的实验结果。我们可视化了三种数据类型上的最终验证损失：HQITP（左）、Obelics（中）和DCLM（右）。}"
    }
]