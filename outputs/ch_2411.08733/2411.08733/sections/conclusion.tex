\vspace{-5pt}
\section{结论}
\vspace{-5pt}

本文提出了动态奖励与提示优化相结合的方法（\ours），这是一种无需微调即可实现大语言模型自我对齐的策略。\ours 将一种新颖的动态奖励机制融入基于搜索的提示优化框架，使大语言模型能够自适应地改进其特定的对齐弱点。对八个大语言模型的实验表明，经过 \ours 增强的基础模型在性能上优于经过 SFT/RLHF 微调的同类模型，其优化后的提示也超过了人类专家所设计的提示。\ours 的适应性与高效性为构建更加个性化的人工智能系统提供了一条有前景的路径。

\newpage
\section*{局限性}

尽管 \ours 在无需调优的自我对齐方面展示了显著的进展，但仍有一些潜在的局限性需要讨论。

\noindent \textbf{优化成本。}  
无需调优的对齐并非不付出代价。理想情况下，为每个查询优化对齐提示可能会更有效，但其计算开销是不可接受的。这个问题类似于基于解码的对齐，其中对齐引导的解码需要针对每个查询进行。然而，\ours 只需要对每个 LLM 进行一次优化，从而将优化后的对齐提示存储在 LLM 内存中供以后使用，显著减少了开销。关于 \ours 的成本的详细分析可以在 \ref{sec:i_cost} 中找到。

\noindent \textbf{计算开销。}  
与 SFT / RLHF 调优的模型相比，\ours 中优化和复杂提示所增加的输入上下文引入了微小的计算开销。随着现代 LLM 的发展，例如更大的上下文窗口，我们认为这种计算开销是可以管理的。此外，一旦有了优化提示，\ours 可以通过提示压缩技术进一步减少提示的长度，而不会牺牲性能，未来的工作可以进一步探讨这一点。

\noindent \textbf{自动奖励。}  
另一个我们注意到的潜在局限性是 \ours 中完全自动化的内部奖励过程可能被忽视。例如，动态奖励可能会分配不精确的奖励，导致不良行为。我们承认这一潜在问题，并已经手动审查了优化提示，未发现与此自动化优化过程相关的严重问题。未来的工作应该开发系统化的方法来监控并确保奖励分配的准确性以及由此产生的模型行为。

\noindent \textbf{LLM 的自我修正能力。}  
LLM 的自我修正能力也可能是一个潜在的局限性。在优化系统提示和上下文示例时，我们依赖于 LLM 生成的反馈，而这些反馈有时可能是不准确的。在分析反馈痕迹时，我们观察到虽然一些反馈过于苛刻，但大多数反馈是建设性的。重要的是，搜索过程减轻了过于苛刻或不正确的反馈对整体优化质量的影响。未来的工作可能会探索额外的保护措施，以进一步确保 LLM 生成的反馈在整个过程中是正确和可靠的。

\noindent \textbf{与微调的结合。}  
人们可能自然会想知道，\ours 是否可以用来合成对齐数据，并与微调方法结合以进一步提高对齐性能。答案是肯定的；然而，正如论文中所强调的，\ours 的独特优势之一是其适应性，能够迅速适应新的奖励或用户特定的要求。我们重视这种特性，并将 \ours 与微调的结合留待未来的工作进行探讨。

\noindent \textbf{模型的容量假设。}  
\ours 所涉及的模型有一些假设。首先，\ours 利用强大的 LLM，特别是 GPT-4，作为优化器来最大化动态奖励和对齐反馈的性能。未来的研究可以探索其他优化器模型，包括开源选项，从而实现 \ours 的应用普及。此外，\ours 对基础模型提出了一定的容量要求。考虑到我们优化的对齐提示的复杂性，较小且不太强大的 LLM，如 LLaMA-7b~\cite{touvron2023llama}，可能无法通过 \ours 获得显著的提升，尽管仍然可能有一定的增强。我们的假设是，经过更好预训练和能遵循指令的模型有更大的潜力通过 \ours 得到增强。我们将这个有意义的问题留给未来的研究，探索 LLM 的对齐潜力和阈值。

最后，未来的工作可能会进一步探索动态奖励机制的增强和 \ours 在不同领域和任务中的更广泛应用。
\section*{致谢}

我们感谢匿名评审人提供的建设性评论和建议。我们还感谢 Enze Ma 将 \ours 集成到 \texttt{LLM Reasoners} 中，并感谢与 MixLab 成员的宝贵讨论。本研究得到了 OpenAI Agentic AI 研究资助计划的支持。本文中表达的观点和结论仅代表作者本人，未必反映资助机构的观点。
