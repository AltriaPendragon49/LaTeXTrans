\section{方法论}
\vspace{-5pt}

在本节中，我们正式介绍我们的公式化方法，并提出 \ours 以通过优化对齐指令来解决对齐问题。
\subsection{问题表述}

给定一个大型语言模型（LLM）$\mathcal{B}$，一个对齐指令由两部分组成：系统提示 $\mathcal{P}$ 和一组 $N$ 个上下文学习（ICL）示例 $\mathcal{I}$。
系统提示 $\mathcal{P}$ 作为前缀提供高级指令，设定语气，并对模型的响应施加约束。每个 ICL 示例 $\mathcal{I}_i$ 由一对 $(q_i, d_i)$ 组成，其中 $q_i$ 是输入查询，$d_i$ 是对应的期望响应，因此我们可以表示 $\mathcal{I} = \{(q_1, d_1), (q_2, d_2), \ldots, (q_N, d_N)\}$。

在系统提示 $\mathcal{P}$ 和选择的 $K$ 个 ICL 示例子集 $\mathcal{I}_K \subseteq \mathcal{I}$ 条件下，输入 $x$ 的对齐模型响应 $y$ 由以下公式生成：
\[
y = \mathcal{B}(x \mid \mathcal{P}, \mathcal{I}_K)
\]

\ours 旨在优化系统提示 $\mathcal{P}$ 和 ICL 示例 $\mathcal{I}_K$ 以增强对齐。这涉及找到最佳的 $\mathcal{P}^*$ 和 $\mathcal{I}_K^*$，以最大化模型响应的对齐度。这个优化问题可以表述为：
\[
(\mathcal{P}^*, \mathcal{I}_K^*) = \arg\max_{\mathcal{P}, \mathcal{I}_K} \mathbb{E}_{x \sim \mathcal{D}_x} \left[\mathcal{B}(x \mid \mathcal{P}, \mathcal{I}_K) \right]
\]
\noindent 其中 $\mathcal{D}_x$ 表示输入查询的分布，期望 $\mathbb{E}$ 表示基于特定度量的响应对齐性能。
\subsection{动态奖励与提示优化 (\ours)}

鉴于系统提示和ICL示例的不同性质，我们提出分别优化它们，从而形成一个两步优化方法。我们首先构建一个通用的ICL示例集，并优化它们的响应以获得$\mathcal{I}^*$。接下来，我们根据优化后的通用集$\mathcal{I}^*$估算一个特定模型的系统提示$\mathcal{P}^*$。值得注意的是，我们利用了\texttt{LLM Reasoners}\footnote{\url{https://github.com/maitrix-org/llm-reasoners}}框架~\cite{hao2023reasoning, hao2024llm}作为提示优化（PO）框架。具体而言，\texttt{LLM Reasoners}包含一个基础模型$\mathcal{B}$、一个优化器$\mathcal{O}$和一个评估器$\mathcal{E}$。它作为一个搜索代理，迭代地与模型的环境交互，利用优化器$\mathcal{O}$根据奖励函数$\mathcal{R}$调整提示$\mathcal{P}$或ICL示例$\mathcal{I}$。有关更多细节，请参阅原始文献。接下来，我们将介绍\ours的核心组件。
\subsubsection{动态奖励用于对齐}

我们将这个优化问题表述为马尔可夫决策过程（MDP）。在这个框架中，状态 $s \in \mathcal{S}$ 代表我们的优化目标，可以是系统提示或上下文示例。动作 $a \in \mathcal{A}$ 是根据在评估任何给定状态时获得的对齐反馈来定义的。关键动机是利用大语言模型（LLM）出色的泛化能力来评估和分析状态，引导状态转换向最优状态前进。我们采用不同的评估技术来进行系统提示和上下文示例的优化，这些将在后续章节中详细介绍。高效地遍历这个状态空间至关重要，为此，我们采用了束搜索，因为它在有效性和计算成本方面具有优势。

我们优化任务中的一个关键挑战是设计一个能够处理像对齐这样广泛和通用问题的奖励函数。如图~\ref{fig:dynamic_rewarding}所示，单一的统一奖励函数由于我们旨在与基础 LLM $\mathcal{B}$ 对齐的庞大查询空间而不切实际。不同的查询强调不同的焦点，这意味着某些评估标准可能对某些查询适用，但对其他查询则不适用。为了克服这一问题，我们引入了一个动态奖励函数 $\mathcal{R}$，它可以根据正在评估的特定查询动态调整。值得注意的是，我们的方法与一些最近的对齐研究在概念上有相似之处，这些研究也提倡可适应和查询敏感的对齐策略~\cite{bai2022constitutional, sun2024principle}。然而，关键的区别在于我们的动态奖励函数不仅能够实现灵活的评估，还能够无缝地融入一个正式定义的优化框架。

具体来说，我们首先预定义了一组奖励标准 $\mathbb{R}$，模型可以根据需要动态选择最相关的奖励，同时也保留在必要时提出新奖励的灵活性。形式上，对于给定的查询 \( q \)，动态奖励函数 $\mathcal{R}$ 根据动态选择或提出的奖励 $\mathbb{R}_q$ 来评估模型的响应 $\sigma$，其中 $\mathbb{R}_q \subseteq \mathbb{R} \cup \mathbb{R}^*$，且 $\mathbb{R}^*$ 表示新提出的奖励。奖励函数定义为：

\[
\mathcal{R}(\sigma \mid \mathbb{R}_q) = \frac{1}{|\mathbb{R}_q|} \sum_{r \in \mathbb{R}_q} r(\sigma)
\]

其中，$\mathbb{R}_q$ 表示针对给定查询 \( q \) 量身定制的相关奖励，$r(\sigma)$ 表示在评估任何响应 \(\sigma\) 时特定奖励的得分。

这使我们能够根据每个特定查询最相关的标准灵活地对响应进行评分和评估，确保评估在上下文上是适当的和全面的。
\subsubsection{ICL 示例优化}

为了优化上下文学习（ICL）示例，我们从一组基础 ICL 示例 $\mathcal{I}_{\text{base}} = \{(q_1, b_1), (q_2, b_2), \ldots, (q_N, b_N)\} $ 开始，其中 $q_i$ 是查询，$b_i$ 是对该查询的基础回应，$N$ 是上下文示例的总数。我们的总体目标是寻找一个通用集合 $\mathcal{I}^{*}$，以最大化在不同模型之间的一致性。

我们对每个 ICL 示例 $(q_i, b_i)$ 进行单独优化。ICL 示例的搜索树初始状态被定义为该查询的基础回应，即 $s_0 = b_i$。在任意时间 $t$，搜索树的状态 $s_t$ 表示该示例的回应。这使我们能够在任意时刻 $t$ 系统地监控和评估回应。状态空间 $\mathcal{S}$ 包含了对查询 $q_i$ 的所有可能回应。

为了评估并提升一致性，我们使用动态奖励函数 $\mathcal{R}$。与查询 $q_i$ 相关的奖励 $\mathbb{R}_{q_i}$ 是经过专门选择或可能被提出的新奖励。奖励函数 $\mathcal{R}$ 和评估器 $\mathcal{E}$ 根据这些奖励对状态 $s_t$ 进行评估，提供奖励 $r_t$ 和一致性反馈 $a_t$：

\[
\begin{aligned}
& r_t = \mathcal{R}(s_t \mid \mathbb{R}_{q_i}) \\
& a_t = \mathcal{E}(s_t \mid \mathbb{R}_{q_i})
\end{aligned}
\]

请注意，在实际中，评估和奖励生成是通过单一提示同时进行的，因此该评估过程也可以被视为动态的。由优化器 $\mathcal{O}$ 实现的状态转移函数 $\mathcal{T}$ 接着更新状态：

\[
s_{t+1} = \mathcal{T}(s_t, a_t)
\]

该优化过程的详细伪代码见附录 \ref{sec:opti_algo} 中的算法 \ref{alg:icl_opti}，我们算法所使用的提示见附录 \ref{sec:meta_prompts}。
\subsubsection{系统提示优化}

系统提示的优化过程类似于ICL示例优化过程。对于系统提示优化，我们使用 $K$ 个优化后的ICL示例 $\mathcal{I}_K^*  \subseteq \mathcal{I}^*$，其中 $K$ 个ICL示例是通过基于相似性的检索选择的。我们收集一组种子样本 $\mathcal{X} = \{x_1, x_2, \ldots, x_N \}$，其中 $x_i$ 是用于测试基础模型 $\mathcal{B}$ 对齐性的查询。此过程的目标是找到最佳提示 $\mathcal{P}^*$（假设我们已经获得了 $\mathcal{I}_K^*$），使得LLM $\mathcal{B}$ 的对齐性最大化。此提示是特定于基础模型 $\mathcal{B}$ 的，并将为模型提供可操作的见解和指导，以改善其对齐性。

优化过程首先通过定义初始状态 $s_0$ 作为基本系统提示（例如，“你是一个有帮助的助手”）开始。在任何时刻 $t$，状态 $s_t$ 表示当前的系统提示，状态空间 $\mathcal{S}$ 包括给定LLM $\mathcal{B}$ 所有可能的系统提示。

对于给定的状态 $s_t$，我们从种子样本 $\mathcal{X}$ 中抽取一个查询 $x_t$。查询 $x_t$ 的相关奖励 $\mathbb{R}_{x_t}$ 是特定选择的或可能提出的新奖励。奖励函数 $\mathcal{R}$ 和评估器 $\mathcal{E}$ 然后评估由模型 $\mathcal{B}$ 在给定系统提示 $s_t$ 和选择的上下文示例 $\mathcal{I}_K^*$ 的情况下生成的响应，提供奖励 $r_t$ 和对齐反馈 $a_t$：

\[
\begin{aligned}
& r_t = \mathcal{R}(\mathcal{B}(x_t \mid s_t, \mathcal{I}_K^*)\mid \mathbb{R}_{x_t}) \\
& a_t = \mathcal{E}(\mathcal{B}(x_t \mid s_t, \mathcal{I}_K^*)\mid \mathbb{R}_{x_t})
\end{aligned}
\]

优化器 $\mathcal{O}$ 作为一个转移函数，然后更新状态，$ s_{t+1} = \mathcal{T}(s_t, a_t) $。该优化过程的详细伪代码在附录 \ref{sec:opti_algo} 的算法 \ref{alg:prompt_opti} 中提供。
