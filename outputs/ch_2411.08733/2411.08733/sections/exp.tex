\section{实验}
\subsection{实验设置}

\input{tables/main_table}

\noindent \textbf{评估数据集}。
我们使用标准的对齐基准 \texttt{just-eval-instruct}~\cite{Lin2024ReAlign}，该基准结合了五个流行的对齐数据集，提供了全面且细致的 LLM 对齐评估。该基准包含 1,000 个示例：前 800 个用于评估模型的有帮助性，其余 200 个用于评估它们的无害性。前 800 个示例根据五个细化的方面进行评估：\textit{有帮助性}、\textit{清晰度}、\textit{事实性}、\textit{深度} 和 \textit{参与度}，而剩余的 200 个则使用 \textit{安全性} 方面进行评估。我们使用 GPT-4 Turbo (\texttt{gpt-4-1106-preview})，这是我们实验中可用的最新 GPT-4 模型之一，来使用原始 URIAL 论文中指定的提示~\cite{Lin2024ReAlign}评估这两类示例。评分范围从 1 到 5，表示 ``强烈不同意''、``不同意''、``中立''、``同意'' 和 ``强烈同意''。请注意，我们使用的是比 URIAL 更近期的 GPT-4 版本，从而提高了我们评估管道的严格性和准确性。因此，我们在更新的评估设置下重新进行了 URIAL 的基准测试，以确保所有结果的一致性。

\noindent \textbf{种子样本}。
在使用 \ours 优化系统提示时，我们从种子数据集 $\mathcal{X}$ 中采样，以衡量系统提示在每个时间步骤的对齐性能。该种子数据集由 180 个示例组成，使用 \texttt{AlpacaEval} \cite{alpaca_eval}、\texttt{LIMA} \cite{zhou2024lima} 和 \texttt{HH-RLHF-redteam} \cite{Ganguli2022RedTL} 的数据构建。关于该数据集构建的更多细节，请参见附录 \ref{sec:impl_details}。

\noindent \textbf{模型}。
我们在实验中基准测试了 6 个开源 LLM 模型：Mistral 7b (v0.1)、Mistral 7b (Instruct)~\cite{Jiang2023Mistral7}、Llama 2 70$b^q$、Llama 2 70$b^q$ (chat)（4-bit AWQ~\cite{lin2023awq} 量化模型）~\cite{Touvron2023Llama2O}、Llama 3 8b、Llama 3 8b (Instruct)~\cite{llama3modelcard}，以及 2 个闭源模型：OpenAI 的 GPT-3.5 Turbo (\texttt{gpt-3.5-turbo}) 和 GPT-4 (\texttt{gpt-4-0613})。没有 ``chat'' 或 ``instruct'' 标签的模型为基础模型，即未通过 SFT/RLHF 调整。为了评估，我们使用贪婪解码（温度 = 0）以确保可重复性。

\noindent \textbf{基准方法}。
我们首先将 \ours 应用到基础模型中，使得未使用 \ours 的 SFT/RLHF 调整版本成为自然的基准。例如，我们比较 Mistral 7B + \ours 和 Mistral 7b (Instruct)。此外，我们还有两个基准： (1) 基础方法，在不使用 ICL 示例的情况下应用基本提示。 (2) URIAL~\cite{Lin2024ReAlign}，我们使用作者提出的提示和 ICL 示例。我们还提供了我们方法的广泛消融基准，例如将搜索算法从 Beam search 更改为 Greedy Search 或 Monte Carlo search，以及使用 ``静态奖励'' 来理解动态奖励的影响。有关这些的详细信息，请参见附录~\ref{sec:impl_details}。

\noindent \textbf{实现细节}。
我们使用 GPT-4-turbo (\texttt{gpt-4-0125-preview}) 作为优化器 $\mathcal{O}$ 和评估器 $\mathcal{E}$，除非另有说明。初始的上下文学习示例集 $\mathcal{I}_{base}$ 包含 16 个示例：3 个来自 URIAL \cite{Lin2024ReAlign} 和 13 个由 \texttt{gpt-4-0125-preview} 生成的示例。关于 $\mathcal{I}_{base}$ 设计选择的更多细节，请参见附录 \ref{sec:impl_details}。我们使用句子变换器 \cite{reimers-2019-sentence-bert} 从 $\mathcal{I}^*$ 中检索 K 个上下文学习示例。我们使用 $D$ 作为 beam 深度，$W$ 作为 beam 宽度，以及 $M$ 作为每个状态的动作样本数量（用于增长下一个迭代的树）。关于精确超参数的更多信息，请参见附录 \ref{sec:impl_details}。
\subsection{结果}

\noindent \textbf{与基线的比较}。
表 \ref{tab:main_table} 展示了 \ours 与基线的性能比较。 \ours 在调优模型和未调优模型上都优于所有基线。如图 \ref{fig:overall_comparison_chart} 所示，在强大的基础模型（如 Mistral 7b 和 LLama 2 70b$^q$）上使用 \ours，即使在基础设置下，也能超越经过 RLHF/SFT 调优的模型。值得注意的是，尽管使用的上下文学习示例较少，\ours 的表现仍优于 URIAL \citep{Lin2024ReAlign}，突出了 \ours 优化对齐指令的质量。请注意，虽然 \texttt{just-eval-instruct} 包含了一个 \textit{安全性} 指标，但我们未报告该指标，因为在我们的分析中发现该安全性指标已经饱和，所有方法（RLHF/SFT、URIAL 和 \ours）都达到了 consistently 高的分数。这种饱和是一个好兆头，表明像 \ours 这样的无调优方法可以得到非常安全的模型，符合人类的价值观。

\noindent \textbf{分类性能}。
附录~\ref{sec:cat_perf} 展示了在不同领域（如“程序”、“生活方式”、“信息搜索”、“STEM”等）上的模型性能。在此实验中，我们将 \ours 应用于基础模型，并在多个与人类相关且对齐至关重要的领域中比较其表现。 \ours 展示了持续强劲的性能，在大多数领域超越了 RLHF/SFT 调优模型，领先于所有基线。

\input{tables/prompt_transfer}
\noindent \textbf{提示迁移}。
我们还进行了提示迁移实验，即评估一个针对某个 LLM 优化的对齐指令在另一个 LLM 上的表现。表~\ref{tab:prompt_transfer} 展示了将不同优化提示迁移到 Mistral 7b 和 Llama 2 70$b^q$ 的结果。虽然在针对目标模型优化的提示下能获得最佳结果，但迁移优化提示仍能显著提升对齐性能。在 LLaMA 2 70B$^q$ 的案例中，优化的 Mistral 7B 提示能带来显著的性能提升。

\noindent \textbf{关于系统提示和 ICL 示例的消融实验}。
表 \ref{tab:ablation_icl_prompt} 显示了从 \ours 中去除系统提示和上下文学习示例的影响。使用系统提示和上下文学习示例获得了最佳性能，强调了两者在对齐中的重要性。值得指出的是，与去除系统提示相比，去除上下文学习示例的性能下降更为显著，提示上下文学习示例在对齐中相对较为重要。鉴于此，我们优化的上下文学习示例是一个有价值的资产，将公开发布，以促进进一步的对齐研究\footnote{\url{https://github.com/Singla17/DRPO}}。

\input{tables/ablation_table_prompt_icl}

\noindent \textbf{搜索算法的消融研究}。
表 \ref{tab:ablation_search_algo} 展示了搜索算法对提示优化的影响。我们保持了状态和动作定义不变，仅更改了底层搜索算法。在此实验中，我们确保 MC 和 Beam 采样的提示数量相同，即成本相同，而贪心搜索由于束宽度固定为 1，因此成本较低。更多实现细节可以在附录 \ref{sec:impl_details} 中找到。采用束搜索的 \ours 给出了最佳结果，表明对于最佳结果需要深思熟虑的搜索和高效的优化。

\input{tables/ablation_search_algo}

\input{tables/ablation_methodological}

\noindent \textbf{动态奖励的消融研究}。
我们对动态奖励机制进行了消融实验。表 \ref{tab:ablation_method} 显示了 \ours 在当前设置下，使用动态奖励优化系统提示和 ICL 时效果最佳。没有使用动态奖励的上下文示例和提示，也通过“静态奖励”进行优化，以便进行公平比较，即我们要求优化器始终优化所有奖励。更多细节请见附录 \ref{sec:impl_details}。

\noindent \textbf{上下文示例数量的影响}。
图 \ref{fig:icl_variation_chart} 可视化了上下文学习示例数量变化对对齐性能的影响。选择 $K = 2$ 为 Mistral 7b 带来了最佳的整体性能，确保了在较低上下文长度成本下的强对齐效果。并且，正如图 \ref{fig:icl_variation_chart} 所示，更高的 $K$ 并不一定能提高性能，暗示了 ICL 示例的质量更为重要。质量的重要性也在表 \ref{tab:main_table} 中得到了体现，其中 \ours 在较低 $K$ 下优于 URIAL。

\begin{figure}[!t]
    \centering
    \includegraphics[ width=\linewidth]{images/icl_variation_line_chart_white_bg_v2.png}
    \caption{Mistral 7b (指令) 在变化的 ICL 示例数量上的表现。两个示例在较低的上下文长度成本下提供了最佳性能。}
    \label{fig:icl_variation_chart}
    \vspace{-5pt}
\end{figure}

\input{tables/gpt_prompt_analysis}
\noindent \textbf{优化提示的定性分析}。
最后，我们展示了定性结果，以展示 \ours 识别模型对齐弱点并量身定制系统提示以解决这些问题的能力，如表 \ref{tab:gpt_prompt} 中针对 \texttt{gpt-3.5-turbo} 的例子所示。表中的彩色文本突出了 \texttt{gpt-3.5-turbo} 模型的特定弱点，并提供了可操作的改进建议。特别是，它强调了 \ctext[RGB]{233,252,232}{模型的知识局限性}，\ctext[RGB]{255,225,255}{提升互动性的建议}，以及 \ctext[RGB]{230,246,255}{技术性术语的使用}。对于像 Mistral 7b 这样较弱的模型，\ours 识别了重复令牌的问题，而这种问题在像 \texttt{gpt-3.5-turbo} 这样的强模型中并不存在。两个模型的完整优化提示，以及关于差异的详细注释，见附录 \ref{sec:prompt_case_study}。
