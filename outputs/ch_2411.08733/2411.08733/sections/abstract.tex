\begin{abstract}

  对于大型语言模型（LLM）的对齐，传统方法通常依赖于昂贵的训练和人工偏好注释。自我对齐旨在通过使模型自行对齐来减少这些开销。为了进一步降低成本，并在没有昂贵调优或注释的情况下实现对齐，我们提出了一种新的无需调优的自我对齐方法——动态奖励与提示优化（\ours）。我们的方法利用基于搜索的优化框架，使LLM能够反复自我改进，并制定最佳的对齐指令，且无需额外的训练或人工干预。\ours的核心是一个动态奖励机制，它识别并纠正模型特定的对齐弱点，从而使LLM能够高效地适应不同的对齐挑战。在对八个最近发布的LLM（包括开源和闭源模型）进行的实证评估中，\ours显著提高了对齐性能，且基础模型的表现超过了经过SFT/RLHF调优的对应模型。此外，通过\ours自动优化的提示超过了人工专家精心设计的提示，进一步验证了我们方法的有效性。我们的研究结果突显了当前LLM通过推理时优化实现自适应自我对齐的巨大潜力，补充了基于调优的对齐方法。\footnote{代码可用：\url{https://github.com/Singla17/DRPO}}

\end{abstract}
