[
    {
        "section": 0,
        "content": "\\documentclass[11pt]{article}\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\\usepackage{graphicx}\n\\usepackage{longtable}\n\\usepackage{wrapfig}\n\\usepackage{rotating}\n\\usepackage[normalem]{ulem}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{capt-of}\n\\usepackage{hyperref}\n\\usepackage[final]{latex/acl}\n\\usepackage{times}\n\\usepackage{latexsym}\n\\usepackage[utf8]{inputenc}\n\\usepackage{microtype}\n\\usepackage{inconsolata}\n\\usepackage{enumitem}\n\\usepackage{multirow}\n\\setcounter{secnumdepth}{1}\n\\date{\\today}\n\n\\author{\nDebarghya Datta \\and Soumajit Pramanik \\\\\nDepartment of Computer Science\\\\\nIndian Institute of Technology, Bhilai\\\\\n\\texttt{\\{debarghyad,soumajit\\}@iitbhilai.ac.in}\\\\\n}\n\n<PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n\n<PLACEHOLDER_CAP_1>",
        "trans_content": "\\documentclass[11pt]{article}\n\\usepackage[utf8]{inputenc}\n\\usepackage[T1]{fontenc}\n\\usepackage{graphicx}\n\\usepackage{longtable}\n\\usepackage{wrapfig}\n\\usepackage{rotating}\n\\usepackage[normalem]{ulem}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{capt-of}\n\\usepackage{hyperref}\n\\usepackage[final]{latex/acl}\n\\usepackage{times}\n\\usepackage{latexsym}\n\\usepackage[utf8]{inputenc}\n\\usepackage{microtype}\n\\usepackage{inconsolata}\n\\usepackage{enumitem}\n\\usepackage{multirow}\n\\setcounter{secnumdepth}{1}\n\\date{\\today}\n\n\\author{\nDebarghya Datta \\and Soumajit Pramanik \\\\\nDepartment of Computer Science\\\\\nIndian Institute of Technology, Bhilai\\\\\n\\texttt{\\{debarghyad,soumajit\\}@iitbhilai.ac.in}\\\\\n}\n\n<PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n\n<PLACEHOLDER_CAP_1>"
    },
    {
        "section": 0,
        "content": "\\begin{document}\n\\maketitle\n<PLACEHOLDER_ENV_1>",
        "trans_content": "\\begin{document}\n\\maketitle\n<PLACEHOLDER_ENV_1>"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\nNamed Entity Disambiguation (NED) is the task of resolving the ambiguity associated\nwith entity mentions in a document by linking them to the appropriate entries in a\nKnowledge Base (KB).\nRecently, NED has been applied in various fields, including digital humanities, art, architecture, literature, and biomedical science, for tasks such as searching~\\cite{meij2014entity}, question answering\n~\\cite{yih-etal-2015-semantic} and information\nextraction~\\cite{nooralahzadeh-ovrelid-2018-sirius}.\n\nThe key challenges in such domain specific NED tasks are twofold -\n(a) they provide little or no training data with ground truth annotations and\n(b) the associated knowledge graphs (KG) are typically small and with no or very limited entity descriptions~\\cite{shi2023knowledge}. In order to deal with such challenges, in this work we consider the setting where entity disambiguation is needed to be performed with \\emph{absolute absence of annotated data}.\nIn such constrained scenarios, leveraging the state-of-the-art neural entity linkers become infeasible as they are primarily dependent on a\nlarge corpus of annotated data and long enough entity descriptions from KG~\\cite{CadavidSnchez2023EvaluatingEE, arora2021low}.\nSimilarly this setting also disqualifies unsupervised NED approaches such as ~\\cite{pan2015unsupervised} which rely on labeled data to generate candidate entities such as domain-adaptive transformer-based models~\\cite{aydin-etal-2022-find}, BLINK~\\cite{wu2019zero}, Zeshel~\\cite{logeswaran-etal-2019-zero}, and auto-regressive models like GENRE~\\cite{decao2021autoregressive}.\n\nIn the literature, only a few approaches fit our constrained setting such as graph-based using mention distances~\\cite{hoffart2011robust}, PageRank/random walk based~\\cite{guo2018robust}, and graph ranking based~\\cite{alhelbawy2014graph}. A recent approach by~\\cite{arora2021low} also explores singular value decomposition, showing gold entities in a low-rank subspace. However, these methods often struggle in achieving the required efficacy while disambiguating entities.\n\nIn this work, we present a novel unsupervised NED approach for domain specific low-resource\nscenarios, which leverages the concept of Group Steiner Trees (GSTs) ~\\cite{garg2000polylogarithmic}.\nIn this approach, we map the candidate entities for each mention in the document, to nodes in the associated knowledge graph, obtain the subgraph connecting these nodes and then extract minimum cost GSTs from this sub-graph. Such GSTs facilitate collective entity disambiguation exploiting the\nfact that the entities that are truly mentioned in\na document (the `gold entities') tend to form a\ndense subgraph among the set of all candidate entities in the document.\n\n<PLACEHOLDER_ENV_2>\n\nIn summary, our main contributions are the following -\n(a) We propose an unsupervised \\textbf{G}roup \\textbf{S}teiner \\textbf{T}ree based \\textbf{N}amed \\textbf{E}ntity \\textbf{D}isambiguation (\\emph{GST-NED}) method which is capable to perform NED for low resource domains at the absence of any annotated data;\n(b) We compare our proposed approach with several state-of-the-art baselines across multiple domain specific datasets and demonstrate its superior performance\nwith significant improvements in the metrics (more than $40\\%$ in avg. in Precision@1 scores)~\\footnote{Code is available at \\url{https://github.com/deba-iitbh/GST-NED}}.",
        "trans_content": "\\section{引言}\n命名实体消歧（NED）是通过将文档中的实体提及链接到知识库（KB）中的相应条目来解决实体提及的歧义性任务。最近，NED 已被应用于多个领域，包括数字人文、艺术、建筑、文学和生物医学科学，用于搜索~\\cite{meij2014entity}、问答~\\cite{yih-etal-2015-semantic}和信息提取~\\cite{nooralahzadeh-ovrelid-2018-sirius}等任务。\n\n此类特定领域的 NED 任务的主要挑战有两方面——(a) 它们提供的训练数据很少或没有带有真实标注的数据，(b) 相关的知识图谱（KG）通常很小，并且没有或很少的实体描述~\\cite{shi2023knowledge}。为了应对这些挑战，在这项工作中，我们考虑在完全没有标注数据的情况下进行实体消歧。在这种受限的情境下，利用最先进的神经实体链接器变得不可行，因为它们主要依赖于大量的标注数据和足够长的知识图谱中的实体描述~\\cite{CadavidSnchez2023EvaluatingEE, arora2021low}。同样，这种设置也排除了无监督的 NED 方法，例如~\\cite{pan2015unsupervised}，这些方法依赖于标记数据来生成候选实体，如基于领域自适应变压器的模型~\\cite{aydin-etal-2022-find}、BLINK~\\cite{wu2019zero}、Zeshel~\\cite{logeswaran-etal-2019-zero} 和自回归模型如 GENRE~\\cite{decao2021autoregressive}。\n\n在文献中，只有少数方法适合我们的受限设置，例如基于图的使用提及距离的方法~\\cite{hoffart2011robust}、基于 PageRank/随机游走的方法~\\cite{guo2018robust}和基于图排序的方法~\\cite{alhelbawy2014graph}。最近~\\cite{arora2021low}提出的方法还探索了奇异值分解，展示了低秩子空间中的黄金实体。然而，这些方法在消歧实体时往往难以达到所需的效果。\n\n在这项工作中，我们提出了一种新颖的无监督 NED 方法，用于特定领域的低资源场景，该方法利用了\\textbf{群体斯坦纳树}（GSTs）的概念~\\cite{garg2000polylogarithmic}。在这种方法中，我们将文档中每个提及的候选实体映射到相关知识图谱中的节点，获取连接这些节点的子图，然后从该子图中提取最小成本的 GSTs。这些 GSTs 通过利用一个事实来促进集体实体消歧，即文档中真正提到的实体（“黄金实体”）往往在文档中所有候选实体的集合中形成一个密集的子图。\n\n<PLACEHOLDER_ENV_2>\n\n总之，我们的主要贡献如下 - (a) 我们提出了一种无监督的\\textbf{群体斯坦纳树}（\\textbf{GST}）\\textbf{命名实体消歧}（\\textbf{GST-NED}）方法，该方法能够在没有标注数据的低资源领域进行 NED；(b) 我们将我们提出的方法与多个领域特定数据集中的几种最先进的基线进行比较，并展示其在各项指标上的显著性能提升（Precision@1 分数平均提高超过 $40\\%$）~\\footnote{代码可在 \\url{https://github.com/deba-iitbh/GST-NED} 获取}。"
    },
    {
        "section": "2",
        "content": "\\section{Problem Statement}\nSimilar to most previous works in the NED literature (with a few exceptions \\cite{kolitsas2018end, sil2013re}), we assume that document-wise mention spans\n(usually obtained by a named entity recognizer) are already\nprovided.\nLet $d$ be a single document from a collection $D$ of documents.\nAlso, let $M_d = \\{m_1, m_2, \\ldots , m_M\\}$\nbe the set of $M$ mentions contained in $d$, and let $\\mathcal{E}$\nbe the collection of all the entities contained in the reference domain specific Knowledge Graph $KG$. The task here is to find, for each mention $m_i$ the correct entity $e \\in \\mathcal{E}$ it refers to.\n\nTypically, given the set of mentions, an NED approach performs the disambiguation in two steps -\n(a) Candidate generation, where candidate entities from the $KG$ are retrieved for each of the mentions, and (b) Candidate Ranking, where the candidate entities are ranked based on their propensities to be mapped with the corresponding mentions.\nOur primary focus in this study is the candidate ranking/disambiguation step.\nIn the following, we describe our proposed candidate ranking method and mention the approaches adhered for the other step.",
        "trans_content": "\\section{问题陈述}\n与NED文献中的大多数先前工作类似（有少数例外\\cite{kolitsas2018end, sil2013re}），我们假设文档级提及范围（通常由命名实体识别器获得）已经提供。设$d$为文档集合$D$中的一个单独文档。另设$M_d = \\{m_1, m_2, \\ldots , m_M\\}$为$d$中包含的$M$个提及的集合，并设$\\mathcal{E}$为特定领域知识图谱$KG$中包含的所有实体的集合。这里的任务是为每个提及$m_i$找到其所指的正确实体$e \\in \\mathcal{E}$。\n\n通常，给定提及集合，一个NED方法通过两个步骤进行消歧——(a) 候选生成，其中从$KG$中为每个提及检索候选实体，(b) 候选排序，其中根据候选实体与相应提及的映射倾向进行排序。在本研究中，我们的主要关注点是候选排序/消歧步骤。接下来，我们描述我们提出的候选排序方法，并提及其他步骤中采用的方法。"
    },
    {
        "section": "3",
        "content": "\\section{Methodology}",
        "trans_content": "\\section{方法论}"
    },
    {
        "section": "3_1",
        "content": "\\subsection{Candidate Generation}\nWe index the domain specific $KG$ and use fuzzy text search \\cite{max_bachmann_2021_5584996} to retrieve candidates based on the surface form of the annotated mention. This is found to be the standard practice in most of the recent unsupervised NED approaches~\\cite{yang2023b, simos2022computationally}\nFuzzy text search returns a confidence value with each potential match; we keep only the candidates which are returned with more than $0.75$ confidence value (chosen empirically)~\\footnote{In case of exact match with a KG node, we consider it to be the correct match for the mention and skip the candidate ranking step.}.",
        "trans_content": "\\subsection{候选生成}\n我们索引特定领域的 $KG$，并使用模糊文本搜索 \\cite{max_bachmann_2021_5584996} 根据标注提及的表面形式检索候选项。这被认为是最近大多数无监督NED方法中的标准做法~\\cite{yang2023b, simos2022computationally}。模糊文本搜索为每个潜在匹配返回一个置信值；我们仅保留置信值超过 $0.75$（经验选择）的候选项~\\footnote{如果与KG节点完全匹配，我们认为这是提及的正确匹配，并跳过候选排名步骤。}。"
    },
    {
        "section": "3_2",
        "content": "\\subsection{Candidate Ranking}\n\nWe use the knowledge graph ($KG$) to create a subgraph connecting all pairs of candidate entities obtained from the candidate generation step for a particular document $d$. To keep the graph size manageable, we limit path lengths to be a maximum of three hops between entity candidates. We further enhance the graph by adding node weights based on the Jaro-Winkler distance~\\cite{wang2017efficient} (reflecting similarities of candidates with mentions), and edge weights based on cosine similarities of Node2Vec \\cite{grover2016node2vec} structural embeddings of the endpoints.\nIn Figure~\\ref{fig:el-task}, we depict a document with three mentions and the corresponding induced subgraph of candidate entities (left side).\n\n\\noindent\n\\textbf{Finding GST:} Our approach to identify the correct candidates\nrelies on the intuition that a gold entity candidate from a document $d$ should be more tightly connected with other gold candidates in the induced subgraph\ncompared to other non-gold candidates.\nIn other words, we expect the gold entities within the induced subgraph to form cohesive and closely linked subgraphs due to their contextual proximities (as they are used in the same document).\nIn order to exploit this intuition, we first define the notion of terminals - for every mention $m_i$, we denote the corresponding candidate entity nodes as the terminal nodes for that mention and group them together as $T_i$.\nFurther the task remains is to select the correct candidate node from each terminal group for which we leverage the concept of Group Steiner Trees (GST)~\\cite{ding2006finding,pramanik2024uniqorn} as defined below,\n<PLACEHOLDER_ENV_3>\nIn our case, we consider $c_{ij}=(1 - w_{ij})$ where $w_{ij}$ represents the edge weight between nodes $i$ and $j$.\nAs per definition, each GST would have to necessarily choose at least one candidate entity from each of the terminal groups. Hence, each detected GST would provide at least one potential solution to the entity disambiguation problem.\nAs we further posit that the gold candidate entities are more tightly connected compared to non-gold candidates, the probability of the gold candidates to be chosen in the minimum cost GST increases (as the minimum cost GST ensures shorter distances between the chosen candidates and higher weighted edges i.e. lower edge-costs). For instance, in the right side of the Fig.~\\ref{fig:el-task}, we depict that the minimum cost GST extracted from the induced subgraph contains all the gold candidate entities corresponding to the mentions in the document.\n\n\\noindent\n\\textbf{Relaxation to GST-k and Ranking Criteria}:\nIn our setting, we actually look for the entity candidates extracted from k least cost GSTs (used k=10 for our work empirically) rather than relying upon only the minimum cost GST. This is for enhancing the robustness of the approach as it allows us to rank the different candidate entities efficiently.\nWe utilize the following three intutive ranking schemes to rank the candidate entities for each mention and choose the higher ranked one -  \\textbf{(a) GST count}: Number of GSTs where the candidate is present; the higher the better, \\textbf{(b) GST Cost}: Total cost of the GSTs where the candidate is present; the lower the better, and\n\\textbf{(c) Node Weight:} The sum of node weights in the GSTs where the candidate is present; the higher the better. Subsequently, we compare the performance of all three schemes to choose the best one.\n\n\\noindent\n\\textbf{Complexity:} Steiner trees are among the classical NP-complete problems~\\cite{ding2006finding}, and this holds for the GST problem too. However, the problem has tractable fixed-parameter complexity when the number of terminals is treated as a constant \\cite{downey2013fundamentals}, and there are also good polynomial-time approximation algorithms extensively applied in the area of keyword search over databases \\cite{ding2006finding,kacholia2005bidirectional,li2016efficient}. In\n\\emph{GST-NED}, we build on the exact solution method by \\cite{ding2006finding}, which uses a dynamic programming approach and has exponential runtime in the number of mentions (which is typically limited) but has $O(n\\log n)$ complexity in the graph size.",
        "trans_content": "\\subsection{候选排名}\n\n我们使用知识图（$KG$）为特定文档 $d$ 中从候选生成步骤获得的所有候选实体对创建一个子图。为了保持图的规模可控，我们将路径长度限制为实体候选之间最多三跳。我们进一步通过基于Jaro-Winkler距离~\\cite{wang2017efficient}（反映候选与提及的相似性）添加节点权重，以及基于节点2Vec \\cite{grover2016node2vec}结构嵌入的端点余弦相似性添加边权重来增强图。在图~\\ref{fig:el-task} 中，我们展示了一个具有三个提及的文档及其相应的候选实体诱导子图（左侧）。\n\n\\noindent\n\\textbf{寻找GST：}我们识别正确候选实体的方法依赖于这样的直觉：文档 $d$ 中的一个黄金实体候选应该在诱导子图中比其他非黄金候选与其他黄金候选连接得更紧密。换句话说，我们期望在诱导子图中的黄金实体由于其上下文接近性（因为它们在同一个文档中使用）形成紧密相连的子图。为了利用这一直觉，我们首先定义终端的概念——对于每个提及 $m_i$，我们将相应的候选实体节点标记为该提及的终端节点，并将它们分组为 $T_i$。接下来任务是从每个终端组中选择正确的候选节点，为此我们利用了组斯坦纳树（GST）的概念~\\cite{ding2006finding,pramanik2024uniqorn}，定义如下， <PLACEHOLDER_ENV_3> 在我们的情况下，我们考虑 $c_{ij}=(1 - w_{ij})$，其中 $w_{ij}$ 表示节点 $i$ 和 $j$ 之间的边权重。如定义所述，每个GST必须至少选择每个终端组中的一个候选实体。因此，每个检测到的GST将至少提供一个潜在的实体消歧问题的解决方案。由于我们进一步假设黄金候选实体比非黄金候选连接得更紧密，因此增加了在最低成本GST中被选择的黄金候选的概率（因为最低成本GST确保了所选候选之间的距离较短且边权重较高，即边成本较低）。例如，在图~\\ref{fig:el-task} 的右侧，我们展示了从诱导子图中提取的最低成本GST包含了所有对应于文档中提及的黄金候选实体。\n\n\\noindent\n\\textbf{放宽到GST-k和排名标准}： 在我们的设置中，我们实际上寻找的是从k个最低成本GST中提取的实体候选（在我们的工作中经验性地使用了k=10），而不是仅依赖于最低成本GST。这是为了增强方法的鲁棒性，因为它允许我们有效地排名不同的候选实体。我们利用以下三种直观的排名方案为每个提及的候选实体排名，并选择排名较高的实体 - \\textbf{(a) GST计数}：候选出现的GST数量；越多越好，\\textbf{(b) GST成本}：候选出现的GST总成本；越低越好，以及 \\textbf{(c) 节点权重：} 候选出现的GST中的节点权重之和；越高越好。随后，我们比较所有三种方案的表现以选择最佳方案。\n\n\\noindent\n\\textbf{复杂性：}斯坦纳树是经典的NP完全问题之一~\\cite{ding2006finding}，这同样适用于GST问题。然而，当将终端数视为常数时，该问题具有可处理的固定参数复杂性 \\cite{downey2013fundamentals}，并且在数据库关键词搜索领域广泛应用了良好的多项式时间近似算法 \\cite{ding2006finding,kacholia2005bidirectional,li2016efficient}。在\\emph{GST-NED} 中，我们基于 \\cite{ding2006finding} 的精确解法，该方法采用动态规划方法，在提及数（通常有限）上具有指数运行时间，但在图规模上具有 $O(n\\log n)$ 复杂性。"
    },
    {
        "section": "4",
        "content": "\\section{Experimental Setup}",
        "trans_content": "\\section{实验设置}"
    },
    {
        "section": "4_1",
        "content": "\\subsection{Datasets}\nIn order to show the efficacy of our model, we choose the following four datasets from diverse domains of literature, law, museum artifacts and chemicals (see Table.~\\ref{tab:data-stat} for more details).\n\n\\noindent\n\\textbf{WWO}\\footnote{\\url{https://www.wwp.northeastern.edu/wwo}} is a collection of textual documents (poems, plays and novels) by pre-Victorian women writers, partially\nannotated~\\cite{flanders2010encoding} with person, works and places entities.\n\n<PLACEHOLDER_ENV_4>\n\n\\noindent\n\\textbf{1641}\\footnote{\\url{http://1641.tcd.ie/}} consists of legal texts in the form of court witness statements recorded after the Irish Rebellion of 1641, partially\nannotated with person names against a subset of DBpedia KB~\\cite{klie2020zero}.\n\n\\noindent\n\\textbf{Chemical} dataset\nis sourced from the BC5CDR corpus ~\\cite{li2016biocreative}. It features a comprehensive human annotations of chemicals, each tagged with unique MeSH identifiers.\nFor the categorization of chemicals, the Chemicals vocabulary is sourced from the Comparative Toxicogenomics Database (CTD)~\\footnote{\\url{https://www.ctdbase.org/}}.\n\n\\noindent\n\\textbf{Artifact}~\\cite{CadavidSnchez2023EvaluatingEE} is a collection of digital descriptions of Museum objects\nannotated with four different text fields:\ntitle, detailed description, free-form metadata against the Getty\nArts, and Architecture Thesaurus~\\footnote{\\url{https://www.getty.edu/research/tools/vocabularies/aat/about.html}}(AAT).",
        "trans_content": "\\subsection{数据集}\n为了展示我们模型的有效性，我们选择了来自文学、法律、博物馆文物和化学品等不同领域的以下四个数据集（更多详细信息见表~\\ref{tab:data-stat}）。\n\n\\noindent\n\\textbf{WWO}\\footnote{\\url{https://www.wwp.northeastern.edu/wwo}} 是一个由维多利亚时代前女性作家创作的文本文档（诗歌、戏剧和小说）组成的集合，部分标注了人物、作品和地点实体~\\cite{flanders2010encoding}。\n\n<PLACEHOLDER_ENV_4>\n\n\\noindent\n\\textbf{1641}\\footnote{\\url{http://1641.tcd.ie/}} 包含记录在1641年爱尔兰叛乱后法庭证人陈述形式的法律文本，部分标注了与DBpedia知识库子集对比的人名~\\cite{klie2020zero}。\n\n\\noindent\n\\textbf{Chemical} 数据集来自BC5CDR语料库 ~\\cite{li2016biocreative}。它包含了化学品的全面人工标注，每个化学品都有独特的MeSH标识符。化学品的分类使用了来自比较毒理基因组学数据库（CTD）~\\footnote{\\url{https://www.ctdbase.org/}} 的化学品词汇。\n\n\\noindent\n\\textbf{Artifact}~\\cite{CadavidSnchez2023EvaluatingEE} 是一个博物馆对象的数字描述集合，标注了四个不同的文本字段：标题、详细描述、对Getty艺术及建筑词表~\\footnote{\\url{https://www.getty.edu/research/tools/vocabularies/aat/about.html}}（AAT）的自由形式元数据。"
    },
    {
        "section": "4_2",
        "content": "\\subsection{Baselines}\nTo compare the performance of our proposed approach, we leverage the following baselines~\\footnote{All the Datasets and Baseline codes are available under MIT \\& Apache License.}.\n\n\\noindent\n\\textbf{NameMatch}\\cite{klie2020zero}. We employ a string-matching approach to select candidates that exactly match the surface form of the mention.\n\n\\noindent\n\\textbf{BLINK*}~\\cite{wu2019zero}. We adapt a fine-tuned BLINK model in our domain specific setup for predicting named entities for each mention. As it matches entities to Wikipedia\\footnote{\\url{https://www.wikipedia.org/}} by default, we subsequently perform a fuzzy matching process to align the predicted entities with our domain specific knowledge base.\n\n\\noindent\n\\textbf{WalkingNED}~\\cite{guo2018robust} is a graph-based approach to disambiguate the mention candidates, based on local similarity (surface form similarity) and global similarity (similarity between the semantic signatures of the candidate and the document computed using PageRank).\n\n\\noindent\n\\textbf{Eigenthemes}~\\cite{arora2021low}\nis an approach which\nleverages the inherent property of `gold entities' to cluster together within the embedding space by representing entities as vectors and utilizing Singular Value Decomposition (SVD).\n\n<PLACEHOLDER_ENV_5>",
        "trans_content": "\\subsection{基线}\n为了比较我们提出方法的性能，我们利用以下基线\\footnote{所有数据集和基线代码均可在MIT \\& Apache许可证下获得。}。\n\n\\noindent\n\\textbf{NameMatch}\\cite{klie2020zero}。我们采用字符串匹配的方法来选择与提及的表面形式完全匹配的候选项。\n\n\\noindent\n\\textbf{BLINK*}~\\cite{wu2019zero}。我们在领域特定设置中调整了一个微调的BLINK模型，用于预测每个提及的命名实体。由于默认情况下它将实体匹配到维基百科\\footnote{\\url{https://www.wikipedia.org/}}，我们随后进行模糊匹配过程，以将预测的实体与我们领域特定的知识库对齐。\n\n\\noindent\n\\textbf{WalkingNED}~\\cite{guo2018robust} 是一种基于图的方式，用于基于局部相似性（表面形式相似性）和全局相似性（候选项的语义签名与文档之间的相似性，通过PageRank计算）来消除提及候选项的歧义。\n\n\\noindent\n\\textbf{Eigenthemes}~\\cite{arora2021low} 是一种方法，利用“黄金实体”在嵌入空间内聚集在一起的固有性质，通过将实体表示为向量并利用奇异值分解（SVD）。 \n\n<PLACEHOLDER_ENV_5>"
    },
    {
        "section": "4_3",
        "content": "\\subsection{Metrics}\nSimilar to the state-of-the-art literature in NED, we use Precision@1 (correctness of top ranked candidate) and Hit@5 (presence of gold entity in top five ranked candidate) as our evaluation metrics.",
        "trans_content": "\\subsection{指标}\n与 NED 领域的最新文献类似，我们使用 Precision@1（最高排名候选的正确性）和 Hit@5（前五名排名候选中存在金标实体）作为我们的评估指标。"
    },
    {
        "section": "5",
        "content": "\\section{Results and Discussion}\nWe compared our proposed \\emph{GST-NED} approach with other baselines algorithms and the corresponding results are depicted in Table.~\\ref{tab:org1c6e60d}. We can observe that our method outperforms the state-of-the-art in all the datasets (especially in terms of $P@1$). In 1641, the relatively poor performance of all the algorithms stems from the poor recall of the candidate entities (see Table.~\\ref{tab:data-stat}). BLINK* in general works poorly as it struggles to find a suitable match in the domain specific knowledge bases.\n\n<PLACEHOLDER_ENV_6>\n\n\\noindent\n\\textbf{Analysing Ranking Schemes}\n In Table.~\\ref{tab:abl_tab}, we analyse the impact of choosing different ranking schemes for candidate ranking in $\\emph{GST-NED}$. It is observed that the GST-count scheme performs the best in our scenario.\n\n\\noindent\n\\textbf{Parameter Fine-tuning}\nIn order to optimize the metric values, we conduct extensive empirical experiments with varying fuzzy threshold values for candidate generation and different numbers of top-ranked GSTs (k) for candidate ranking. These experiments are performed on a small held-out subset ($~10\\%$) of the 'WWO' and 'Artifact' datasets, with results presented in Table.~\\ref{tbl:hyper_threshold}, \\ref{tbl:hyper_k}.\nBased on our analysis, considering the fuzzy threshold value of $0.75$ and top-10 GSTs yield the highest Precision@1 score for our setup. Consequently, these parameters are used for all the experiments reported in this work.\n\n<PLACEHOLDER_ENV_7>\n\n<PLACEHOLDER_ENV_8>\n\\noindent\n\\textbf{Error Analysis}\nWe conduct a detailed error analysis to identify the distribution of errors in our proposed pipeline. Specifically, we compute the proportion of instances where error occurs due to: (a) the gold (correct) entity not being present in the candidate list, (b) the gold entity being present in the candidate list but not in the top-k GSTs, and (c) the gold entity being included in the top-k GSTs but does not rank in the top-1 position. On the ‘WWO’ dataset, 14\\% of errors corresponded to (a), 11\\% to (b), and 18\\% to (c), while the remaining 57\\% of cases were correctly resolved, resulting in a precision@1 score of 0.57. These findings suggest that enhancing both the ranking mechanism and candidate generation process are critical for achieving improved performance.",
        "trans_content": "\\section{结果与讨论}\n我们将提出的 \\emph{GST-NED} 方法与其他基线算法进行了比较，相应的结果如表~\\ref{tab:org1c6e60d} 所示。我们可以观察到，我们的方法在所有数据集上（尤其是在 $P@1$ 方面）优于现有的最先进技术。在 1641 数据集中，所有算法表现相对较差的原因在于候选实体的召回率较低（参见表~\\ref{tab:data-stat}）。总体而言，BLINK* 表现不佳，因为它难以在特定领域的知识库中找到合适的匹配。\n\n<PLACEHOLDER_ENV_6>\n\n\\noindent\n\\textbf{排名方案分析}\n在表~\\ref{tab:abl_tab} 中，我们分析了在 $\\emph{GST-NED}$ 中选择不同的候选排名方案的影响。观察到 GST-count 方案在我们的场景中表现最佳。\n\n\\noindent\n\\textbf{参数微调}\n为了优化指标值，我们进行了广泛的实证实验，调整了候选生成的模糊阈值和候选排名中不同数量的前 k 名 GST。这些实验在 'WWO' 和 'Artifact' 数据集的一个小型保留子集（约 $10\\%$）上进行，结果如表~\\ref{tbl:hyper_threshold}、\\ref{tbl:hyper_k} 所示。根据我们的分析，考虑 $0.75$ 的模糊阈值和前 10 个 GST 能够在我们的设置中获得最高的 Precision@1 分数。因此，这些参数被用于本文报告的所有实验。\n\n<PLACEHOLDER_ENV_7>\n\n<PLACEHOLDER_ENV_8>\n\\noindent\n\\textbf{错误分析}\n我们进行了详细的错误分析，以确定我们提出的流程中错误的分布情况。具体来说，我们计算了由于以下原因导致错误的实例比例：（a）候选列表中不存在黄金（正确）实体，（b）黄金实体存在于候选列表中但不在前 k 个 GST 中，（c）黄金实体包含在前 k 个 GST 中但未排在前 1 位。在 'WWO' 数据集中，14\\% 的错误属于（a），11\\% 属于（b），18\\% 属于（c），其余 57\\% 的情况得到正确解决，precision@1 分数为 0.57。这些发现表明，加强排名机制和候选生成过程对于实现性能提升至关重要。"
    },
    {
        "section": "6",
        "content": "\\section{Conclusion}\nIn this paper, we have addressed the problem of NED of domain-specific corpora in the absence of annotated data. It works based on the intuition that a gold\nentity candidate from a document should be more cohesively connected with other gold candidates in the\nknowledge graph compared to other non-gold candidates. We have leveraged the concept of Group Steiner Trees (GSTs), that relies solely on the availability of candidate entity names and a domain specific knowledge graph. Extraction of minimum cost GSTs in our proposed approach \\emph{GST-NED}, ensures that the chosen entities are closely connected in the domain specific knowledge graphs.\nExperiments on benchmark datasets from varied domains have portrayed the effectiveness of our proposed approach against the state-of-the art unsupervised and zero-shot approaches.",
        "trans_content": "\\section{结论}\n在本文中，我们解决了在缺乏标注数据的情况下对特定领域语料库进行命名实体识别的问题。其工作原理基于这样一个直觉：来自文档的一个黄金实体候选应该在知识图谱中与其他黄金候选相比于非黄金候选有更紧密的联系。我们利用了群体斯坦纳树（Group Steiner Trees, GSTs）的概念，该概念仅依赖于候选实体名称和特定领域知识图谱的可用性。在我们提出的方法\\emph{GST-NED}中提取最小成本GSTs，确保所选实体在特定领域知识图谱中紧密相连。对来自不同领域的基准数据集的实验表明，我们提出的方法在对比当前最先进的无监督和零样本方法时具有有效性。"
    },
    {
        "section": "7",
        "content": "\\section*{Limitations}\nOur entity disambiguation method, \\emph{GST-NED}, depends on the presence of sufficient number of entities per document to function accurately as we rely upon joint disambiguation of entities. As a result, when the entity count is very low, it fails to provide the correct response.\nOn the other hand, considering relatively longer document chunks with too many entities increases the graph size, affecting our computational efficacy. Hence, it is essential to analyze this trade-of with a detailed and thorough study. Interestingly, considering longer documents also enhances the possibility of same mention being used multiple times with different meanings which is beyond the capability of our model for the time being.\nAdditionally, further works need to be done to improve the scalability of the Steiner tree algorithm we use to compute the optimal trees. Presently it takes around 2 seconds per document for small KGs like WWO, 1641 or Artifact and around 40 seconds per document on the relatively larger KG of Chemical dataset (on a system with $3.9$GHz CPU with $16$ GB RAM).\n\n\\noindent",
        "trans_content": "\\section*{局限性}\n我们的实体消歧方法，\\emph{GST-NED}，依赖于每个文档中存在足够数量的实体以准确地运行，因为我们依靠实体的联合消歧。因此，当实体数量非常少时，它无法提供正确的响应。另一方面，考虑包含过多实体的相对较长的文档块会增加图的大小，从而影响我们的计算效率。因此，有必要通过详细和彻底的研究来分析这一权衡。有趣的是，考虑较长的文档也增加了同一提及多次使用不同含义的可能性，而这超出了我们模型目前的能力范围。\n\n\\noindent 此外，还需要进一步的工作来提高我们用于计算最优树的斯坦纳树算法的可扩展性。目前，对于像WWO、1641或Artifact这样的小型知识图谱，每个文档大约需要2秒，而在化学数据集的相对较大的知识图谱上，每个文档大约需要40秒（在具有$3.9$GHz CPU和$16$ GB RAM的系统上）。"
    },
    {
        "section": "8",
        "content": "\\section*{Ethics}\nThe data and models in this work are publicly\navailable. They could contain bias, and should be\nused with discretion.\n\n\\bibliography{acl_latex}\n\n\\end{document}",
        "trans_content": "\\section*{伦理}\n本工作中的数据和模型是公开可用的。它们可能包含偏见，应谨慎使用。\n\n\\bibliography{acl_latex}\n\n\\end{document}"
    }
]