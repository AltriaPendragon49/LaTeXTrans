[
    {
        "section": "-1",
        "content": "\n\\documentclass[10pt,twocolumn,letterpaper]{article}\n\n\\usepackage[pagenumbers]{cvpr}\n\n\\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}\n\\usepackage{multirow}\n\\usepackage{arydshln}\n\\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}\n\n\\definecolor{Lavender}{RGB}{230, 230, 250}\n\\definecolor{indigo}{rgb}{0.294, 0.0, 0.51}\n\n<PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n<PLACEHOLDER_NEWCOMMAND_2>\n\n<PLACEHOLDER_NEWCOMMAND_3>\n<PLACEHOLDER_NEWCOMMAND_4>\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n<PLACEHOLDER_NEWCOMMAND_7>\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\nRunmao Yao \\quad\nYi Du \\quad\nZhuoqun Chen \\quad\nHaoze Zheng \\quad\nChen Wang \\\\\nSpatial AI \\& Robotics (SAIR) Lab, University at Buffalo \\\\\n{\\tt\\small \\{yaorunmao, zhz19231211\\}@gmail.com, \\{yid, chenw\\}@sairlab.org, zhc057@ucsd.edu}\n}",
        "trans_content": "\n\\documentclass[10pt,twocolumn,letterpaper]{article}\n\n\\usepackage[pagenumbers]{cvpr}\n\n\\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}\n\\usepackage{multirow}\n\\usepackage{arydshln}\n\\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}\n\n\\definecolor{Lavender}{RGB}{230, 230, 250}\n\\definecolor{indigo}{rgb}{0.294, 0.0, 0.51}\n\n<PLACEHOLDER_NEWCOMMAND_0>\n<PLACEHOLDER_NEWCOMMAND_1>\n<PLACEHOLDER_NEWCOMMAND_2>\n\n<PLACEHOLDER_NEWCOMMAND_3>\n<PLACEHOLDER_NEWCOMMAND_4>\n<PLACEHOLDER_NEWCOMMAND_5>\n<PLACEHOLDER_NEWCOMMAND_6>\n<PLACEHOLDER_NEWCOMMAND_7>\n\n<PLACEHOLDER_CAP_1>\n\n\\author{\nRunmao Yao \\quad\nYi Du \\quad\nZhuoqun Chen \\quad\nHaoze Zheng \\quad\nChen Wang \\\\\nSpatial AI \\& Robotics (SAIR) Lab, University at Buffalo \\\\\n{\\tt\\small \\{yaorunmao, zhz19231211\\}@gmail.com, \\{yid, chenw\\}@sairlab.org, zhc057@ucsd.edu}\n}"
    },
    {
        "section": "0",
        "content": "\\begin{document}\n\\maketitle\n<PLACEHOLDER_sec/0_abstract_begin><PLACEHOLDER_ENV_1><PLACEHOLDER_sec/0_abstract_end>\n<PLACEHOLDER_sec/1_intro_begin>\\vspace{-5pt}",
        "trans_content": "\\begin{document}\n\\maketitle\n<PLACEHOLDER_sec/0_abstract_begin><PLACEHOLDER_ENV_1><PLACEHOLDER_sec/0_abstract_end>\n<PLACEHOLDER_sec/1_intro_begin>\\vspace{-5pt}"
    },
    {
        "section": "1",
        "content": "\\section{Introduction}\n\\label{sec:intro}\n\\vspace{-2pt}\n\nWith the rapid development of spatial computing, room reidentification (ReID) has become a key area of interest, enabling advancements in applications like augmented reality (AR) \\cite{schult2023controlroom3droomgenerationusing} and homecare robotics \\cite{sarch2022tideetidyingnovelrooms}. It plays a crucial role in enhancing user experiences across various scenarios.\nFor instance, on devices like the Apple Vision Pro, accurate room ReID enables smooth transitions between virtual and real-world elements.\nSimilarly, in AR-guided museum tours, precisely identifying a user’s position within specific rooms is essential for delivering location-sensitive content.\n\n<PLACEHOLDER_ENV_2>\n\nUnlike outdoor environments, where visual place recognition (VPR) methods have matured and perform reliably \\cite{arandjelović2016netvladcnnarchitectureweakly, hausler2021patchnetvladmultiscalefusionlocallyglobal, keetha2023anylocuniversalvisualplace}, indoor room ReID remains a challenging problem. A primary reason for this difficulty is the cluttered nature of indoor scenes, which are often densely packed with man-made objects \\cite{xu2023clusvprefficientvisualplace}. These densely distributed objects often pose significant challenges to existing methods, which were originally designed for city-style and distinct structures \\cite{7339473}. Consequently, these methods struggle to fully capture the intricate details and varied spatial layouts of indoor environments.\nFor instance, foundation models like DINO \\cite{caron2021emergingpropertiesselfsupervisedvision} and DINOv2 \\cite{oquab2024dinov2learningrobustvisual} can generate global descriptors that capture broad scene-level features. However, these descriptors may struggle in semantically similar environments, such as adjacent rooms with similar layouts or decorations, where distinguishable features are minimal \\cite{cai2022patchnetvladlearnedpatchdescriptor}. In contrast, methods like Patch-NetVLAD \\cite{hausler2021patchnetvladmultiscalefusionlocallyglobal}, AirLoc \\cite{aryan2023airlocobjectbasedindoorrelocalization} and AnyLoc \\cite{keetha2023anylocuniversalvisualplace} create a global descriptor by aggregating local features, which can enhance discriminative power. Yet, in indoor settings densely populated with similar and repetitive objects, these approaches may still face difficulties in distinguishing between highly similar features, reducing their effectiveness in such contexts \\cite{sattler2019understandinglimitationscnnbasedabsolute}.\n\nAdditionally, different from room categorization \\cite{lee2017roomnetendtoendroomlayout}, which relies on identifying object types to classify spaces into semantic categories,\nroom ReID requires accurately retrieving the same room instance from a reference database based on a given query image.\nFor instance, reidentifying a particular kitchen demands a combination of global functional contexts and fine-grained matching of specific object attributes. Moreover, room ReID must handle viewpoint variations, which necessitates tolerance for partial mismatches in object arrangement and appearance. These requirements often result in the failure of algorithms based solely on object categorization, as they lack the precision needed to reidentify unique room instances accurately \\cite{Snderhauf2015PlaceRW}.\n\nThis raises an important question: \\textit{“What kinds of object attributes are truly essential for room ReID?”} To address this, we conduct the first comprehensive study exploring multi-level object-oriented information and its impact on room ReID.\nAs shown in \\fref{fig:example_image}, our experiments show that all four levels of object-oriented information, \\ie, global context, object patches, object segmentation, and keypoints, are essential.\nSpecifically, we find that each level plays a unique role in room ReID. Global context, such as the combination of objects like a couch and television, conveys essential semantic information for categorizing a room as a living room. Object patches provide finer details, enabling differentiation within a room, such as distinguishing a bedside table in a bedroom from a desk in a workspace. Object segmentation offers further granularity by isolating individual items, like separating a dining table from surrounding chairs to clarify the room layout. Finally, keypoints on objects, such as handles on a dresser, enhance room ReID by filtering out visually similar furniture in other rooms. Moreover, integrating multi-level object-oriented information adds robustness to viewpoint variations.\n\nBased on these observations, we propose AirRoom, a simple yet highly effective room reidentification (ReID) system consisting of three stages: Global, Local, and Fine-Grained. In the Global stage, a Global Feature Extractor is used to capture global context features, which are then employed to coarsely select five functionally similar candidate rooms. In the Local stage, instance segmentation is applied to identify individual objects, followed by the Receptive Field Expander to extract object patches. An Object Feature Extractor is then used to obtain both object and patch features, which are utilized in Object-Aware Scoring to narrow the selection down to two candidate rooms. Finally, in the Fine-Grained stage, feature matching is employed to precisely identify the final room.\n\nIn summary, our contributions include:\n\n<PLACEHOLDER_ENV_3><PLACEHOLDER_sec/1_intro_end>\n<PLACEHOLDER_sec/2_related_work_begin>",
        "trans_content": "\\section{引言}\n\\label{sec:intro}\n\\vspace{-2pt}\n\n随着空间计算的迅速发展，房间重识别（Room ReID）已成为一个关键研究领域，推动了诸如增强现实（AR）\\cite{schult2023controlroom3droomgenerationusing} 和居家护理机器人 \\cite{sarch2022tideetidyingnovelrooms} 等应用的进步。它在多种场景中提升用户体验方面发挥着至关重要的作用。  \n例如，在 Apple Vision Pro 等设备上，精确的房间重识别能够实现虚拟与现实元素之间的平滑过渡。  \n同样，在 AR 引导的博物馆导览中，准确识别用户在特定房间内的位置对于提供基于位置的内容至关重要。\n\n<PLACEHOLDER_ENV_2>\n\n与室外环境中已经成熟且表现可靠的视觉位置识别（VPR）方法不同 \\cite{arandjelović2016netvladcnnarchitectureweakly, hausler2021patchnetvladmultiscalefusionlocallyglobal, keetha2023anylocuniversalvisualplace}，室内房间重识别仍然是一个具有挑战性的问题。造成这一困难的主要原因在于室内场景的杂乱特性，这些场景通常密集布置着大量人造物体 \\cite{xu2023clusvprefficientvisualplace}。这些密集分布的物体对现有方法构成了重大挑战，而这些方法最初是为城市风格和结构清晰的环境设计的 \\cite{7339473}。因此，这些方法难以充分捕捉室内环境中复杂的细节和多样的空间布局。  \n例如，像 DINO \\cite{caron2021emergingpropertiesselfsupervisedvision} 和 DINOv2 \\cite{oquab2024dinov2learningrobustvisual} 等基础模型能够生成捕捉整体场景特征的全局描述符。然而，在语义相似的环境中，例如布局或装饰风格相近的相邻房间，这些描述符可能难以区分细微差别 \\cite{cai2022patchnetvladlearnedpatchdescriptor}。相比之下，Patch-NetVLAD \\cite{hausler2021patchnetvladmultiscalefusionlocallyglobal}、AirLoc \\cite{aryan2023airlocobjectbasedindoorrelocalization} 和 AnyLoc \\cite{keetha2023anylocuniversalvisualplace} 等方法通过聚合局部特征来构建全局描述符，从而提升区分能力。尽管如此，在物体高度相似且重复出现的室内环境中，这些方法仍可能难以区分相似特征，从而降低在此类场景中的效果 \\cite{sattler2019understandinglimitationscnnbasedabsolute}。\n\n此外，与依赖物体类型识别以将空间分类为语义类别的房间分类方法不同 \\cite{lee2017roomnetendtoendroomlayout}，房间重识别的目标是在给定查询图像的基础上，从参考数据库中精确检索出同一个房间实例。  \n例如，重新识别某个特定厨房需要结合全局功能上下文以及对特定物体属性的细粒度匹配。  \n此外，房间重识别还需应对视角变化，因此必须具备容忍物体排列和外观部分不匹配的能力。这些需求常常导致仅基于物体分类的算法失效，因为它们缺乏准确识别唯一房间实例所需的精度 \\cite{Snderhauf2015PlaceRW}。\n\n这引出了一个重要问题：\\textit{“哪些物体属性对于房间重识别是真正关键的？”}  \n为了解决这个问题，我们开展了首个全面研究，探索多层次面向物体的信息及其对房间重识别的影响。  \n如 \\fref{fig:example_image} 所示，我们的实验表明，四种层次的面向物体信息，即全局上下文、物体图块、物体分割以及关键点，都是必不可少的。  \n具体而言，我们发现每个层次在房间重识别中扮演着独特的角色。全局上下文（例如沙发与电视的组合）传达了用于将房间分类为客厅的关键语义信息。物体图块提供更精细的细节，使得可以在房间内部进行区分，例如将卧室中的床头柜与工作区的书桌区分开来。物体分割进一步细化，通过分离餐桌与周围椅子等个体物体，有助于澄清房间布局。最后，物体上的关键点（如衣柜上的把手）可通过过滤其他房间中外观相似的家具来增强房间重识别的能力。此外，集成多层次的面向物体信息还能增强对视角变化的鲁棒性。\n\n基于上述观察，我们提出了 AirRoom——一个简单却高效的房间重识别系统（ReID），该系统由三个阶段组成：全局、局部和细粒度阶段。  \n在全局阶段，使用全局特征提取器捕捉全局上下文特征，进而粗略筛选出五个功能相似的候选房间。  \n在局部阶段，首先应用实例分割识别出单个物体，然后通过感受野扩展器提取物体图块。接着使用物体特征提取器提取物体及图块特征，并通过面向物体的评分机制将候选范围缩小到两个房间。  \n最后，在细粒度阶段，利用特征匹配精确地识别出最终房间。\n\n总之，我们的贡献包括：\n\n<PLACEHOLDER_ENV_3><PLACEHOLDER_sec/1_intro_end>\n<PLACEHOLDER_sec/2_related_work_begin>"
    },
    {
        "section": "2+2_1",
        "content": "\\section{Related Work}\n\\label{sec:related_work}\n\nIn this section, we review areas mostly related to our work, \\ie, image retrieval and visual place recognition.\n\n\n\\subsection{Image Retrieval}\n\nImage retrieval is a fundamental and well-established task in computer vision that involves searching for images similar to a given query within a large database.\nThe process of image retrieval typically consists of two stages: global retrieval and re-ranking. In the first stage, a global descriptor that aggregates local features is used to retrieve $k$ candidates from a large database. This is followed by spatial verification through local feature matching to re-rank these $k$ candidates. Early research relied on handcrafted features \\cite{Lowe2004DistinctiveIF, BAY2008346}, while current methods utilize deep networks to learn informative representations \\cite{cao2020unifyingdeeplocalglobal, radenović2018finetuningcnnimageretrieval}.\n\nMost image retrieval methods focus on selecting diverse relevant images to help users discover options that align with their interests or needs in real-world applications \\cite{Wan2014DeepLF}. Although these methods are effective in retrieving similar images, they often lack the emphasis on distinguishing between categories or achieving precise ReID \\cite{10.1145/1348246.1348248}.\nIn \\mbox{contrast}, our approach prioritizes achieving accurate ReID. Following a ``global retrieval and re-ranking\" pipeline, we first use global context features to identify the top five room candidates. Our object-aware mechanism then refines the search in a coarse-to-fine manner, progressively distinguishing among candidates until the most similar room is \\mbox{identified}, yielding accurate results.\n\n<PLACEHOLDER_ENV_4>",
        "trans_content": "\\section{相关工作}\n\\label{sec:related_work}\n\n在本节中，我们回顾了与我们工作最相关的研究方向，\\ie，图像检索和视觉位置识别。\n\n\\subsection{图像检索}\n\n图像检索是计算机视觉中的一项基础且成熟的任务，其目标是在一个大型数据库中搜索与给定查询图像相似的图像。图像检索过程通常包括两个阶段：全局检索和重排序。在第一个阶段，使用聚合局部特征的全局描述符从大型数据库中检索出 $k$ 个候选图像。随后通过局部特征匹配进行空间验证，以对这 $k$ 个候选图像进行重排序。早期研究依赖于手工设计的特征 \\cite{Lowe2004DistinctiveIF, BAY2008346}，而当前的方法则利用深度网络学习具有区分力的表示 \\cite{cao2020unifyingdeeplocalglobal, radenović2018finetuningcnnimageretrieval}。\n\n大多数图像检索方法关注于选择多样且相关的图像，以帮助用户在真实应用中发现符合其兴趣或需求的选项 \\cite{Wan2014DeepLF}。尽管这些方法在检索相似图像方面表现出色，但它们通常缺乏对类别区分或精确 ReID 的重视 \\cite{10.1145/1348246.1348248}。与此 \\mbox{相比}，我们的方法优先考虑实现精确的 ReID。我们遵循“全局检索和重排序”的流程，首先利用全局上下文特征识别排名前五的房间候选项。随后，我们的面向物体的机制以由粗到细的方式细化搜索，逐步区分候选项，直至识别出最相似的房间，从而获得准确的结果。\n\n<PLACEHOLDER_ENV_4>"
    },
    {
        "section": "2_2",
        "content": "\\subsection{Visual Place Recognition}\n\nVisual place recognition (VPR) is often framed as a special image retrieval problem, aiming to match a view of a location with an image of the same place taken under different conditions.\nPrevious methods fall into two categories: those that directly use global descriptors and those that aggregate local features into a global descriptor. Earlier approaches that relied on global descriptors primarily used CNN-based backbones, such as ResNet \\cite{he2015deepresiduallearningimage}, to generate these descriptors. More recent methods, however, leverage foundation models like DINOv2 \\cite{oquab2024dinov2learningrobustvisual} for enhanced feature representation. In the aggregation category, early techniques employed handcrafted features like SIFT \\cite{Lowe2004DistinctiveIF}, SURF \\cite{10.1007/11744023_32}, and ORB \\cite{6126544}. Later advancements, including the NetVLAD series \\cite{arandjelović2016netvladcnnarchitectureweakly, hausler2021patchnetvladmultiscalefusionlocallyglobal} and AnyLoc \\cite{keetha2023anylocuniversalvisualplace}, adopted learning-based models to extract feature maps and combine local features into comprehensive global descriptors.\n\nHowever, the high performance of most VPR approaches is largely attributed to large-scale training on VPR-specific datasets \\cite{keetha2023anylocuniversalvisualplace}. Collecting extensive data for outdoor scenes is relatively straightforward due to natural variations in daylight, weather, and seasons. However, such data collection is more challenging in indoor rooms, making large-scale training on indoor datasets difficult and potentially limiting their effectiveness.\nOur approach effectively tackles this challenge by focusing on object-oriented feature representations, allowing us to leverage mature, pre-trained models for object feature learning. This design enables AirRoom to deliver robust performance without requiring any additional training or fine-tuning on specific datasets.<PLACEHOLDER_sec/2_related_work_end>\n<PLACEHOLDER_sec/3_proposed_approach_begin>",
        "trans_content": "\\subsection{视觉位置识别}\n\n视觉位置识别（VPR）通常被框架化为一个特殊的图像检索问题，旨在将某一位置的视图与在不同条件下拍摄的同一地点的图像进行匹配。\n先前的方法分为两类：直接使用全局描述符的方法和将局部特征聚合为全局描述符的方法。早期依赖全局描述符的方法主要使用基于卷积神经网络（CNN）的骨干网络，如ResNet \\cite{he2015deepresiduallearningimage}，来生成这些描述符。然而，最近的方法则利用DINOv2等基础模型 \\cite{oquab2024dinov2learningrobustvisual} 来增强特征表示。在聚合类别中，早期技术采用了手工制作的特征，如SIFT \\cite{Lowe2004DistinctiveIF}、SURF \\cite{10.1007/11744023_32} 和ORB \\cite{6126544}。后来的进展，包括NetVLAD系列 \\cite{arandjelović2016netvladcnnarchitectureweakly, hausler2021patchnetvladmultiscalefusionlocallyglobal} 和AnyLoc \\cite{keetha2023anylocuniversalvisualplace}，采用了基于学习的模型来提取特征图，并将局部特征结合成全面的全局描述符。\n\n然而，大多数VPR方法的高性能主要归功于在专门的VPR数据集上进行的大规模训练 \\cite{keetha2023anylocuniversalvisualplace}。由于日光、天气和季节的自然变化，收集户外场景的广泛数据相对简单。然而，在室内房间中进行此类数据收集则更加困难，这使得在室内数据集上进行大规模训练变得困难，并可能限制其有效性。\n我们的方法通过专注于面向物体的特征表示有效地解决了这一挑战，允许我们利用成熟的预训练模型进行物体特征学习。这一设计使得AirRoom能够在无需对特定数据集进行额外训练或微调的情况下，提供强大的性能。<PLACEHOLDER_sec/2_related_work_end>\n<PLACEHOLDER_sec/3_proposed_approach_begin>"
    },
    {
        "section": "3",
        "content": "\\section{Proposed Approach}\n\\label{sec:proposed_approach}\n\nWe propose a simple yet highly effective pipeline, AirRoom, for room reidentification that leverages multi-level object-oriented information, as shown in \\fref{fig:pipeline}. We will now systematically introduce each module of the pipeline, following the sequence of stages in which they are executed.",
        "trans_content": "\\section{提出的方法}\n\\label{sec:proposed_approach}\n\n我们提出了一种简单而高效的管道，AirRoom，用于房间重新识别，利用多层次的面向对象信息，如\\fref{fig:pipeline}所示。接下来，我们将按照执行阶段的顺序系统地介绍管道的每个模块。"
    },
    {
        "section": "3_1",
        "content": "\\subsection{Global Stage}\n\nIn this stage, we utilize the Global Feature Extractor to capture global context features, which are derived from the collective presence of objects within the room. These features are then used for Global Retrieval, coarsely selecting semantically similar candidate rooms from the database.",
        "trans_content": "\\subsection{全局阶段}\n\n在此阶段，我们利用全局特征提取器捕获全局上下文特征，这些特征来源于房间内物体的集体存在。这些特征随后用于全局检索，从数据库中粗略地选择语义相似的候选房间。"
    },
    {
        "section": "3_1_1",
        "content": "\\subsubsection{Global Feature Extractor}\n\\label{sec:section3.1.1}\n\nIndoor rooms exhibit fewer variations compared to outdoor environments. They lack diverse topographies, such as aerial, subterranean, or underwater features, and do not experience temporal changes like day-night or seasonal variations. Consequently, collecting large datasets for each indoor room is challenging, complicating large-scale training as seen in many VPR methods \\cite{arandjelović2016netvladcnnarchitectureweakly, hausler2021patchnetvladmultiscalefusionlocallyglobal, alibey2023mixvprfeaturemixingvisual}.\n\nHowever, indoor rooms are inherently rich in objects, each contributing to the room’s overall semantic context. By leveraging this global context information, we can refine the reference search to specifically focus on rooms with similar semantic features to those in the query image. For this purpose, we prefer backbones pretrained on large image datasets, as they provide strong generalizability and effectively capture informative global context features \\cite{kornblith2019betterimagenetmodelstransfer}. Our model selections, therefore, include pretrained CNN-based models such as ResNet \\cite{he2015deepresiduallearningimage} and transformer-based self-supervised models like DINOv2 \\cite{oquab2024dinov2learningrobustvisual}.",
        "trans_content": "\\subsubsection{全局特征提取器}\n\\label{sec:section3.1.1}\n\n与户外环境相比，室内房间的变化较少。它们缺乏多样的地形特征，如空中、地下或水下特征，也不经历像昼夜或季节性变化这样的时间性变化。因此，为每个室内房间收集大规模数据集是具有挑战性的，这使得许多视觉定位和重识别（VPR）方法的规模化训练变得复杂 \\cite{arandjelović2016netvladcnnarchitectureweakly, hausler2021patchnetvladmultiscalefusionlocallyglobal, alibey2023mixvprfeaturemixingvisual}。\n\n然而，室内房间本身在物体上具有丰富的多样性，每个物体都对房间的整体语义上下文做出贡献。通过利用这种全局上下文信息，我们可以将参考搜索专门集中在与查询图像具有相似语义特征的房间上。为此，我们倾向于选择在大规模图像数据集上预训练的骨干网络，因为它们提供了较强的泛化能力，能够有效地捕捉有价值的全局上下文特征 \\cite{kornblith2019betterimagenetmodelstransfer}。因此，我们的模型选择包括基于卷积神经网络（CNN）的预训练模型，如 ResNet \\cite{he2015deepresiduallearningimage}，以及基于变换器的自监督学习模型，如 DINOv2 \\cite{oquab2024dinov2learningrobustvisual}。"
    },
    {
        "section": "3_1_2",
        "content": "\\subsubsection{Global Retrieval}\n\nUsing the Global Feature Extractor, we extract global context features for \\(M\\) query and \\(N\\) reference images. Let \\(\\mathbf{Q} \\in \\mathbb{R}^{M \\times D_g}\\) and \\(\\mathbf{R} \\in \\mathbb{R}^{N \\times D_g}\\) denote the query and reference features, respectively, where \\(D_g\\) is the feature dimension. The cosine similarity matrix \\(\\mathbf{S}\\) is then computed as:\n<PLACEHOLDER_ENV_5>\nFor each query, we select the top-5 most similar reference candidates using the following formula:\n<PLACEHOLDER_ENV_6>\nwhere \\(\\mathbf{S}_{i, :}\\) represents the cosine similarity for the \\(i\\)-th query.",
        "trans_content": "\\subsubsection{全局检索}\n\n使用全局特征提取器，我们为\\(M\\)个查询图像和\\(N\\)个参考图像提取全局上下文特征。令\\(\\mathbf{Q} \\in \\mathbb{R}^{M \\times D_g}\\)和\\(\\mathbf{R} \\in \\mathbb{R}^{N \\times D_g}\\)分别表示查询特征和参考特征，其中\\(D_g\\)是特征维度。然后计算余弦相似度矩阵\\(\\mathbf{S}\\)为：\n<PLACEHOLDER_ENV_5>\n对于每个查询，我们使用以下公式选择前5个最相似的参考候选：\n<PLACEHOLDER_ENV_6>\n其中\\(\\mathbf{S}_{i, :}\\)表示第\\(i\\)个查询的余弦相似度。"
    },
    {
        "section": "3_2",
        "content": "\\subsection{Local Stage}\n\nGlobal context features provide valuable semantic information that helps narrow down the candidate list. However, when faced with many semantically similar rooms, relying solely on global context is insufficient, and local features become increasingly essential. In this stage, we adopt a local perspective by first applying instance segmentation and the Receptive Field Expander to identify objects and patches. We then use the Object Feature Extractor to extract features from both objects and patches, followed by Object-Aware Scoring to further refine the candidate list.",
        "trans_content": "\\subsection{局部阶段}\n\n全局上下文特征提供了有价值的语义信息，有助于缩小候选列表的范围。然而，当面对许多语义相似的房间时，仅依赖全局上下文是不够的，局部特征变得越来越重要。在此阶段，我们采用局部视角，首先应用实例分割和感受野扩展器来识别物体和图像块。随后，我们使用物体特征提取器从物体和图像块中提取特征，接着通过面向物体的评分进一步优化候选列表。"
    },
    {
        "section": "3_2_1",
        "content": "\\subsubsection{Instance Segmentation}\n\nFor each query image and its corresponding five candidates, we employ instance segmentation methods, such as Mask R-CNN \\cite{he2018maskrcnn} and Semantic-SAM \\cite{li2023semanticsamsegmentrecognizegranularity}, to identify and delineate individual objects. This process generates each object's mask and bounding box. Next, we calculate the center point \\(c\\) of each object using its bounding box, as shown below:\n\n<PLACEHOLDER_ENV_7>\nIn this equation, \\(x\\) and \\(y\\) represent the pixel coordinates of the top-left corner of the bounding box, while \\(W\\) and \\(H\\) denote the width and height of the bounding box, respectively.",
        "trans_content": "\\subsubsection{实例分割}\n\n对于每张查询图像及其对应的五个候选图像，我们采用实例分割方法，如 Mask R-CNN \\cite{he2018maskrcnn} 和 Semantic-SAM \\cite{li2023semanticsamsegmentrecognizegranularity}，来识别并描绘出各个独立的物体。该过程会生成每个物体的掩码和边界框。接下来，我们利用边界框计算每个物体的中心点 \\(c\\)，如下所示：\n\n<PLACEHOLDER_ENV_7>\n\n在此公式中，\\(x\\) 和 \\(y\\) 表示边界框左上角的像素坐标，而 \\(W\\) 和 \\(H\\) 分别表示边界框的宽度和高度。"
    },
    {
        "section": "3_2_2",
        "content": "\\subsubsection{Receptive Field Expander}\n\nSingle object information alone is not sufficiently discriminative. For example, although different desks may have distinct appearances, they can be found in both dining halls and offices. However, when an object is connected with its neighboring items—such as a desk alongside a computer, keyboard, or notebook—it suggests that the room is more likely to be an office rather than a dining hall. This insight motivates us to expand the receptive field from a single object to a patch containing multiple objects.\n\nGiven the center points of all objects in an image, we employ Delaunay triangulation \\cite{10.5555/1370949} to generate a triangulated graph of object relationships. Specifically, Delaunay triangulation is applied to the set of object centers, ensuring that no object centers are inside the circumcircle of any triangle. This method maximizes the minimum angle of the triangles, preventing narrow, elongated triangles and ensuring more uniform object adjacency. By analyzing the adjacency relationships among the resulting triangles, we can construct the object adjacency matrix, which encodes the spatial and relational proximity of objects within the room.\n\n\\vspace{-10pt}\n<PLACEHOLDER_ENV_8>\n\nGiven the object adjacency matrix and bounding boxes in an image, for each object, we consider the bounding boxes of its neighboring objects and enlarge the current object's bounding box to encompass all adjacent objects. This expansion increases the receptive field, enabling us to capture richer contextual information, as illustrated in \\fref{fig:expander_image}. We then apply Non-Maximum Suppression (NMS) to select the highest confidence bounding boxes, removing overlapping ones based on their Intersection over Union (IoU) scores. This results in a set of clean, informative object patches.\n\n<PLACEHOLDER_ENV_9>",
        "trans_content": "\\subsubsection{感受野扩展器}\n\n单个物体的信息本身并不足以具有良好的判别性。例如，尽管不同的书桌可能具有不同的外观，它们既可能出现在食堂中，也可能出现在办公室中。然而，当一个物体与其邻近物体（如与计算机、键盘或笔记本并列的书桌）相关联时，就暗示该房间更有可能是办公室而不是食堂。这一见解促使我们将感受野从单一物体扩展到包含多个物体的图像区域。\n\n给定图像中所有物体的中心点，我们采用 Delaunay 三角剖分法 \\cite{10.5555/1370949} 来生成物体关系的三角形图。具体而言，Delaunay 三角剖分作用于物体中心点集合，确保没有任何物体中心位于任意三角形的外接圆内部。该方法最大化三角形的最小角度，避免出现狭长的三角形，从而确保物体邻接关系更加均匀。通过分析所得到三角形之间的邻接关系，我们可以构建物体邻接矩阵，该矩阵编码了房间内物体之间的空间与关系接近度。\n\n\\vspace{-10pt}\n<PLACEHOLDER_ENV_8>\n\n给定图像中的物体邻接矩阵和边界框，对于每一个物体，我们考虑其邻接物体的边界框，并将当前物体的边界框扩展，以包含所有邻接物体。这种扩展增加了感受野，使我们能够捕捉更丰富的上下文信息，如 \\fref{fig:expander_image} 所示。随后我们应用非极大值抑制（Non-Maximum Suppression，NMS），选取置信度最高的边界框，并基于其交并比（IoU）分数移除重叠边界框，从而获得一组干净且信息丰富的物体图像块。\n\n<PLACEHOLDER_ENV_9>"
    },
    {
        "section": "3_2_3",
        "content": "\\subsubsection{Object-Aware Refinement}\n\\label{subsec:refinement}\n\nThe Object-Aware Refinement module is composed of three key submodules: Object Feature Extractor, Mutual Nearest Neighbors, and Object-Aware Scoring.\n\n\\vspace{-6pt}\n\\paragraph{Object Feature Extractor}\n\nTo effectively leverage object patches and object segmentation information, we prioritize global features over local feature aggregation. The latter approach may fail to capture object characteristics effectively and can significantly increase computational complexity and storage demands \\cite{zheng2018sift}. As discussed in Section~\\ref{sec:section3.1.1}, we continue to rely on models pre-trained on large image datasets. Using the Object Feature Extractor, we obtain features for both query and reference patches and objects. Let \\(Q_p=\\{\\mathbf{p_i^q}\\}_{i=1}^{n_{qp}}\\) and \\(Q_o=\\{\\mathbf{o_i^q}\\}_{i=1}^{n_{qo}}\\) represent the query patch and object feature sets, respectively. For each reference image among the query’s five \\mbox{candidates}, we define the reference patch and object feature sets as \\(R_p=\\{\\mathbf{p_i^r}\\}_{i=1}^{n_{rp}}\\) and \\(R_o=\\{\\mathbf{o_i^r}\\}_{i=1}^{n_{ro}}\\).\n\n\\vspace{-6pt}\n\\paragraph{Mutual Nearest Neighbors} Given a set of query features \\(\\{\\mathbf{f_i^q}\\}_{i=1}^{n_q}\\) and reference features \\(\\{\\mathbf{f_i^r}\\}_{i=1}^{n_r}\\), we obtain feature pairs by identifying mutual nearest neighbor matches through exhaustive comparison of the two sets. Let \\(P\\) denote the set of cosine similarity scores for these mutual nearest neighbor matches, then we have\n<PLACEHOLDER_ENV_10>\nwhere\n<PLACEHOLDER_ENV_11>\n<PLACEHOLDER_ENV_12>\n<PLACEHOLDER_ENV_13>\nBy utilizing mutual nearest neighbors, we can significantly improve retrieval accuracy, simultaneously narrowing the search space and enhancing overall retrieval efficiency \\cite{zhong2017reranking}.\n\n\\vspace{-6pt}\n\\paragraph{Object-Aware Scoring} The object-aware score \\(s\\) is the sum of the global score \\(s_{\\text{global}}\\) (calculated in Equation~\\ref{eq:global feature cosine similarity}), the patch score \\(s_{\\text{patch}}\\), and the object score \\(s_{\\text{object}}\\):\n<PLACEHOLDER_ENV_14>\nHere, \\(s_{\\text{patch}}\\) and \\(s_{\\text{object}}\\) can either be \\(s_{\\text{mean}}\\) or \\(s_{\\max}\\), where\n<PLACEHOLDER_ENV_15>\nIn these equations, \\(P\\) denotes the set of cosine similarity scores for mutual nearest neighbor matches, with \\(Q_t\\) representing either \\(Q_p\\) or \\(Q_o\\), and \\(R_t\\) representing either \\(R_p\\) or \\(R_o\\). The global score \\(s_{\\text{global}}\\) serves as a prior, indicating that the initial five candidates vary in relevance. Thus, we retain this term to account for their differing levels of relevance.\n\n\\vspace{-8pt}\n\\paragraph{Object-Aware Refinement} For each query, we select the top-2 most similar reference candidates from the initial five using the Object-Aware Scoring:\n<PLACEHOLDER_ENV_16>\nwhere \\(\\mathbf{s}_{i}\\) is the object-aware scores for the \\(i\\)-th query.",
        "trans_content": "\\subsubsection{面向对象的细化}\n\\label{subsec:refinement}\n\n面向对象的细化模块由三个关键子模块组成：对象特征提取器、互近邻算法和面向对象的评分。\n\n\\vspace{-6pt}\n\\paragraph{对象特征提取器}\n\n为了有效地利用对象块和对象分割信息，我们优先考虑全局特征，而不是局部特征聚合。后者方法可能无法有效捕捉对象特征，并且可能显著增加计算复杂度和存储需求 \\cite{zheng2018sift}。如第~\\ref{sec:section3.1.1}节所讨论的，我们继续依赖于在大规模图像数据集上预训练的模型。使用对象特征提取器，我们可以获得查询块和参考块及对象的特征。设\\(Q_p=\\{\\mathbf{p_i^q}\\}_{i=1}^{n_{qp}}\\)和\\(Q_o=\\{\\mathbf{o_i^q}\\}_{i=1}^{n_{qo}}\\)分别表示查询块和对象的特征集。对于查询的五个\\mbox{候选}参考图像，我们将参考块和对象的特征集定义为\\(R_p=\\{\\mathbf{p_i^r}\\}_{i=1}^{n_{rp}}\\)和\\(R_o=\\{\\mathbf{o_i^r}\\}_{i=1}^{n_{ro}}\\)。\n\n\\vspace{-6pt}\n\\paragraph{互近邻算法} 给定一组查询特征\\(\\{\\mathbf{f_i^q}\\}_{i=1}^{n_q}\\)和参考特征\\(\\{\\mathbf{f_i^r}\\}_{i=1}^{n_r}\\)，通过对两个特征集进行穷举比较，识别出互近邻匹配对。设\\(P\\)表示这些互近邻匹配对的余弦相似度得分集，则有\n<PLACEHOLDER_ENV_10>\n其中\n<PLACEHOLDER_ENV_11>\n<PLACEHOLDER_ENV_12>\n<PLACEHOLDER_ENV_13>\n通过利用互近邻算法，我们可以显著提高检索准确性，同时缩小搜索空间并提高整体检索效率 \\cite{zhong2017reranking}。\n\n\\vspace{-6pt}\n\\paragraph{面向对象的评分} 面向对象的得分\\(s\\)是全局得分\\(s_{\\text{global}}\\)（在方程~\\ref{eq:global feature cosine similarity}中计算）、块得分\\(s_{\\text{patch}}\\)和对象得分\\(s_{\\text{object}}\\)的和：\n<PLACEHOLDER_ENV_14>\n其中，\\(s_{\\text{patch}}\\)和\\(s_{\\text{object}}\\)可以是\\(s_{\\text{mean}}\\)或\\(s_{\\max}\\)，其中\n<PLACEHOLDER_ENV_15>\n在这些方程中，\\(P\\)表示互近邻匹配对的余弦相似度得分集，\\(Q_t\\)表示\\(Q_p\\)或\\(Q_o\\)，而\\(R_t\\)表示\\(R_p\\)或\\(R_o\\)。全局得分\\(s_{\\text{global}}\\)作为先验，表明初始的五个候选具有不同的相关性。因此，我们保留这一项以考虑它们的不同相关性。\n\n\\vspace{-8pt}\n\\paragraph{面向对象的细化} 对于每个查询，我们使用面向对象的评分从初始的五个候选中选择最相似的前两个参考候选：\n<PLACEHOLDER_ENV_16>\n其中，\\(\\mathbf{s}_{i}\\)是第\\(i\\)个查询的面向对象的得分。"
    },
    {
        "section": "3_3",
        "content": "\\subsection{Fine-Grained Stage}\n\nPatch and object features provide valuable information for understanding the room layout; however, they may be insufficient when distinguishing highly visually similar rooms, particularly in the presence of viewpoint variations and occlusions. Keypoints on objects, by contrast, exhibit strong robustness to texture and appearance variations, enabling them to effectively handle partial occlusions and reject outliers \\cite{1498756}. This allows keypoints to offer a more refined approach, capturing finer details for more accurate room identification. In this stage, we use Fine-Grained Retrieval to select the final top-1 result.",
        "trans_content": "\\subsection{细粒度阶段}\n\n补丁和物体特征为理解房间布局提供了有价值的信息；然而，在区分高度视觉相似的房间时，尤其是在视角变化和遮挡存在的情况下，它们可能不足以提供足够的区分度。与此相比，物体上的关键点表现出对纹理和外观变化的强大鲁棒性，使其能够有效地处理部分遮挡并排除异常值 \\cite{1498756}。这使得关键点能够提供一种更精细的方法，捕捉更细致的细节，从而实现更精确的房间识别。在此阶段，我们使用细粒度检索来选择最终的 top-1 结果。"
    },
    {
        "section": "3_3_1",
        "content": "\\subsubsection{Fine-Grained Retrieval}\n\nDeep matchers, such as SuperGlue \\cite{sarlin2020supergluelearningfeaturematching}, perform well in visual localization tasks under challenging conditions, both indoors and outdoors. However, they tend to face efficiency issues. In contrast, LightGlue \\cite{lindenberger2023lightgluelocalfeaturematching} offers high efficiency without compromising matching accuracy, making it an ideal choice for our Fine-Grained Retrieval.\n\nFor each query image and its two candidate reference images, we match the query to each candidate and record the number of matching keypoint pairs. A higher number of matches typically indicates greater overlap and consistency between the features of the two images, suggesting a higher degree of similarity in their content \\cite{Lowe2004DistinctiveIF}. The candidate with more matches is selected as the final result.<PLACEHOLDER_sec/3_proposed_approach_end>\n<PLACEHOLDER_sec/4_experimental_results_begin>\\vspace{-3pt}",
        "trans_content": "\\subsubsection{细粒度检索}\n\n深度匹配器，如 SuperGlue \\cite{sarlin2020supergluelearningfeaturematching}，在室内外的挑战性条件下，在视觉定位任务中表现良好。然而，它们通常面临效率问题。相比之下，LightGlue \\cite{lindenberger2023lightgluelocalfeaturematching} 提供了高效性，并且没有牺牲匹配准确性，使其成为我们细粒度检索的理想选择。\n\n对于每个查询图像及其两个候选参考图像，我们将查询图像与每个候选图像进行匹配，并记录匹配的关键点对数量。更多的匹配通常意味着两张图像的特征之间有更大的重叠和一致性，表明它们内容的相似度较高 \\cite{Lowe2004DistinctiveIF}。具有更多匹配的候选图像被选为最终结果。<PLACEHOLDER_sec/3_proposed_approach_end>\n<PLACEHOLDER_sec/4_experimental_results_begin>\\vspace{-3pt}"
    },
    {
        "section": "4",
        "content": "\\section{Experimental Results}\n\\label{sec:experimental_results}\n\n<PLACEHOLDER_ENV_17>",
        "trans_content": "\\section{实验结果}\n\\label{sec:experimental_results}\n\n<PLACEHOLDER_ENV_17>"
    },
    {
        "section": "4_1",
        "content": "\\subsection{Datasets}\n\\vspace{-5pt}\nNo existing indoor scene datasets are ideally suited for room reidentification tasks, as none fully satisfy the requirements.  Datasets like ScanNet++ \\cite{yeshwanth2023scannethighfidelitydataset3d} and MIT Indoor Scenes \\cite{5206537} lack room-level segmentation, resulting in multiple rooms sharing a single scene label. The 17 Places \\cite{7801503} dataset includes uniquely labeled rooms but offers limited viewpoint variations, and the images are often vague. While this dataset also includes day-night changes, these are not particularly relevant for most indoor scenarios. The Reloc110 \\cite{aryan2023airlocobjectbasedindoorrelocalization} dataset is likely the most suitable option; however, its quality is insufficient, with many images containing only solid-colored walls or floors due to random sampling, resulting in minimal contextual information.\n\nSeveral high-quality indoor 3D datasets—such as Matterport3D \\cite{Matterport3D}, Habitat-Matterport3D \\cite{ramakrishnan2021hm3d}, the Gibson Database of 3D Spaces \\cite{xiazamirhe2018gibsonenv}, and Replica \\cite{replica19arxiv}—offer real-world indoor scenes. Building on these resources and utilizing the interactive Habitat Simulator \\cite{puig2023habitat3, szot2021habitat, habitat19iccv}, we created four new datasets: MPReID, HMReID, GibsonReID, and ReplicaReID, as shown in \\fref{fig:dataset_image}.\n\nUsing the Habitat Simulator, we configured an agent for each room and manually selected 5 to 10 key poses to guide its exploration. The agent captured 640×480 RGB-D images from various angles, resulting in 300 to 800 images per room, depending on the number of key poses. However, many randomly sampled images were of low quality, often containing only walls or floors with minimal context. To address this, we carefully filtered the images for each room, retaining those that accurately represented the space and provided valuable information for room ReID.\n\nIn total, the datasets are as follows: MPReID includes 15 scenes, 105 rooms, and 16,231 RGB-D images; HMReID consists of 21 scenes, 105 rooms, and 15,781 RGB-D images; GibsonReID contains 24 scenes, 45 rooms, and 6,743 RGB-D images; and ReplicaReID includes 12 scenes, 19 rooms, and 2,862 RGB-D images.\n\n\\vspace{-4pt}",
        "trans_content": "\\subsection{数据集}\n\\vspace{-5pt}\n目前没有现有的室内场景数据集完全适用于房间再识别任务，因为没有一个数据集能完全满足要求。像 ScanNet++ \\cite{yeshwanth2023scannethighfidelitydataset3d} 和 MIT Indoor Scenes \\cite{5206537} 这样的数据集缺乏房间级别的分割，导致多个房间共享一个场景标签。17 Places \\cite{7801503} 数据集包含了唯一标签的房间，但视角变化有限，而且图像往往较为模糊。尽管该数据集也包含昼夜变化，但这些变化对于大多数室内场景并不特别相关。Reloc110 \\cite{aryan2023airlocobjectbasedindoorrelocalization} 数据集可能是最合适的选择；然而，它的质量不够理想，许多图像仅包含纯色的墙壁或地板，由于随机采样，导致上下文信息非常少。\n\n一些高质量的室内 3D 数据集——如 Matterport3D \\cite{Matterport3D}、Habitat-Matterport3D \\cite{ramakrishnan2021hm3d}、Gibson Database of 3D Spaces \\cite{xiazamirhe2018gibsonenv} 和 Replica \\cite{replica19arxiv}——提供了真实世界的室内场景。基于这些资源，并利用互动式 Habitat Simulator \\cite{puig2023habitat3, szot2021habitat, habitat19iccv}，我们创建了四个新的数据集：MPReID、HMReID、GibsonReID 和 ReplicaReID，如 \\fref{fig:dataset_image} 所示。\n\n使用 Habitat Simulator，我们为每个房间配置了一个代理，并手动选择了 5 到 10 个关键姿势来引导其探索。代理从不同角度捕捉了 640×480 的 RGB-D 图像，每个房间的图像数量为 300 到 800 张，具体取决于关键姿势的数量。然而，许多随机采样的图像质量较差，通常只包含墙壁或地板，缺乏足够的上下文信息。为了解决这一问题，我们仔细筛选了每个房间的图像，保留了那些准确代表空间并为房间 ReID 提供有价值信息的图像。\n\n总的来说，这些数据集如下：MPReID 包括 15 个场景、105 个房间和 16,231 张 RGB-D 图像；HMReID 包含 21 个场景、105 个房间和 15,781 张 RGB-D 图像；GibsonReID 包含 24 个场景、45 个房间和 6,743 张 RGB-D 图像；ReplicaReID 包括 12 个场景、19 个房间和 2,862 张 RGB-D 图像。\n\n\\vspace{-4pt}"
    },
    {
        "section": "4_2",
        "content": "\\subsection{Database Preprocess}\n\\vspace{-4pt}\n\nIn the room reidentification setting, we have multiple query images and a reference database. For each dataset, we select only one image per room to build the database. Specifically, for all the images of each room, we first use CLIP \\cite{radford2021learningtransferablevisualmodels} to extract feature embeddings. Then, we apply K-means clustering with the number of clusters set to 1. The image closest to the cluster center is chosen as the reference image, as it best represents the room's visual characteristics \\cite{tan2005introduction}.\n\nAfter building the reference database, we preprocess features. First, we use the Global Feature Extractor to obtain and save the global context features. Next, we apply the instance segmentation module to segment the objects. Then, we use our Receptive Field Expander to obtain object patches and the Object Feature Extractor to extract and save the features of both the objects and the patches.\n\n\\vspace{-4pt}",
        "trans_content": "\\subsection{数据库预处理}\n\\vspace{-4pt}\n\n在房间重识别设置中，我们有多个查询图像和一个参考数据库。对于每个数据集，我们仅选择每个房间的一张图像来构建数据库。具体来说，对于每个房间的所有图像，我们首先使用 CLIP \\cite{radford2021learningtransferablevisualmodels} 提取特征嵌入。然后，我们应用 K-means 聚类，设定聚类数为 1。距离聚类中心最近的图像被选择为参考图像，因为它最能代表房间的视觉特征 \\cite{tan2005introduction}。\n\n在构建参考数据库后，我们对特征进行预处理。首先，我们使用全局特征提取器来获取并保存全局上下文特征。接着，我们应用实例分割模块对物体进行分割。然后，我们使用我们的感受野扩展器来获取物体补丁，并使用物体特征提取器来提取并保存物体和补丁的特征。\n\n\\vspace{-4pt}"
    },
    {
        "section": "4_3",
        "content": "\\subsection{Experimental Overview}\n\\vspace{-4pt}\nWe conducted five primary experiments: overall performance comparison, group-wise performance comparison, pipeline flexibility evaluation, ablation studies, and runtime analysis. For evaluation, we used accuracy, precision, recall, and the F1 score as metrics. Per-class precision, recall, and F1-score were computed using a multi-class confusion matrix, followed by macro averaging. Accuracy was measured as the ratio of correctly matched queries to the total number of queries. A detailed runtime analysis and additional experimental results are provided in the appendix.\n\n<PLACEHOLDER_ENV_18>\n\n<PLACEHOLDER_ENV_19>\n\n\\vspace{-4pt}",
        "trans_content": "\\subsection{实验概述}\n\\vspace{-4pt}\n我们进行了五个主要实验：整体性能比较、分组性能比较、管道灵活性评估、消融研究和运行时分析。在评估过程中，我们使用了准确率、精确度、召回率和F1分数作为评价指标。每个类别的精确度、召回率和F1分数是通过多类混淆矩阵计算得出的，随后进行了宏平均。准确率是通过正确匹配的查询与总查询数之比来衡量的。详细的运行时分析和其他实验结果在附录中提供。\n\n<PLACEHOLDER_ENV_18>\n\n<PLACEHOLDER_ENV_19>\n\n\\vspace{-4pt}"
    },
    {
        "section": "4_4",
        "content": "\\subsection{Overall Performance Comparison}\n\\vspace{-4pt}\n\\label{sec:section4.4}\n\nIn this section, we present a performance comparison between the best-performing version of our approach and several state-of-the-art methods, allowing us to benchmark our pipeline against established room reidentification models across different feature extraction and retrieval strategies.\n\nWe selected three categories of baseline methods: image retrieval (CVNet \\cite{lee2022correlationverificationimageretrieval}), global descriptor-based visual place recognition (VPR) (DINOv2 \\cite{oquab2024dinov2learningrobustvisual}), and VPR using aggregated local features (Patch-NetVLAD \\cite{hausler2021patchnetvladmultiscalefusionlocallyglobal} and AnyLoc \\cite{keetha2023anylocuniversalvisualplace}). Specifically, we used the Base version of DINOv2, configured CVNet with a ResNet50 \\cite{he2015deepresiduallearningimage} backbone and a reduction dimension of 2048, selected the performance version of Patch-NetVLAD, and set up AnyLoc with AnyLoc-VLAD-DINOv2 using 32 VLAD clusters.\n\n<PLACEHOLDER_ENV_20>\n\n\\tref{tab:overall} presents a quantitative comparison between AirRoom and baseline methods, showing that AirRoom outperforms all baselines on nearly all metrics and datasets. In room reidentification tasks, image retrieval methods generally exhibit lower classification metrics due to their focus not being on top-1 precision, while VPR methods yield better results. Global descriptor-based VPR methods capture only high-level semantic information, often retrieving rooms with similar semantics but lacking detailed features. In contrast, VPR methods using aggregated local features, such as Patch-NetVLAD, emphasize low-level encodings but may overlook global context, resulting in less accurate retrievals. \\fref{fig:failure} illustrates failure cases for CVNet, DINOv2, Patch-NetVLAD, and AnyLoc, highlighting these limitations.\nAlthough AnyLoc, known for its robust performance in ``anywhere, anytime, anyview\" VPR, performs well, AirRoom further enhances performance, achieving a 20\\% to 40\\% improvement within the available margin compared to AnyLoc. For instance, AnyLoc achieves 89.69\\% accuracy on HMReID, leaving approximately 10\\% room for improvement. AirRoom, with an accuracy of 93.80\\%, demonstrates up to a 40\\% improvement within this remaining margin. These results highlight AirRoom's superior precision and refinement in room reidentification.",
        "trans_content": "\\subsection{整体性能比较}\n\\vspace{-4pt}\n\\label{sec:section4.4}\n\n在本节中，我们展示了我们方法的最佳版本与多种最先进方法之间的性能对比，从而使我们能够在不同的特征提取和检索策略下，将我们的管道与已有的房间重识别模型进行基准测试。\n\n我们选择了三类基线方法：图像检索方法（CVNet \\cite{lee2022correlationverificationimageretrieval}）、基于全局描述子的视觉位置识别（VPR）（DINOv2 \\cite{oquab2024dinov2learningrobustvisual}），以及使用局部特征聚合的 VPR 方法（Patch-NetVLAD \\cite{hausler2021patchnetvladmultiscalefusionlocallyglobal} 和 AnyLoc \\cite{keetha2023anylocuniversalvisualplace}）。具体来说，我们使用了 DINOv2 的 Base 版本，将 CVNet 配置为 ResNet50 \\cite{he2015deepresiduallearningimage} 主干网络，并将降维维度设为 2048，选择了 Patch-NetVLAD 的性能优化版本，并配置 AnyLoc 为 AnyLoc-VLAD-DINOv2，使用 32 个 VLAD 聚类。\n\n<PLACEHOLDER_ENV_20>\n\n\\tref{tab:overall} 展示了 AirRoom 与基线方法之间的定量比较，结果表明 AirRoom 在几乎所有指标和数据集上均优于所有基线方法。在房间重识别任务中，图像检索方法由于其并非专注于 top-1 精度，通常在分类指标上表现较差，而 VPR 方法则取得了更好的结果。基于全局描述子的 VPR 方法仅捕捉高层语义信息，常常检索到语义相似但缺乏细节的房间；相比之下，使用局部特征聚合的 VPR 方法（如 Patch-NetVLAD）强调低层编码，但可能忽视全局上下文，从而导致检索准确性下降。 \\fref{fig:failure} 展示了 CVNet、DINOv2、Patch-NetVLAD 和 AnyLoc 的失败案例，突显了这些方法的局限性。\n\n尽管 AnyLoc 因其在“任何位置、任何时间、任何视角” VPR 中表现稳健而著称，并具有良好性能，但 AirRoom 进一步提升了表现，在可用提升空间内相较 AnyLoc 提高了 20\\% 到 40\\%。例如，AnyLoc 在 HMReID 上取得了 89.69\\% 的准确率，留下了约 10\\% 的提升空间；而 AirRoom 凭借 93.80\\% 的准确率，在剩余空间内实现了高达 40\\% 的提升。这些结果突显了 AirRoom 在房间重识别任务中卓越的精度与优化表现。"
    },
    {
        "section": "4_5",
        "content": "\\subsection{Group-Wise Performance Comparison}\n\\label{sec:section4.5}\n\\vspace{-3pt}\nMany baseline methods adopt a “backbone + enhancement mechanism” paradigm, which our approach also follows. In this section, we compare the performance of our object-aware enhancement mechanism with that of several state-of-the-art methods, using the same backbone as each group’s baseline. This setup allows us to directly assess the effectiveness of our object-aware enhancement mechanism.\n\nFor the ResNet50 backbone group, we use CVNet \\cite{lee2022correlationverificationimageretrieval} as the baseline. In the NetVLAD backbone group, we employ Patch-NetVLAD \\cite{hausler2021patchnetvladmultiscalefusionlocallyglobal} as the baseline, testing it at three reduction dimensions: 4096, 512, and 128.\n\n\\tref{tab:grouped} reveals that within each group, the single backbone outperforms the baseline methods that attempt to enhance performance through various mechanisms, indicating that these mechanisms do not effectively capture critical information in indoor rooms. In contrast, our object-aware enhancement mechanism significantly improves the backbone’s performance by emphasizing the importance of \\mbox{objects} in indoor environments.",
        "trans_content": "\\subsection{按组别的性能比较}\n\\label{sec:section4.5}\n\\vspace{-3pt}\n许多基准方法采用“骨干网络 + 增强机制”的范式，我们的方法也遵循这一范式。在本节中，我们将使用与每个组基准相同的骨干网络，将我们的面向对象增强机制与几种最先进的方法进行性能比较。此设置使我们能够直接评估我们的面向对象增强机制的有效性。\n\n对于ResNet50骨干网络组，我们使用CVNet \\cite{lee2022correlationverificationimageretrieval} 作为基准。在NetVLAD骨干网络组中，我们采用Patch-NetVLAD \\cite{hausler2021patchnetvladmultiscalefusionlocallyglobal} 作为基准，并在三种降维度下进行测试：4096、512和128。\n\n\\tref{tab:grouped}显示，在每个组别中，单一的骨干网络优于那些通过各种机制尝试增强性能的基准方法，这表明这些机制未能有效地捕捉到室内环境中的关键信息。相比之下，我们的面向对象增强机制通过强调室内环境中\\mbox{对象}的重要性，显著提升了骨干网络的性能。"
    },
    {
        "section": "4_6",
        "content": "\\subsection{Pipeline Flexibility Evaluation}\n\\label{sec:section4.6}\n\\vspace{-3pt}\nIn this section, we systematically evaluate the flexibility and adaptability of AirRoom by testing different configurations of its key modules. The results clearly demonstrate that AirRoom is not reliant on any specific model and can effectively integrate a diverse range of models.\n\n\\vspace{-5pt}",
        "trans_content": "\\subsection{管道灵活性评估}  \n\\label{sec:section4.6}  \n\\vspace{-3pt}  \n在本节中，我们通过测试其关键模块的不同配置，系统性地评估了 AirRoom 的灵活性和适应性。结果清楚地表明，AirRoom 并不依赖于任何特定模型，能够有效集成各种不同类型的模型。  \n\n\\vspace{-5pt}"
    },
    {
        "section": "4_6_1",
        "content": "\\subsubsection{Global Feature Extractor}\n\\vspace{-4pt}\nWe test various Global Feature Extractors, including ViT \\cite{dosovitskiy2021imageworth16x16words}, DINO \\cite{caron2021emergingpropertiesselfsupervisedvision}, DINOv2 \\cite{oquab2024dinov2learningrobustvisual}, and AnyLoc-VLAD-DINOv2 \\cite{keetha2023anylocuniversalvisualplace} with VLAD cluster sizes of 16 and 8.\n\nAs shown in \\tref{tab:global feature extractor flexibility}, AirRoom consistently achieves over 85\\% across all metrics and datasets in nearly every case, regardless of the capabilities of the Global Feature Extractor used. Even in the single exception with DINOv2, AirRoom still improves performance by nearly 15\\%. This demonstrates that the effectiveness of our pipeline is not reliant on any specific Global Feature Extractor, highlighting AirRoom's adaptability to various backbone configurations and underscoring its robust flexibility.\n\n\\vspace{-5pt}",
        "trans_content": "\\subsubsection{全局特征提取器}\n\\vspace{-4pt}\n我们测试了多种全局特征提取器，包括ViT \\cite{dosovitskiy2021imageworth16x16words}、DINO \\cite{caron2021emergingpropertiesselfsupervisedvision}、DINOv2 \\cite{oquab2024dinov2learningrobustvisual} 和AnyLoc-VLAD-DINOv2 \\cite{keetha2023anylocuniversalvisualplace}，VLAD簇大小分别为16和8。\n\n如\\tref{tab:global feature extractor flexibility}所示，AirRoom在几乎所有情况下，在所有度量标准和数据集上始终能够超过85\\%，无论使用的全局特征提取器的能力如何。即使是在DINOv2的唯一例外情况下，AirRoom的表现仍然提高了近15\\%。这表明我们的管道的有效性并不依赖于任何特定的全局特征提取器，突显了AirRoom在各种主干配置下的适应性，并强调了其强大的灵活性。\n\n\\vspace{-5pt}"
    },
    {
        "section": "4_6_2",
        "content": "\\subsubsection{Instance Segmentation}\n\\vspace{-4pt}\nWe compare traditional instance segmentation methods, such as Mask R-CNN \\cite{he2018maskrcnn}, with more recent approaches, including Semantic-SAM \\cite{li2023semanticsamsegmentrecognizegranularity}, which leverage advanced techniques for more granular segmentation.\n\n\\tref{tab:is flexibility} shows that AirRoom consistently outperforms the baseline by over 15\\%, regardless of the instance segmentation module used. This demonstrates that our pipeline is not dependent on any specific instance segmentation method, underscoring its adaptability in this component.\n\n<PLACEHOLDER_ENV_21>",
        "trans_content": "\\subsubsection{实例分割}\n\\vspace{-4pt}\n我们将传统的实例分割方法（如 Mask R-CNN \\cite{he2018maskrcnn}）与更近期的 approaches，包括 Semantic-SAM \\cite{li2023semanticsamsegmentrecognizegranularity} 进行比较，后者利用先进技术实现更细粒度的分割。\n\n\\tref{tab:is flexibility} 显示，无论使用何种实例分割模块，AirRoom 始终比基准方法高出超过 15\\%。这证明了我们的管道不依赖于任何特定的实例分割方法，强调了其在此组件中的适应性。\n\n<PLACEHOLDER_ENV_21>"
    },
    {
        "section": "4_6_3",
        "content": "\\subsubsection{Object Feature Extractor}\n\nWe experiment with both traditional backbones, such as ResNet50 \\cite{he2015deepresiduallearningimage}, and more modern backbones, like DINOv2 \\cite{oquab2024dinov2learningrobustvisual}, as the Object Feature Extractor.\n\nAs shown in \\tref{tab:ofe flexibility}, AirRoom achieves substantial performance improvements over the baseline, with minimal performance variation between different Object Feature Extractors. This supports the flexibility of our pipeline in accommodating a range of feature extraction methods.\n\n<PLACEHOLDER_ENV_22>",
        "trans_content": "\\subsubsection{目标特征提取器}\n\n我们实验了传统的骨干网络，如 ResNet50 \\cite{he2015deepresiduallearningimage}，以及更现代的骨干网络，如 DINOv2 \\cite{oquab2024dinov2learningrobustvisual}，作为目标特征提取器。\n\n如 \\tref{tab:ofe flexibility} 所示，AirRoom 在基准模型上实现了显著的性能提升，不同目标特征提取器之间的性能变化很小。这支持了我们管道在适应各种特征提取方法方面的灵活性。\n\n<PLACEHOLDER_ENV_22>"
    },
    {
        "section": "4_6_4",
        "content": "\\subsubsection{Object-Aware Scoring}\n\nWe evaluate both the mean (\\(s_{\\text{mean}}\\)) and max (\\(s_{\\max}\\)) strategies for computing the patch score (\\(s_{\\text{patch}}\\)) and object score (\\(s_{\\text{object}}\\)), assessing their impact on the overall performance.\n\n\\tref{tab:os flexibility} shows that AirRoom’s performance remains stable regardless of the object-aware scoring method used. This underscores the robustness of object-oriented information in room reidentification and demonstrates AirRoom’s flexibility in adapting to different scoring strategies.\n\n<PLACEHOLDER_ENV_23>",
        "trans_content": "\\subsubsection{面向对象评分}\n\n我们评估了均值 (\\(s_{\\text{mean}}\\)) 和最大值 (\\(s_{\\max}\\)) 两种策略，用于计算补丁得分 (\\(s_{\\text{patch}}\\)) 和对象得分 (\\(s_{\\text{object}}\\))，并评估它们对整体性能的影响。\n\n\\tref{tab:os flexibility} 显示，无论使用何种面向对象的评分方法，AirRoom的性能保持稳定。这突显了面向对象信息在房间重新识别中的稳健性，并展示了AirRoom在适应不同评分策略方面的灵活性。\n\n<PLACEHOLDER_ENV_23>"
    },
    {
        "section": "4_7",
        "content": "\\subsection{Ablation Studies}\n\\label{sec:section4.7}\n\nIn this section, we remove certain modules from our pipeline—including the global score \\(s_{\\text{global}}\\),  the patch score \\(s_{\\text{patch}}\\), the object score \\(s_{\\text{object}}\\), within object-aware scoring, and the entire Fine-Grained Retrieval (FGR)—to assess the importance and effectiveness of each component.\n\n<PLACEHOLDER_ENV_24>\n\n<PLACEHOLDER_ENV_25>\n\n\\tref{tab:ablation w/o global score} shows that removing any module from our pipeline leads to a performance drop. However, as long as at least one module remains, our pipeline still outperforms the baseline. \\tref{tab:ablation on global score}  demonstrates that when the Global Feature Extractor (ViT) performs well, the global score \\(s_{\\text{global}}\\) significantly enhances performance. On the other hand, when the Global Feature Extractor (DINOv2) is less effective, the global score \\(s_{\\text{global}}\\) has a slight negative impact, causing a small drop in performance. This result aligns with our hypothesis in Section~\\ref{subsec:refinement}, where the global score acts as a prior to rank the priority of the five candidates. Overall, these ablation studies confirm that every module in our pipeline is both important and necessary.",
        "trans_content": "\\subsection{消融研究}\n\\label{sec:section4.7}\n\n在本节中，我们从我们的管道中移除某些模块——包括全局得分 \\(s_{\\text{global}}\\)、局部得分 \\(s_{\\text{patch}}\\)、目标得分 \\(s_{\\text{object}}\\)（在目标感知评分中）以及整个细粒度检索（FGR）——以评估每个组件的重要性和有效性。\n\n<PLACEHOLDER_ENV_24>\n\n<PLACEHOLDER_ENV_25>\n\n\\tref{tab:ablation w/o global score} 显示，移除任何模块都会导致性能下降。然而，只要至少保留一个模块，我们的管道仍然优于基线。 \\tref{tab:ablation on global score} 证明，当全局特征提取器（ViT）表现良好时，全局得分 \\(s_{\\text{global}}\\) 显著提升性能。另一方面，当全局特征提取器（DINOv2）效果较差时，全局得分 \\(s_{\\text{global}}\\) 会产生轻微的负面影响，导致性能略微下降。这个结果与我们在第~\\ref{subsec:refinement}节中的假设一致，其中全局得分充当优先级排序的先验，用于排名五个候选者的优先级。总体而言，这些消融研究确认了我们管道中的每个模块都是重要且必要的。"
    },
    {
        "section": "4_8",
        "content": "\\subsection{Limitations}\n\nWhile AirRoom achieves state-of-the-art performance in room reidentification under various viewpoint variations, a limitation of our work is the inability to verify robustness to indoor object rearrangements caused by movable objects. Although our mutual nearest neighbors-based Object-Aware Scoring method is somewhat robust to such rearrangements, the datasets used in our experiments lack these cases. In contrast, recent advances in dynamic scene understanding \\cite{zhao2024dynamicsceneunderstandingobjectcentric} focus on recognizing scenes in the presence of moving objects, potentially offering greater robustness than our approach. Future work should consider constructing datasets that include object rearrangements and integrating new techniques to enhance robustness to movable objects, thereby improving room reidentification.<PLACEHOLDER_sec/4_experimental_results_end>\n<PLACEHOLDER_sec/5_conclusion_begin>\\vspace{-5pt}",
        "trans_content": "\\subsection{局限性}\n\n尽管AirRoom在不同视角变化下的房间重识别任务中达到了最先进的性能，但我们工作的一个局限性是无法验证其对由可移动物体引起的室内物品重排的鲁棒性。尽管我们基于互最近邻的物体感知评分方法在一定程度上对这种重排具有鲁棒性，但我们实验中使用的数据集缺乏这些情况。相比之下，最近在动态场景理解方面的进展 \\cite{zhao2024dynamicsceneunderstandingobjectcentric} 专注于在存在移动物体的情况下识别场景，可能比我们的方法提供更强的鲁棒性。未来的工作应考虑构建包含物体重排的数据集，并集成新技术以增强对可移动物体的鲁棒性，从而提高房间重识别的性能。<PLACEHOLDER_sec/4_experimental_results_end>\n<PLACEHOLDER_sec/5_conclusion_begin>\\vspace{-5pt}"
    },
    {
        "section": "5",
        "content": "\\section{Conclusion}\n\\vspace{-5pt}\n\\label{sec:conclusion}\n\nRoom reidentification is a challenging yet crucial research area, with growing applications in fields like augmented reality and homecare robotics. In this paper, we introduce AirRoom, a training-free, object-aware approach for room reidentification. AirRoom leverages multi-level object-oriented features to capture both spatial and contextual information of indoor rooms. To evaluate AirRoom, we constructed four novel datasets specifically for room reidentification. Experimental results demonstrate its robustness to viewpoint variations and superior performance over state-of-the-art methods across nearly all metrics and datasets. Furthermore, the pipeline is highly flexible, maintaining high performance without relying on specific model configurations. Collectively, our work establishes AirRoom as a powerful and versatile solution for precise room reidentification, with broad potential for real-world applications.\n\n\\begin{center}\n\\textbf{Acknowledgments}\n\\end{center}\n<PLACEHOLDER_ENV_26>\n<PLACEHOLDER_sec/5_conclusion_end>\n{\n    \\small\n    \\bibliographystyle{ieeenat_fullname}\n    \\bibliography{main}\n}\n\n<PLACEHOLDER_sec/X_suppl_begin>\\clearpage\n\\setcounter{page}{1}\n\\maketitlesupplementary",
        "trans_content": "\\section{结论}\n\\vspace{-5pt}\n\\label{sec:conclusion}\n\n房间重识别是一个具有挑战性但至关重要的研究领域，在增强现实和居家护理机器人等领域的应用不断增长。在本文中，我们提出了AirRoom，这是一种无训练、面向对象的房间重识别方法。AirRoom利用多层次的面向对象特征来捕捉室内房间的空间和上下文信息。为了评估AirRoom，我们专门构建了四个新的数据集用于房间重识别。实验结果证明，AirRoom在视角变化下具有较强的鲁棒性，并且在几乎所有度量和数据集上都优于现有的最先进方法。此外，该管道高度灵活，在不依赖于特定模型配置的情况下仍能保持高性能。总的来说，我们的工作确立了AirRoom作为一种强大且多功能的房间重识别解决方案，具有广泛的现实应用潜力。\n\n\\begin{center}\n\\textbf{致谢}\n\\end{center}\n<PLACEHOLDER_ENV_26>\n<PLACEHOLDER_sec/5_conclusion_end>\n{\n    \\small\n    \\bibliographystyle{ieeenat_fullname}\n    \\bibliography{main}\n}\n\n<PLACEHOLDER_sec/X_suppl_begin>\\clearpage\n\\setcounter{page}{1}\n\\maketitlesupplementary"
    },
    {
        "section": "6",
        "content": "\\section{Datasets}\nTable \\ref{tab:MPReID} presents the composition of MPReID, while Table \\ref{tab:HMReID}, Table \\ref{tab:GibsonReID}, and Table \\ref{tab:ReplicaReID} outline the compositions of HMReID, GibsonReID, and ReplicaReID, respectively. Table \\ref{tab:Statistics} reports the number of semantically different rooms in each room ReID dataset.\n\n<PLACEHOLDER_ENV_27>\n\n\\vspace{-1em}\n\n<PLACEHOLDER_ENV_28>\n\n\\vspace{-1em}\n\n<PLACEHOLDER_ENV_29>\n\n\\vspace{-1em}\n\n<PLACEHOLDER_ENV_30>\n\n<PLACEHOLDER_ENV_31>",
        "trans_content": "\\section{数据集}  \n表 \\ref{tab:MPReID} 展示了 MPReID 的组成，而表 \\ref{tab:HMReID}、表 \\ref{tab:GibsonReID} 和表 \\ref{tab:ReplicaReID} 分别展示了 HMReID、GibsonReID 和 ReplicaReID 的组成。表 \\ref{tab:Statistics} 报告了每个房间 ReID 数据集中语义上不同房间的数量。\n\n<PLACEHOLDER_ENV_27>\n\n\\vspace{-1em}\n\n<PLACEHOLDER_ENV_28>\n\n\\vspace{-1em}\n\n<PLACEHOLDER_ENV_29>\n\n\\vspace{-1em}\n\n<PLACEHOLDER_ENV_30>\n\n<PLACEHOLDER_ENV_31>"
    },
    {
        "section": "7+7_1",
        "content": "\\section{Experimental Details}\n\n\n\\subsection{Overall Performance Comparison}\n\\label{sec:appendix_overall}\n\n\\paragraph{Baseline Configuration} For CVNet, we use ResNet50 as the backbone and set the reduction dimension to 2048. For DINOv2, we utilize the DINOv2-Base checkpoint. For Patch-NetVLAD, we load pre-trained weights optimized on the Pittsburgh dataset, apply WPCA to reduce feature embedding dimensionality to 4096, set RANSAC as the matcher, use patch weights of 0.45, 0.15, and 0.4, configure patch sizes to 2, 5, and 8 with strides of 1 for all. For AnyLoc, we adopt AnyLoc-VLAD-DINOv2 with the DINOv2 ViT-G/14 architecture, set the descriptor layer to 31, use VLAD with 32 clusters, and specify the domain as \\mbox{indoor}.\n\n\\paragraph{Baseline Adaptation} For CVNet and Patch-NetVLAD, we perform global retrieval by selecting the top-5 candidates, followed by re-ranking. For CVNet, the candidate with the highest CVNet-Rerank image similarity score is chosen as the final result, while for Patch-NetVLAD, the reference with the highest RANSAC score in the Pairwise Local Matching stage is selected. For DINOv2 and AnyLoc, global features are extracted from the query and reference images, and cosine similarity is computed. The reference image with the highest cosine similarity score is selected as the final match.\n\n\\paragraph{AirRoom Configuration} For the Global Feature Extractor, we use AnyLoc-VLAD-DINOv2 with the DINOv2 ViT-G/14 architecture, setting the descriptor layer to 31, applying VLAD with 32 clusters, and specifying the domain as indoor. For Instance Segmentation, we employ Semantic-SAM with pre-trained weights from SA-1B and a SwinL backbone. The Object Feature Extractor is implemented using a ResNet50 model pre-trained on the ImageNet dataset. For Fine-Grained Retrieval, we use LightGlue with the maximum number of keypoints set to 2048.",
        "trans_content": "\\section{实验细节}\n\n\\subsection{整体性能比较}\n\\label{sec:appendix_overall}\n\n\\paragraph{基准配置} 对于CVNet，我们使用ResNet50作为骨干网络，并将降维维度设置为2048。对于DINOv2，我们使用DINOv2-Base检查点。对于Patch-NetVLAD，我们加载在Pittsburgh数据集上优化的预训练权重，应用WPCA来将特征嵌入维度降低到4096，设置RANSAC作为匹配器，使用0.45、0.15和0.4的补丁权重，配置补丁大小为2、5和8，步幅为1。对于AnyLoc，我们采用AnyLoc-VLAD-DINOv2，使用DINOv2 ViT-G/14架构，将描述符层设置为31，使用VLAD并设置32个聚类，指定域为\\mbox{室内}。\n\n\\paragraph{基准适配} 对于CVNet和Patch-NetVLAD，我们通过选择前5个候选项进行全局检索，然后进行重新排序。对于CVNet，选择具有最高CVNet-Rerank图像相似度得分的候选项作为最终结果；而对于Patch-NetVLAD，则选择在成对局部匹配阶段具有最高RANSAC得分的参考项。对于DINOv2和AnyLoc，从查询和参考图像中提取全局特征，并计算余弦相似度。选择具有最高余弦相似度得分的参考图像作为最终匹配项。\n\n\\paragraph{AirRoom配置} 对于全局特征提取器，我们使用AnyLoc-VLAD-DINOv2，采用DINOv2 ViT-G/14架构，将描述符层设置为31，应用VLAD并设置32个聚类，指定域为室内。对于实例分割，我们采用Semantic-SAM，并使用来自SA-1B的预训练权重和SwinL骨干网络。物体特征提取器使用在ImageNet数据集上预训练的ResNet50模型实现。对于细粒度检索，我们使用LightGlue，并将最大关键点数设置为2048。"
    },
    {
        "section": "7_2",
        "content": "\\subsection{Group-Wise Performance Comparison}\n\n\\paragraph{Baseline Configuration} For the ResNet50 backbone group, the configurations for ResNet50 and CVNet follow those detailed in \\sref{sec:appendix_overall}. For the NetVLAD backbone group, we use NetVLAD with VGG-16 as the feature extractor, configured with 64 clusters and a feature dimensionality of 512. For Patch-NetVLAD, the feature dimensionalities are set to 4096, 512, and 128, respectively, with all other settings consistent with \\sref{sec:appendix_overall}.\n\n\\paragraph{Baseline Adaptation} For the ResNet50 backbone group, ResNet50 extracts global features from the query and reference images, with cosine similarity used to select the reference image with the highest score as the final match. The adaptation for CVNet is detailed in \\sref{sec:appendix_overall}. For the NetVLAD backbone group, NetVLAD aggregates global descriptors from the query and reference local features, and the reference with the highest cosine similarity score is chosen as the final result. The adaptation for Patch-NetVLAD also follows \\sref{sec:appendix_overall}.\n\n\\paragraph{AirRoom Configuration} For the ResNet50 backbone group, ResNet50 is used as the Global Feature Extractor, with the configuration consistent with \\sref{sec:appendix_overall}. For the NetVLAD backbone group, NetVLAD is used as the Global Feature Extractor, following the configuration outlined in the Baseline Configuration paragraph in this section. The configurations for the remaining modules in both groups are also consistent with \\sref{sec:appendix_overall}.",
        "trans_content": "\\subsection{按组性能比较}\n\n\\paragraph{基线配置} 对于ResNet50主干网络组，ResNet50和CVNet的配置遵循\\sref{sec:appendix_overall}中详细介绍的设置。对于NetVLAD主干网络组，我们使用NetVLAD与VGG-16作为特征提取器，配置为64个聚类和512维特征。对于Patch-NetVLAD，特征维度分别设置为4096、512和128，其余设置与\\sref{sec:appendix_overall}一致。\n\n\\paragraph{基线适配} 对于ResNet50主干网络组，ResNet50从查询图像和参考图像中提取全局特征，并使用余弦相似度选择得分最高的参考图像作为最终匹配。CVNet的适配过程详见\\sref{sec:appendix_overall}。对于NetVLAD主干网络组，NetVLAD从查询图像和参考局部特征中聚合全局描述符，并选择余弦相似度得分最高的参考图像作为最终结果。Patch-NetVLAD的适配过程同样遵循\\sref{sec:appendix_overall}。\n\n\\paragraph{AirRoom配置} 对于ResNet50主干网络组，ResNet50作为全局特征提取器，配置与\\sref{sec:appendix_overall}一致。对于NetVLAD主干网络组，NetVLAD作为全局特征提取器，配置遵循本节中“基线配置”段落中的设置。两组中其余模块的配置也与\\sref{sec:appendix_overall}一致。"
    },
    {
        "section": "7_3+7_3_1",
        "content": "\\subsection{Pipeline Flexibility Evaluation}\n\n\n\\subsubsection{Global Feature Extractor}\n\n\\paragraph{Baseline Configuration} For ViT, we use the Base variant with a patch size of 16 and an input image size of 224×224, loading pre-trained weights from ImageNet. For DINO, we adopt the DINO-pretrained Vision Transformer Small (ViT-S/16) variant. The configuration for DINOv2 follows \\sref{sec:appendix_overall}. For AnyLoc, VLAD clusters are set to 16 and 8, with all other configurations consistent with \\sref{sec:appendix_overall}.\n\n\\paragraph{Baseline Adaptation} All baselines are used to extract features from query and reference images, with cosine similarity computed to identify the reference room with the highest similarity score.\n\n\\paragraph{AirRoom Configuration} For comparisons with a backbone baseline, the backbone is used as the Global Feature Extractor. Backbone configurations follow those outlined in the Baseline Configuration paragraph of this section, while the configurations for the remaining modules in AirRoom are consistent with \\sref{sec:appendix_overall}.",
        "trans_content": "\\subsection{管道灵活性评估}\n\n\\subsubsection{全局特征提取器}\n\n\\paragraph{基准配置} 对于ViT，我们使用Base变体，补丁大小为16，输入图像大小为224×224，并加载来自ImageNet的预训练权重。对于DINO，我们采用DINO预训练的Vision Transformer Small (ViT-S/16)变体。DINOv2的配置遵循\\sref{sec:appendix_overall}。对于AnyLoc，VLAD聚类设置为16和8，其他所有配置与\\sref{sec:appendix_overall}一致。\n\n\\paragraph{基准适配} 所有基准用于从查询图像和参考图像中提取特征，计算余弦相似度，以识别具有最高相似度得分的参考房间。\n\n\\paragraph{AirRoom配置} 为了与主干基准进行比较，主干被用作全局特征提取器。主干配置遵循本节“基准配置”段落中概述的内容，而AirRoom中其余模块的配置与\\sref{sec:appendix_overall}一致。"
    },
    {
        "section": "7_3_2",
        "content": "\\subsubsection{Instance Segmentation}\n\n\\paragraph{AirRoom Configuration} DINOv2 is used as the Global Feature Extractor. For Mask R-CNN, we use Mask R-CNN with a ResNet50 backbone and FPN, loading pre-trained weights trained on COCO. For Semantic-SAM, we employ Semantic-SAM with pre-trained weights from SA-1B and a SwinL backbone. The configurations for the remaining modules are consistent with \\sref{sec:appendix_overall}.",
        "trans_content": "\\subsubsection{实例分割}\n\n\\paragraph{AirRoom 配置} DINOv2 被用作全局特征提取器。对于 Mask R-CNN，我们使用带有 ResNet50 主干和 FPN 的 Mask R-CNN，并加载在 COCO 上训练的预训练权重。对于 Semantic-SAM，我们使用带有从 SA-1B 预训练的权重和 SwinL 主干的 Semantic-SAM。其余模块的配置与 \\sref{sec:appendix_overall} 一致。"
    },
    {
        "section": "8",
        "content": "\\section{Large-Scale Evaluation}\nSince the four room ReID datasets were curated in a consistent format, we evaluate our method on their union, resulting in more examples for each room type and assessing the feasibility of the proposed method when scaling the data. To this end, we construct a large-scale dataset, UnionReID, by combining all four datasets. Table \\ref{tab:Union} presents a performance comparison between AirRoom and four baseline methods, demonstrating that AirRoom continues to outperform them under large-scale conditions.\n\n<PLACEHOLDER_ENV_32>",
        "trans_content": "\\section{大规模评估}\n由于四个房间ReID数据集采用了一致的格式，我们在它们的联合数据集上评估我们的方法，这样可以为每种房间类型提供更多的样本，并评估在数据扩展时所提出方法的可行性。为此，我们通过将所有四个数据集合并，构建了一个大规模数据集UnionReID。表 \\ref{tab:Union} 展示了AirRoom与四种基线方法的性能比较，表明在大规模条件下，AirRoom仍然优于它们。\n\n<PLACEHOLDER_ENV_32>"
    },
    {
        "section": "9",
        "content": "\\section{Evaluation on Indoor Localization Datasets}\nStrictly speaking, room ReID is a novel task with no previously established datasets and is fundamentally distinct from indoor localization. To address this gap, we introduced four new datasets. However, after reviewing existing indoor localization datasets, we identified two that are marginally usable: InLoc  \\cite{taira2018inlocindoorvisuallocalization} and Structured3D \\cite{Structured3D}. InLoc \\cite{taira2018inlocindoorvisuallocalization} employs area-based rather than room-based splits, with some images capturing only corridors and corners. Structured3D \\cite{Structured3D} contains tens of thousands of room instances, but each room has fewer than six viewpoints. These limitations reduce the suitability of these two datasets, though they remain partially usable. Nonetheless, evaluating our method on them can further reinforce its validation.\n\nTable \\ref{tab:Indoor} presents the comparison results on the two indoor localization datasets, where AirRoom continues to outperform other methods. Additionally, as InLoc represents a more realistic real-world setting, the results further demonstrate AirRoom's robustness in practical environments.\n\n<PLACEHOLDER_ENV_33>",
        "trans_content": "\\section{室内定位数据集评估}\n严格来说，房间重识别（Room ReID）是一项新任务，尚无先前建立的数据集，并且与室内定位任务有本质区别。为了填补这一空白，我们引入了四个新数据集。然而，在审查现有的室内定位数据集后，我们识别出两个勉强可用的数据集：InLoc \\cite{taira2018inlocindoorvisuallocalization} 和 Structured3D \\cite{Structured3D}。InLoc \\cite{taira2018inlocindoorvisuallocalization} 采用基于区域的划分，而非基于房间的划分，其中一些图像仅捕捉走廊和角落。Structured3D \\cite{Structured3D} 包含数万个房间实例，但每个房间的视角少于六个。这些局限性减少了这两个数据集的适用性，尽管它们仍然在某种程度上可用。然而，在这些数据集上评估我们的方法仍能进一步加强其验证。\n\n表 \\ref{tab:Indoor} 展示了在这两个室内定位数据集上的比较结果，结果表明，AirRoom 始终优于其他方法。此外，由于 InLoc 代表了更为现实的实际环境，结果进一步证明了 AirRoom 在实际环境中的鲁棒性。\n\n<PLACEHOLDER_ENV_33>"
    },
    {
        "section": "10",
        "content": "\\section{Runtime Analysis}\n\nIn this section, we evaluate the runtime of each module and compare the total runtime of our pipeline with several state-of-the-art methods to assess the efficiency of our approach.\n\n<PLACEHOLDER_ENV_34>\n\n<PLACEHOLDER_ENV_35>\n\n<PLACEHOLDER_ENV_36>\n\n<PLACEHOLDER_ENV_37>\n\n<PLACEHOLDER_ENV_38>\n\n<PLACEHOLDER_ENV_39>\n\nWhen Mask R-CNN is used for instance segmentation, \\tref{tab:module_runtime_mr} demonstrates that increasing the object mask score threshold significantly reduces the runtime of the Object Feature Extractor when ResNet is employed. This is attributed to the reduced number of objects and patches requiring processing. A similar trend is observed with DINOv2 as the Object Feature Extractor, as shown in \\tref{tab:module_runtime_md}. Additionally, \\tref{tab:module_accuracy} indicates that AirRoom's performance remains largely unaffected by the rise in the object mask score threshold, regardless of the chosen Object Feature Extractor. This observation is further illustrated in \\fref{fig:runtime}. However, when Semantic-SAM is used for instance segmentation, AirRoom faces efficiency challenges due to Semantic-SAM's significantly slower performance, as detailed in \\tref{tab:module_runtime_ssam}.\n\n\\tref{tab:runtime_comparison} compares runtime across methods. AirRoom requires 80ms more than CVNet but achieves over 80\\% performance improvement. Compared to Patch-NetVLAD, AirRoom's runtime is approximately double, with a performance gain exceeding 30\\%. While DINOv2 completes tasks in 10–20ms, AirRoom adds 170ms and improves performance by over 40\\%. Relative to AnyLoc, AirRoom increases runtime by just over 150ms but captures an additional 20\\% of the remaining performance potential. These results demonstrate that AirRoom delivers significant performance gains even within limited improvement margins, underscoring its effectiveness despite incremental runtime.\n\nCurrently, AirRoom allocates approximately 90ms to Fine-Grained Retrieval, utilizing LightGlue for feature matching. Exploring more lightweight and faster alternatives could further enhance efficiency. In real-world applications such as Real-Time Navigation, room reidentification times between 50–200ms are generally acceptable, with accuracy as the primary concern. While AirRoom is slightly slower than some baselines, it achieves substantial accuracy improvements, effectively balancing runtime and performance. This makes AirRoom well-suited for practical scenarios, meeting real-world runtime requirements while maintaining high reliability and precision.<PLACEHOLDER_sec/X_suppl_end>\n\n\\end{document}",
        "trans_content": "\\section{运行时分析}\n\n在本节中，我们评估了每个模块的运行时，并将我们的管道的总运行时与几种最先进的方法进行比较，以评估我们方法的效率。\n\n<PLACEHOLDER_ENV_34>\n\n<PLACEHOLDER_ENV_35>\n\n<PLACEHOLDER_ENV_36>\n\n<PLACEHOLDER_ENV_37>\n\n<PLACEHOLDER_ENV_38>\n\n<PLACEHOLDER_ENV_39>\n\n当使用 Mask R-CNN 进行实例分割时，\\tref{tab:module_runtime_mr} 证明了当使用 ResNet 时，提高目标掩模分数阈值会显著减少对象特征提取器的运行时。这是由于需要处理的对象和补丁数量减少所致。使用 DINOv2 作为对象特征提取器时，\\tref{tab:module_runtime_md} 中也观察到类似的趋势。此外，\\tref{tab:module_accuracy} 显示，AirRoom 的性能在对象掩模分数阈值升高时保持基本不变，无论选择哪种对象特征提取器。这一观察在 \\fref{fig:runtime} 中得到了进一步说明。然而，当使用 Semantic-SAM 进行实例分割时，由于 Semantic-SAM 显著较慢的性能，AirRoom 面临效率上的挑战，具体内容见 \\tref{tab:module_runtime_ssam}。\n\n\\tref{tab:runtime_comparison} 比较了不同方法的运行时。AirRoom 比 CVNet 多需要 80ms，但实现了超过 80\\% 的性能提升。与 Patch-NetVLAD 相比，AirRoom 的运行时大约是其两倍，性能提升超过 30\\%。虽然 DINOv2 完成任务需要 10–20ms，AirRoom 增加了 170ms 并提升了超过 40\\% 的性能。相较于 AnyLoc，AirRoom 增加了约 150ms 的运行时，但捕获了额外的 20\\% 性能潜力。这些结果表明，尽管在有限的改进空间内，AirRoom 仍能提供显著的性能提升，突显了其在运行时上的有效性。\n\n目前，AirRoom 为细粒度检索分配了约 90ms，使用 LightGlue 进行特征匹配。探索更轻量和更快速的替代方案可以进一步提高效率。在如实时导航等实际应用中，房间重识别时间在 50–200ms 之间通常是可接受的，精度是主要考虑因素。虽然 AirRoom 比一些基线稍慢，但它实现了显著的精度提升，有效地平衡了运行时和性能。这使得 AirRoom 非常适合实际场景，能够满足现实世界的运行时要求，同时保持高可靠性和精确度。<PLACEHOLDER_sec/X_suppl_end>\n\n\\end{document}"
    }
]