\vspace{-0.5cm}
\begin{abstract}

构建能够通过多模态信号有效感知世界的通用模型一直是一个长期目标。当前的方法涉及集成单独预训练的组件，例如将视觉编码器连接到大型语言模型（LLMs）并继续进行多模态训练。尽管这些方法展示了显著的样本效率，但是否这种后期融合架构本质上优于其他架构仍是一个开放问题。在本工作中，我们重新审视了原生多模态模型（NMMs）的架构设计——那些从底层开始就在所有模态上进行训练的模型——并进行了广泛的缩放定律研究，涵盖了457个具有不同架构和训练混合的模型。我们的研究表明，后期融合架构相对于早期融合架构没有内在优势，而早期融合架构不依赖于图像编码器。相反，早期融合架构在较低参数计数下表现出更强的性能，训练效率更高，部署更简单。受早期融合架构强大性能的启发，我们展示了一种引入混合专家模型（MoEs）的方法，使模型能够学习模态特定的权重，显著提升了性能。





\end{abstract}