

\section{\edit{讨论和 }局限性} 

\cpar{\edit{多模态数据混合的缩放定律。}} 我们的缩放定律研究涵盖了不同的模型配置和训练混合。虽然结果表明缩放定律系数在混合中基本保持一致，但仍需要更广泛的混合变化探索来验证这一观察，并建立一个统一的缩放定律来考虑这个因素。  

\cpar{\edit{缩放定律和下游任务性能。} 与之前的缩放定律研究类似，我们的分析集中在通过验证损失衡量的预训练性能上。然而，这些发现转化为下游性能的程度仍然是一个开放的问题，需要进一步研究。}

\cpar{\edit{外推法到更大的规模。}} 缩放定律的预测准确率随着FLOPs的增加而提高~\cref{app:scaling_laws}。
\edit{此外，我们在外推到更大的模型大小时验证了我们的定律（\cref{sec:scaling_laws_evaluation}）。} 然而，这些定律是否可以可靠地外推到极大规模的模型仍然是一个悬而未决的问题。 

\cpar{\edit{高分辨率和早期融合模型。} 使用高分辨率输入训练早期融合模型会导致视觉词元数量显著增加。虽然池化技术已被广泛应用于晚期融合模型，但对于早期融合可能需要替代方法。\edit{鉴于早期融合模型与大型语言模型的相似性，扩展上下文长度的技术似乎是有益的。}}

\cpar{\edit{多模态MoE模型的缩放规律。}} 对于MoE，我们仅考虑一种配置（顶级-1路由，8个专家）。\edit{我们发现此配置在我们的设置中表现合理，并遵循标准的MoE实现。}然而，当优化MoE架构或探索不同的负载均衡、路由策略以及不同专家实现时，结果可能会有所不同。\edit{更多}

\section{结论} 
我们探索了针对本地多模态模型进行计算最优预训练的各种策略。我们发现，NMM（神经多模态模型）遵循与LLM（大型语言模型）相似的缩放定律。与普遍的看法相反，我们发现在早期融合架构和晚期融合架构之间没有固有的优势。虽然这两种架构表现出相似的缩放特性，但早期融合模型更易于训练，并在较低的计算预算下优于晚期融合模型。此外，我们展示了稀疏架构能够鼓励模态特定的特化，从而在保持相同的推理成本的同时提高性能。




\section*{\edit{致谢}} 我们要感谢Philipp Dufter、Samira Abnar、Xiujun Li、Zhe Gan、Alexander Toshev、Yinfei Yang、Dan Busbridge和Jason Ramapuram进行了许多富有成果的讨论。我们要感谢Denise Hui和Samy Bengio在基础设施和计算方面的支持。最后，我们要感谢Louis Béthune、Pierre Ablin、Marco Cuturi以及Apple的MLR团队在整个项目中的支持。