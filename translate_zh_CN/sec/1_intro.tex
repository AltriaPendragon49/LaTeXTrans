
\section{引言}
\label{sec:intro}



多模态为感知和理解世界提供了丰富的信号。  
视觉方面的进展，  
\citep{radford2021learning,oquab2023dinov2,zhai2023sigmoidsiglip,fini2024multimodalaimv2},
\edit{音频 \citep{huang2022masked,elizalde2023clap,chen2022wavlm,hsu2021hubert}}  
和语言模型 \citep{achiam2023gpt4,team2023gemini,dubey2024llama3}  
使得开发能够理解语言、图像和音频的强大多模态模型成为可能。一种常见的方法是将分别预训练的单模态模型拼接在一起，例如，将视觉编码器连接到LLM~\citep{laurenccon2024mattersidefics2,shukor2023epalm,alayrac2022flamingo,
xue2024xgenblip3,beyer2024paligemma,wang2024qwen2,liu2024improvedllava,zhang2023videollama,kong2024audioflam,defossez2024moshi} 的输入层。


尽管这看起来是一种方便的方法，但这样的后期融合策略是否本质上是最优的 \edit{用于理解多模态信号} 仍然是一个开放的问题。此外，随着大量多模态数据的可用性，从单模态预训练进行初始化可能会带来危害，因为它可能引入偏差，阻碍模型 \edit{充分利用跨模态的共依赖关系}。
另一个挑战是扩展此类系统；每个组件（例如视觉编码器、LLM）都有其自己的超参数集，\edit{预训练数据混合}，以及 \edit{在数据和计算量方面} 的扩展特性。一种更灵活的架构可能允许模型在模态之间动态分配其容量，从而简化扩展工作。


在这项工作中，我们关注从零开始在多模态数据上训练的原生多模态模型的缩放特性。首先，我们通过将常用的晚期融合架构与早期融合模型进行比较来研究它们是否具有内在优势，早期融合模型在处理原始多模态输入时不依赖专用视觉编码器。
我们在早期和晚期融合架构上进行缩放实验，推导出缩放定律以预测其性能和计算最优配置。我们的发现表明，当从头开始训练时，晚期融合没有固有的优势。相反，早期融合模型更高效且更容易扩展。此外，我们观察到原生多模态模型遵循与LLMs类似的缩放定律~\citep{hoffmann2022training}，尽管跨模态和数据集的缩放系数略有差异。我们的结果表明，为获得最佳性能，模型参数和训练词元应大致等比例缩放。
此外，我们发现不同的多模态训练混合物表现出相似的整体趋势，这表明我们的发现很可能适用于更广泛的场景。


虽然我们的研究结果倾向于早期融合，但多模态数据本质上是异构的，这表明一定程度的参数特化可能仍然能带来好处。为 \edit{探索} 这一点，我们 \edit{研究利用} 混合专家（MoE）~\citep{shazeer2017outrageously} 技术，该技术使模型能够以对称和平行的方式在模态间动态分配特化的参数，与晚期融合模型形成对比，后者是不对称的且按顺序处理数据。使用 MoE 训练原生多模态模型可以显著提高性能并 \edit{因此} 加快收敛速度。\edit{我们的 MoE 扩展定律表明，扩展训练词元的数量比扩展活跃参数的数量更重要。这种不平衡扩展与密集模型所观察到的情况不同，因为稀疏模型具有更多的总参数。}\edit{此外，} 我们的分析显示，专家倾向于在不同的模态上特化，这种特化在早期和最后一层尤为明显。 




\input{figs/teaser}

\subsection{我们的发现摘要}
我们的发现可概括如下：

\cpar{原生早期融合和晚期融合表现相当：} \edit{从头开始训练的早期融合模型}
与晚期融合对应模型表现相当，对于计算资源有限的情况，早期融合模型略有优势 (\cref{fig:early_vs_early_init_scaledata})。此外，我们的缩放法则研究表明，随着计算预算的增加，早期和晚期融合的最佳计算模型表现相似~(\cref{fig:teaser} 左侧)。

\cpar{NMMs缩放类似LLMs：} 原生多模态模型的缩放规律遵循与仅文本的LLMs相似的规律，缩放指数因目标数据类型和训练混合比例的不同而略有变化 (\cref{tab:early_vs_late_coeffs})。

\cpar{晚期融合需要更多的参数：}  
与早期融合相比，计算最优的晚期融合模型需要更高的参数-to-数据比率（\cref{fig:teaser} 右）。

\cpar{稀疏性显著有利于早期融合的NMMs：} 在相同的推理代价下，稀疏NMMs相较于它们的稠密版本表现出显著的改进~(\cref{fig:dense_vs_moe_scaledata})。此外，当使用稀疏性进行训练时，它们会隐式地学习模态特定的权重~(\cref{fig:app_moes_specialization})。 \edit{此外，计算最优模型在计算预算增加时更多依赖于扩展训练词元的数量而不是活跃参数的数量 (\cref{fig:teaser} 右边)。}

\cpar{模态无关路由优于模态感知路由用于稀疏NMMs：} 训练具有模态无关路由的稀疏混合专家模型始终优于具有模态感知路由的模型 (\cref{fig:hard_vs_moe_scaledata})。


\vspace{-5pt}




